# RAG Pipeline for RedBooks

This repository provides a Retrieval-Augmented Generation (RAG) pipeline for processing and utilizing RedBooks. The RedBooks are pre-converted into markdown files using the Python library `docling`. This pipeline uses ChromaDB for vector database storage and `llama-cpp-python` for Large Language Model (LLM) inference.

## Prerequisites

Before using this project, you need to install several python libraries. Run these commands in order to install micromamba (package manger):


$ curl -Ls https://micro.mamba.pm/api/micromamba/linux-ppc64le/latest | tar -xvj bin/micromamba

$ eval “$(micromamba shell hook –shell bash)”

$ micromamba –version

This should display your current version of micromamba on your terminal. Now we need to create our virtual
environment and install Python packages in it.

$ micromamba create -n rag_env python=3.11

$ micromamba activate rag_env

Now you should see rag_env in the bottom left corner of your terminal. Now we are ready to install python packages into this environment:

$ micromamba install -c rocketce -c defaults pytorch-cpu pyyaml httptools onnxruntime "pandas<1.6.0"
tokenizers

$ pip install -U --extra-index-url https://repo.fury.io/mgiessing --prefer-binary chromadb transformers
psutil langchain sentence_transformers gradio==3.50.2 llama-cpp-python scikit-learn docling



## Usage

Note that every step has been done beforehand, so you do not need to build up the VectorDB or convert the pdfs into markdown.  You need to do 2 steps:

1. Update the `model:path` in the run_model.py variable to point to your **GGUF model**.

2. $ python run_model.py 

and the Gradio frontend should be accesible on IP_your_machine:7680 in a webbrowser. 


If you do want to go through the steps manually, then follow along. Also note, that you can enrich this application with your own files / RedBooks. This is also described in the following. Look at the architecture.png file to get an overall feel for this application.

### 1. Convert the pdf files into a markdown file

1. Run the `converter_docling.py` script:
   ```bash
   python converter_docling.py
   ```

This will take for each RedBook file around 15 min.

### 2. Generate the Vector Database

To generate the vector database from your markdown files:

1. Run the `chromaDB_md.py` script:
   ```bash
   python chromaDB_md.py
   ```

   Note: If you do have other .md files in the markdown folder, then you need to specify this in the database_setup.txt file. Here you need to first give it the name of the collection that you want to add the file to and afterwards the name.

Meaning, that this setup:

POWER10:E1050.md
POWER10:E1080.md
POWER10:S1012.md
POWER10:ScaleOut.md
POWER11:E1180.md
POWER11:E1150.md
POWER11:P11_Scaleout.md
OPENSHIFT:Openshift.md
ANSIBLE:Ansible.md


Creates 4 collections, which can be selected in the frontend, and querries are only going into the files in those collections. You can also do 1 large collections for all of your files.

2. This will create a vector database in the `/db` directory.

### 3. Configure the LLM

To use the Large Language Model with the context from the vector DB (LLM):

1. Open `run_model.py` in your preferred text editor.
2. Update the `model:path` variable to point to your **GGUF model**.

### 4. Run the LLM

Execute the pipeline by running:
```bash
python run_model.py
```

This will start serving the gradio UI over HTTP port `e.g. 7680` 

## Alternative Installation: Ansible Playbooks

Alternatively this demo can be installed on a remote or local ppc64le RHEL host using the ansible playbook in the `ansible` directory.

For possible configuration options see the [example inventory file](ansible/example-inventory.yml).

## Folder Structure

- `/db`: Contains the vector database with collections generated by ChromaDB.
- `chromaDB_md.py`: Script for creating the vector database.
- `run_model.py`: Script for running the RAG pipeline using the configured LLM.

## Notes

- Ensure the RedBooks markdown files are in the expected format before running the pipeline.
- Make sure the GGUF model is compatible with `llama-cpp-python`.

## Contributing

If you would like to contribute to this project, feel free to fork the repository, make changes, and submit a pull request. 

## License

This project is licensed under the [MIT License](LICENSE). Feel free to use, modify, and distribute this project.

---

Happy experimenting with the RAG Pipeline for RedBooks!

