
## Introduction to Power11

IBM Power11 represents a significant leap in enterprise computing. Building on the strengths of Power10, Power11 introduces up to 25% more cores per chip, higher clock speeds, and enhanced energy efficiency. It continues IBM's focus on reliability, availability, and serviceability (RAS), while also integrating quantum-safe security features to future-proof critical workloads. The processor is designed to support demanding enterprise applications, particularly those involving AI, analytics, and hybrid cloud environments.

Power11 leverages Integrated Stacked Capacitor (ISC) technology and advanced 2.5D packaging to boost performance and efficiency. These innovations, combined with enhanced thermal management - such as more efficient fans and heatsinks - significantly improve system density, cooling effectiveness, and overall compute capability.

A standout feature of Power11 is its continued support for AI workloads through the Matrix-Math Assist (MMA) architecture and the integration of the IBM Spyre Accelerator, which is optimized for generative AI and complex model inference. This positions Power11 as a strong alternative to traditional GPU-heavy AI infrastructures. Additionally, Power11 strengthens its virtualization capabilities with deeper KVM integration, enhancing compatibility with Linux-native tools and hybrid cloud platforms. This makes it a versatile choice for enterprises seeking scalable, AI-ready infrastructure.

The following topics are covered in this chapter:

- /SM590000 IBM Power11: Advancing Enterprise Computing
- /SM590000 Power Roadmaps
- /SM590000 Power11 improvements from Power 10
- /SM590000 Power11: Trusted, autonomous, and modern
- /SM590000 IBM Power11 processor architecture
- /SM590000 Operating system support
- /SM590000 Firmware and Hardware Management Console
- /SM590000 Rack support

## 1.1  IBM Power11: Advancing Enterprise Computing

For over 35 years, IBM Power servers have been a cornerstone of enterprise computing, delivering unmatched performance, reliability, and availability for mission-critical workloads across the globe. From the original RS/6000 systems powered by the Power processor to the advanced Power11-based platforms, IBM Power has consistently evolved to meet the demands of modern IT infrastructure.

Power architecture adheres to a steady innovation cadence, introducing a new processor generation approximately every three years. Each release delivers substantial advancement - including increased core and thread counts, enhanced energy efficiency, greater memory bandwidth, and expanded I/O capabilities - ensuring the platform remains a leader in enterprise computing.

The Power11 platform is built on three foundational principles:

- /SM590000 Trusted

Delivers continuous business operations with up to 99.9999% availability (on Power E1180), zero planned downtime for maintenance, quantum-safe cryptography, and accelerated system recovery.

- /SM590000 Autonomous

Boosts operational efficiency through intelligent automation, dynamic performance tuning, and reduced manual intervention.

- /SM590000 Modern

Enables AI-infused application deployment across hybrid environments - on-premises and in the cloud - leveraging off-chip AI acceleration and seamless integration with platforms like Red Hat OpenShift and IBM watsonxfi.

The Power11 family is segmented into three primary tiers to address a wide range of workload requirements:

- /SM590000 Entry-level: Power S1122/L1122/S1124/L1124 - optimized for departmental and edge workloads
- /SM590000 Midrange: Power E1150 - ideal for enterprise consolidation and scalable virtualization
- /SM590000 High-end: Power E1180 - designed for large-scale, mission-critical, and AI-intensive workloads

Power11 introduces several key advancements across the portfolio:

- /SM590000 Up to 6 nines (99.9999%) availability on high-end systems
- /SM590000 Enhanced security with quantum-safe cryptography and secure boot enhancements
- /SM590000 Performance gains of up to 15% per core and up to 25% per thread with combined hardware and software optimizations
- /SM590000 Significant throughput improvements in midrange and entry-level systems
- /SM590000 Up to 50% increase in memory bandwidth, leveraging DDR5 and advanced memory controllers

Additionally, Power11 introduces serial number-preserving upgrades from Power10 systems (available starting Q4 2025), enabling seamless transitions without disrupting asset tracking or licensing.

This IBM Redbook focuses on the Power E1150, the midrange system in the Power11 family. It offers a well-balanced blend of performance, scalability, and cost-efficiency - making it an ideal platform for enterprise workloads, virtualization environments, and AI integration.

## 1.2  Power Roadmaps

IBM continues to advance its Power architecture to address the evolving demands of artificial intelligence, hybrid cloud, and mission-critical enterprise workloads. Central to IBM's hybrid cloud strategy, the Power platform is optimized for Red Hat OpenShift, enabling containerized workloads and cloud-native application development.

IBM Power Virtual Server extends these capabilities into the cloud, allowing clients to run Power workloads either on-premises or in IBM Cloud, while benefiting from flexible consumption models and meeting data residency and compliance requirements. This hybrid approach ensures seamless workload portability and supports modernization initiatives.

The Power platform also maintains a robust ecosystem of operating systems, including IBM AIX - a UNIX-based OS with a roadmap extending beyond 2035; IBM i - an integrated OS renowned for its security, stability, and ease of use; and Linux on Power, which supports distributions such as Red Hat Enterprise Linux (RHEL) and SUSE Linux Enterprise Server (SLES), making it ideal for open-source and cloud-native environments.

Complementing its technology stack, IBM has built a strategic network of alliances that significantly expand the reach and capabilities of the Power platform. Notably, IBM Power Virtual Server is a certified platform for RISE with SAP, enabling high-performance, reliable SAP workload deployment in hybrid cloud environments. Red Hat technologies like OpenShift and Ansible are deeply integrated into Power systems, supporting automation and DevOps practices. Additionally, IBM collaborates with major cloud providers - including Microsoft, AWS, and Google Cloud - to deliver hybrid and multicloud solutions, allowing Power workloads to extend seamlessly into public cloud infrastructures.

This section discusses IBM's roadmaps for evolution for the IBM Power-based infrastructure.

## 1.2.1  IBM Power Processor Roadmap

IBM's Power roadmap continues to evolve with the announcement of Power11. Building on the innovations of Power10, Power11 introduces enhancements across the processor architecture, improving its packaging, and providing advances in energy efficiency.

A key advancement is the integration of an additional silicon layer for improved energy management, allowing for higher performance with lower power consumption. This generation also boosts core strength and count, enabling it to manage even more demanding workloads. IBM is also refining its matrix math accelerators (MMAs), which were first introduced in Power10, to further accelerate AI inferencing directly on the chip - eliminating the need for external GPUs in many scenarios.

Looking beyond Power11, IBM's roadmap emphasizes full-stack innovation and adaptability to emerging technologies. The Power11 platform is designed to be memory-agnostic, supporting both DDR4 and DDR5 via the Open Memory Interface (OMI), and it introduces improved thermal infrastructure through advanced packaging and cooling technologies. These developments not only enhance performance but also reduce operational costs in data centers. Future iterations of the Power architecture are expected to continue this trend, with a focus on hybrid cloud readiness, AI integration, and support for open-source ecosystems. IBM's long-term vision

includes deeper collaboration with the OpenPOWER community and a renewed emphasis on openness, flexibility, and sustainability in enterprise computing.

Figure 1-1 shows the different generations of the Power processor family and shows the continued plans for improvements. With the release of Power11, the next generation of Power processors is already in development and additional processor designs are in the pipeline after that generation.

Figure 1-1   Power processor roadmap from IBM POWER8fi to the future

<!-- image -->

## 1.2.2  IBM AIX Roadmap

IBM AIX (Advanced Interactive eXecutive) operating system is a proprietary UNIX-based operating system built on open standards and specifically designed for IBM Power servers.

For over 38 years, AIX has powered mission-critical applications and databases in industries with high-performance computing needs like finance, retail, healthcare, government, manufacturing, and insurance. Paired with IBM Power11 processor-based systems, the latest AIX 7.3 expands its reach into new markets and workloads to deliver unmatched performance, security, scalability, and reliability, all while supporting digital transformation through flexible subscription models tailored to business needs, and enabling adoption of technologies like hybrid cloud, AI, and cloud-native applications.

AIX's proven binary compatibility allows applications to run unchanged and without recompilation on the newest release, safeguarding IT investments in the platform.

IBM is strongly committed to delivering an AIX release roadmap of further innovations and purposeful support that extends beyond 2035 ensuring a stable, future-proof platform for enterprise workloads.

The current roadmap is shown in Figure 1-2.

Figure 1-2   Support timeline for future generations of AIX

<!-- image -->

For more information on IBM's strategy and roadmap for IBM AIX see Strategy and Roadmap for the IBM AIX Operating System.

Additional information about AIX can be found in section 6.1, 'AIX' on page 122.

## 1.2.3  PowerVM VIOS roadmap

PowerVM Virtual I/O Server (VIOS) is a critical component of the IBM Power Systems virtualization stack, enabling shared access to physical I/O resources for client logical partitions (LPARs). Like all IBM software, VIOS follows a defined product lifecycle that includes general availability, support phases, and end-of-support (EOS) milestones. Table 1-1 shows the VIOS release schedule as of July 2025.

Table 1-1   VIOS release schedule

| VIOS Release   | Release Date   | End of Fix Support a         | Latest FP b      | Next FP c    |
|----------------|----------------|------------------------------|------------------|--------------|
| 4.1.1          | Dec-24         | 31 December 2027 (estimated) | VIOS_FP_4.1.1.0  | 23 July 2025 |
| 4.1.0          | Nov-23         | 30 November 2026 (estimated) | VIOS_FP_4.1.0.30 | 23 July 2025 |
| 3.1.4          | Dec-22         | 30 April 2026 (estimated)    | VIOS_FP_3.1.4.50 | 23 July 2025 |
| 3.1.3          | Sep-21         | 30-Sep-24                    | VIOS_FP_3.1.3.40 | None         |
| 3.1.2          | Nov-20         | 30-Nov-23                    | VIOS_FP_3.1.2.60 | None         |

- a. 'End of Fix Support (EoFS) is the end of the maintenance period for a VIOS Release.  Fix Packs and interim fixes will not be created for a VIOS Release after EoFS.
- b. 'Latest FP' is the most recent FP available for the VIOS Release and links to that FP on Fix Central.
- c. 'Next FP' is the target availability date for the next Fix Pack for the given VIOS Release.  After the final Fix Pack for a VIOS Release is available, further fixes for the release are provided as interim fixes.

Note: The End of Service (EOS) date for a VIOS release is not indicated in this table. EOS dates can be found on the IBM Software Lifecycle page.

For additional details regarding the PowerVM Virtual I/O Server (VIOS) roadmap, please refer to the PowerVM VIOS Lifecycle Information and System to PowerVM Virtual I/O Server maps.

## 1.2.4  Red Hat Enterprise Linux and IBM Power

Red Hat has affirmed its strong commitment to the IBM Power platform, particularly as both companies align around hybrid cloud, AI, and open-source innovation. This partnership is deeply rooted in their shared vision of delivering enterprise-grade solutions that are scalable, secure, and optimized for modern workloads.

At the Red Hat Summit 2025, Red Hat emphasized its dedication to supporting IBM Power through continued enhancements to Red Hat Enterprise Linux (RHEL) on Power Systems. This includes performance tuning for Power10 and Power11 processors, as well as deeper integration with Red Hat OpenShift for containerized and cloud-native applications. These efforts ensure that Power users can fully leverage Red Hat's open hybrid cloud technologies across both on-premises and cloud environments.

Red Hat also plays a pivotal role in enabling AI workloads on Power, with support for AI inference and acceleration technologies that align with IBM's hardware innovations like Matrix Math Assist (MMA) and the upcoming Spyre AI Accelerator. This synergy allows enterprises to deploy AI models efficiently on Power infrastructure using Red Hat's open-source toolchains and platforms.

Furthermore, Red Hat and IBM continue to collaborate on automation and DevOps through tools like Ansible Automation Platform, which is optimized for managing Power environments. This integration simplifies operations, enhances consistency, and accelerates application delivery across hybrid infrastructures.

For more information on Red Hat Enterprise Linux support on IBM Power see 'Red Hat Enterprise Linux' on page 127.

## 1.2.5  SUSE Enterprise Linux and IBM Power

The partnership between SUSE Linux and IBM Power Systems is a long-standing and strategic collaboration focused on delivering enterprise-grade Linux solutions for mission-critical workloads, hybrid cloud, and SAP environments.

SUSE and IBM have worked together for over two decades to provide robust, scalable, and secure Linux solutions on IBM Power infrastructure. Their joint efforts are centered around:

- /SM590000 SUSE Linux Enterprise Server (SLES) for Power Systems
- /SM590000 SLES for SAP Applications, optimized for SAP HANA on Power
- /SM590000 Hybrid and multicloud enablement through container and virtualization technologies

SUSE's open-source model aligns with IBM's strategy to provide flexible, vendor-neutral solutions. This enables customers to modernize their IT environments while avoiding vendor lock-in.

## 1.3  Power11 improvements from Power 10

IBM Power E1150 introduces a comprehensive suite of advanced capabilities, all rooted in IBM's core strength: a full-stack, end-to-end design. By tightly integrating every layer of the infrastructure - from the POWER11 processor and system architecture to firmware, operating systems, and cloud services - IBM Power delivers a synergistic platform built for autonomous IT. This holistic approach consistently drives tangible business outcomes across three foundational pillars: unprecedented business continuity, enhanced productivity and efficiency, and accelerated growth and scalability.

## Unprecedented Business Continuity

IBM Power E1150 redefines operational resilience with features designed to eliminate downtime and ensure continuous service delivery:

- /SM590000 Planned Downtime Elimination: Leveraging technologies such as live updates, rolling upgrades, and autonomous patching, the E1150 enables maintenance without taking applications offline - targeting zero hours of planned downtime.
- /SM590000 Spare Core Technology: At the silicon level, the system includes spare processor cores that can be dynamically activated in response to hardware failures. This proactive fault tolerance mechanism allows seamless failover without requiring system or application restarts, preserving compute capacity and system integrity.
- /SM590000 Power Cyber Vault: Advanced threat detection can identify ransomware attacks in under a minute. Combined with automated recovery mechanisms, this feature significantly enhances cyber resiliency. It is offered in collaboration with IBM Storage and IBM Expert Labs.
- /SM590000 Quantum-Safe Protection: The Power11 platform incorporates quantum-safe cryptography to secure system reboots and live partition mobility, ensuring future-ready data protection.
- /SM590000 Crypto Compliance: Support for the IBM 4770 Crypto Card enables FIPS 140-3 Level 4 certification, enhancing compliance with stringent security standards.
- /SM590000 Automated Diagnostics: Intelligent data collection accelerates error resolution, saving up to 8 hours per support ticket and reducing time to recovery.

## Enhanced Productivity and Efficiency

IBM Power E1150 is engineered to maximize IT efficiency and output:

- /SM590000 Autonomous IT Operations: With built-in automation and AI-driven management, the system reduces manual intervention and operational overhead.
- /SM590000 Integrated Monitoring and Management: Tools like HMC and PowerVC provide centralized control, enabling streamlined operations across hybrid environments.

## Accelerated Growth and Scalability

IBM Power E1150 supports dynamic scaling and hybrid cloud integration to meet evolving business demands:

- /SM590000 IBM Power Virtual Servers (PowerVS): Immediate availability of Power11 in IBM Cloud enables seamless extension of workloads to the cloud, maintaining consistent performance, security, and OS compatibility.
- /SM590000 Hybrid Cloud Agility: Enterprises can scale resources on demand, accelerate development and testing, and optimize costs through a consumption-based model.
- /SM590000 AI Acceleration with MMA: Power11 processor cores feature Matrix Multiply Assist (MMA) a hardware acceleration unit optimized for AI and machine learning workloads. By executing matrix operations directly on-chip, MMA reduces latency and boosts throughput, making the E1150 ideal for deep learning inference and training tasks.

## Technical enhancements

Along with the benefits listed above, Power11 based systems provide enhanced performance benefits from higher core frequencies, additional cores per server and improved memory bandwidth with reduced memory latency. Table 1-2 shows some of the benefits that these improvements can bring to your enterprise.

Table 1-2   Power11 performance benefits

| Improvement                                                                     | Benefit                                                                                                                    |
|---------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|
| Up to 15-25% core performance increase with both HW and SW planned improvements | Exceed SLAs for response times and batch windows.                                                                          |
| 30-45% system performance increase with the new larger 30-Core Power11 DCMs     | Ensure that increasing business workload demands can continue to be met with improved system performance.                  |
| Memorytechnologyimprovementswithupto 50% higher data rates                      | Enhance performance and system scalability for memory intensive workloads by optimizing both memory bandwidth and latency. |

## 1.3.1  New Power11 server portfolio

The new Power11 server portfolio consists of the following offerings, providing clients the choices designed to meet their specific business requirements.

## High-End: IBM Power E1180

At the top of the portfolio is the IBM Power E1180, a high-end enterprise server built for the most demanding workloads. It delivers exceptional performance, scalability, and resiliency, making it ideal for large-scale ERP systems, core banking, and high-throughput analytics. With support for massive memory footprints, advanced RAS features, and robust virtualization capabilities, the Power E1180 is engineered for continuous availability and extreme workload consolidation.

## Midrange: IBM Power E1150

The Power E1150 serves as the midrange workhorse of the Power11 family. It brings many of the high-end capabilities of the Power E1180 - such as spare core technology, pervasive memory encryption, and quantum-safe security - into a more compact and cost-effective form factor. The Power E1150 is ideal for organizations seeking a balance between performance, scalability, and operational efficiency, especially in hybrid cloud environments.

## Entry-Level Scale-Out: IBM Power S1124 and S1122

For smaller enterprises or distributed environments, IBM offers the Power S1124 and Power S1122 servers. These entry-level, scale-out systems are optimized for cost-effective deployment of AIX, IBM i, and Linux workloads. They are well-suited for departmental applications, edge computing, and cloud-native workloads. Despite their compact size, these systems benefit from the same Power11 processor technology, ensuring consistent performance and security across the portfolio.

Figure 1-3 shows the new Power11 server portfolio.

Figure 1-3   Power11 portfolio

<!-- image -->

## Architectural technology Improvements

The Power11 portfolio introduces significant innovations across both the product lineup and the underlying core technologies, all aimed at delivering enhanced performance and greater value. One of the most notable advancements lies in the processor and memory architecture, where Power11 brings substantial improvements over the previous Power10 generation. These enhancements form the foundation for the next level of scalability, efficiency, and intelligent workload optimization.

## Increased core counts and spare cores

Improved thermal dissipation solutions assist in providing optimal cooling, supporting higher core densities and sustained performance.

## Increased processor frequencies

Integrated Stacked Capacitor technology improves power delivery and stability at the silicon level, enabling higher performance under sustained workloads. Refined algorithms dynamically adjust processor frequency based on workload characteristics, maximizing performance while maintaining energy efficiency.

## Increased memory frequencies

The IBM Power11 architecture introduces higher memory frequencies and increased memory bandwidth, resulting in substantial improvements in overall system performance. By supporting faster memory speeds and optimizing the memory subsystem, Power11 significantly boosts memory throughput while simultaneously reducing latency. These enhancements provide quicker data access and processing, which is especially critical for memory-intensive workloads such as real-time analytics, large-scale databases, and AI applications. The combination of higher bandwidth and lower latency enables smoother performance, better responsiveness, and more efficient use of compute resources across the entire system.

Table 1-3 provides a listing of the portfolio with a comparison to the Power10 offerings.

Table 1-3   Portfolio overview

| Segment         | Models                          | Footprint Capacity                                                                                                                                                                                                              | Power10 Compare                                                                                                                                                         |
|-----------------|---------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| High-End (4S+): | IBM Power E1180                 | /SM590000 4 nodes (CEC) each 5U with 4 processor modules each /SM590000 Up to 64 TB - DDR5 based memory                                                                                                                         | /SM590000 Same memory capacity /SM590000 Introducing Resource Groups to maximize system utilization /SM590000 In-place upgrades                                         |
| Mid-Range       | IBM Power E1150                 | /SM590000 2-4 processors in a 4U form factor /SM590000 Up to 30 cores per module /SM590000 Up to 16 TB - DDR5 based memory                                                                                                      | (4S4U) core capacity /SM590000 Same memory capacity /SM590000 Introducing Resource Groups to maximize system utilization /SM590000 In-place upgrades with serial number |
| Scale Out 2S4U  | IBM Power S1124 IBM Power L1124 | /SM590000 Up to 2 processors in a 4U form factor /SM590000 Up to 30 cores per module /SM590000 Up to 8 TB - DDR5 based memory                                                                                                   | /SM590000 Up to 25% more core capacity and same memory capacity /SM590000 Introducing Resource Groups to maximize system utilization                                    |
| Scale Out       | IBM Power S1122 IBM Power L1122 | /SM590000 Integrating the eSCM modules into the S1122 family - four or 10 processors per eSCM /SM590000 Up to 30 cores per dual core module /SM590000 2 processors in a 2U form factor /SM590000 Up to 4 TB - DDR5 based memory | 2S2U /SM590000 Up to 50% more core capacity /SM590000 Same memory /SM590000 Introducing Resource Groups to maximize system utilization                                  |

## Investment protection

IBM Power11 systems are designed with strong investment protection in mind, ensuring that customers can evolve their infrastructure without sacrificing prior investments. A key example of this is the ability to perform same serial number upgrades from the IBM Power E1080 to the new Power E1180 and from the Power E1050 to the Power E1150. This upgrade path allows

organizations to retain their existing system identity, simplifying software licensing, asset tracking, and operational continuity.

As part of this upgrade process, customers can migrate existing memory modules and I/O adapters from their existing systems, preserving valuable hardware investments. This compatibility reduces the cost and complexity of transitioning to the latest technology, while still gaining the performance, scalability, and security benefits of the Power11 architecture.

Additionally, IBM supports Power Enterprise Pools (PEP) that include both Power10 scale-out servers and Power11 scale-out servers. This mixed-system support enables organizations to gradually migrate workloads from older to newer systems within the same resource pool. It allows for dynamic sharing of processor and memory activations across both generations, providing flexibility in capacity planning and workload placement. This approach not only protects existing investments but also enables a smooth, non-disruptive path to modernization.

## Portfolio simplification

One of the most notable changes in the Power11 generation is the streamlining of the scale-out server lineup. IBM has reduced the number of scale-out models, simplifying customer choices and reducing complexity in deployment and support. This focused approach makes it easier for organizations to select the right system for their needs while still benefiting from the full-stack integration and innovation that IBM Power is known for.

Figure 1-4 shows how customers can migrate from their existing IBM POWER9fi or Power10 servers to the new Power11 portfolio.

Figure 1-4   Migration path POWER9 to Power11

<!-- image -->

## 1.4  Power11: Trusted, autonomous, and modern

Our vision for Power is autonomous IT, where Power11 processor-based servers can think, heal and scale on their own. This in turn supports business agility so that enterprises can respond more quickly to market changes and customer demands, rapidly deploy new services, and reduce the time to market for new products.

We are working towards this vision by building automation into the platform, and combining that automation with AI-infused workflows.

This automation spans the day-0 to day-2 IT lifecycle for Power11 processor-based servers and includes capabilities such as Infrastructure as Code with Terraform, Application Configuration Management with Red Hat Ansible and enhancements to existing platform tools such as the Hardware Management Console and Cloud Management Console. On the AI side we are building AI infused workflows with IBM watson.x and new tools like IBM Concertfi.

This section provides insights into some of the technologies and solutions that make Power11 a trusted, autonomous, and modern platform to support your business requirements.

## 1.4.1  Building a trusted infrastructure with Quantum Safe Encryption

Infrastructure built using IBM Power processor-based servers benefits from the robust security technologies integrated into both the hardware and software stacks. Power processor-based servers offer advanced security features at every level of the system, ensuring comprehensive protection for sensitive data and applications. These features include advanced encryption technologies, secure boot capabilities, and integrated firmware updates. Additionally, Power processor-based servers leverage IBM's extensive expertise in securing mission-critical workloads, making them a popular choice for organizations seeking a secure environment for their digital assets.

Workloads on Power11 processor-based servers see significant benefits from improved cryptographic accelerator performance compared to previous generations. Specifically, the Power11 chip supports accelerated cryptographic algorithms such as AES, SHA2, and SHA3, resulting in considerably higher per-core performance for these algorithms. This enhancement allows features like AIX Logical Volume Encryption to operate with minimal impact on system performance.

The processor-core technology of Power11 incorporates integrated security protections aimed at:

- /SM590000 Improved Cryptographic Performance
- Integrated cryptographic support reduces the performance impact of encrypting and decrypting your data, allowing you to make encryption pervasive to protect all your critical data.
- /SM590000 Increased Application Security:
- Hardened defenses against return-oriented programming (ROP) attacks.
- /SM590000 Simplified Hybrid Cloud Security
- Easy-to-use, setup-free hybrid cloud security administration with a single interface.

- /SM590000 Enhanced Virtual Machine Isolation

Providing the industry's most secure virtual machine isolation technology, this technology defends against attacks that exploit operating system or application vulnerabilities in the virtual machine to access other virtual machines or the host system.

## Encryption technologies and their applications

Power11 emphasizes comprehensive security throughout its design, offering multiple encryption options. Key among these is Transparent Memory Encryption (TME), Fully Homomorphic Encryption (FHE), and Quantum-Safe Encryption (QSE).

## Transparent Memory Encryption

Transparent Memory Encryption (TME) encrypts data in memory to protect it from unauthorized access and tampering during runtime. Operating at the hardware level, TME utilizes the Power11 processor's cryptographic engines to perform encryption and decryption tasks efficiently. TME ensures pervasive protection of data in memory with minimal impact on system performance, as encryption and decryption are managed at the chip level. Its integration into normal operations is seamless and automatic.

## Fully Homomorphic Encryption

Fully Homomorphic Encryption (FHE) allows computations to be performed directly on encrypted data without decrypting it first. This ensures that sensitive data remains confidential even during processing. FHE operates at the software level and involves sophisticated mathematical algorithms to enable computations on ciphertexts.

Implementing FHE requires specialized libraries and frameworks. However, FHE is computationally intensive and can introduce performance overhead compared to conventional hardware-only encryption methods due to the complexity of the algorithms.

## Quantum-Safe Encryption

Quantum-Safe Encryption (QSE) is designed to be resistant to quantum attacks, securing data against the computational capabilities of future quantum computers that could potentially break current cryptographic algorithms. QSE employs cryptographic algorithms believed to be resistant to quantum attacks, such as lattice-based, hash-based, and multivariate-quadratic-equation-based cryptography.

Many quantum-safe algorithms are still undergoing testing and standardization to ensure they provide robust security in the face of future quantum advancements. QSE is typically used for securing long-term data, sensitive communications, and critical infrastructure.

The relevant features and differences in these technologies is shown in Table 1-4

Table 1-4   Key Differences

| Feature              | TME                     | FHE                                                      | QSE                                                      |
|----------------------|-------------------------|----------------------------------------------------------|----------------------------------------------------------|
| Encryption Scope     | Secures data in memory  | Allows computations on encrypted data                    | Prevents against future quantum computing threats        |
| Implementation Level | Implemented in hardware | Implemented using a combination of hardware and software | Implemented using a combination of hardware and software |

| Feature            | TME                                                                                                            | FHE                                                                             | QSE                                                                                                                              |
|--------------------|----------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|
| Performance Impact | Hardware accelerated through the Power11 cryptographic engines and designed to have minimal performance impact | Involves substantial computational overhead                                     | The impact of QSE varies; some quantum-safe algorithms may introduce performance overhead; this is a subject of ongoing research |
| Use Cases          | Used to protect data in memory                                                                                 | Used for performing secure computations on sensitive data without decrypting it | Provide long-term data protection and secure communications in the future                                                        |

## Quantum Safe compliance

Quantum-Safe Encryption, also known as Post-Quantum Cryptography (PQC), refers to encryption methods that are secure against both classical and quantum computers. As quantum computers advance, they may pose a threat to existing cryptographic systems, potentially compromising their security. Quantum-Safe Encryption (QSE) is essential for protecting sensitive data, communication channels, and user identities in the age of quantum computing.

The urgency of adopting QSE stems from two primary concerns.

- -Advanced quantum computers could allow adversaries to intercept, and decrypt protected digital communications through Harvest Now, Decrypt Later (HNDL) strategies, even before reaching Q-Day. Q-Day is the anticipated point in time when quantum supremacy becomes widespread and many of the current encryption algorithms are no longer effective.
- -Transitioning to QSE might require over a decade due to the complexities of organizational structures and IT infrastructure.

Consequently, organizations should start evaluating and implementing QSE solutions immediately to ensure continued protection and maintain trust among their stakeholders.

Delaying QSE adoption could have severe consequences. Legacy cryptographic systems left unaltered could be compromised in the event of a successful quantum attack, exposing sensitive data and risking confidential business transactions and individual privacy. Financial institutions, critical infrastructure providers, and government agencies would face significant challenges in maintaining operational integrity and confidentiality. Therefore, prioritizing QSE implementation is crucial for long-term cybersecurity resilience.

Power11 is designed to support these quantum-safe algorithms, ensuring robust security even as quantum computing advances. Power11's support of Quantum-Safe features provide the following.

- /SM590000 Data Encryption Breakage Protection
- -Risk: Quantum computers could break widely used cryptographic algorithms such as RSA, ECC (Elliptic Curve Cryptography), and traditional Diffie-Hellman key exchange protocols. Shor's algorithm, for instance, could efficiently factor large integers and solve discrete logarithms, compromising the security of these algorithms.
- -Protection: Power11 is capable of supporting quantum-safe algorithms like lattice-based, hash-based, code-based, and multivariate quadratic equations-based cryptography, which are believed to be resistant to quantum attacks. The crypto engines in Power11 enhance the performance of these algorithms, ensuring secure encryption and key exchange processes with minimal performance degradation.

- /SM590000 Secure Communications
- -Risk: Quantum computers could intercept and decrypt secure communications, undermining protocols that currently rely on classical encryption methods.
- -Protection: Power11 secures communication channels with quantum-resistant protocols, ensuring data confidentiality even in the presence of quantum adversaries. End-to-end encryption is maintained throughout the data lifecycle, from storage to transmission, using algorithms resistant to quantum attacks.
- /SM590000 Data Integrity and Authenticity
- -Risk: Quantum computers could forge digital signatures or tamper with data.
- -Protection: Power11 can use quantum-safe digital signature algorithms such as XMSS (eXtended Merkle Signature Scheme) and UOV (Unbalanced Oil and Vinegar), providing strong security against quantum attacks.
- /SM590000 Long-Term Data Protection
- -Risk: Sensitive data stored today could be harvested and decrypted in the future as quantum computers become more powerful, threatening long-term confidentiality.
- -Protection: Implementing quantum-safe encryption methods ensures that data remains secure over time, even as quantum computing capabilities evolve. Power11's architecture supports updates to cryptographic libraries and protocols, enabling the adoption of new quantum-safe algorithms as they are developed and standardized.
- /SM590000 Physical and Memory Attack Protection
- -Risk: Physical attacks on memory, such as cold boot attacks, could expose sensitive data if not adequately protected.
- -Protection: Power11's Transparent Memory Encryption (TME) ensures that data in memory is encrypted, protecting it from physical attacks during runtime.

## Quantum-Safe Algorithms Supported by Power11:

The following quantum-safe algorithms are supported by Power11:

- /SM590000 Lattice-Based Cryptography
- -Algorithms: ML-DSA (Dilithium), ML-KEM(Kyber) and NTRUEncrypt.
- -Characteristics: Secure against quantum attacks. Based on lattice problems Learning With Errors (LWE), and Ring-Learning With Errors (Ring-LWE). Relatively efficient for hardware and software implementations.
- /SM590000 Hash-Based Cryptography
- -Algorithms: Merkle Signature Scheme (MSS), XMSS (eXtended Merkle Signature Scheme), and SPHINCS+.
- -Characteristics: Secure based on hash functions, though generally produces larger signatures and keys.
- /SM590000 Code-Based Cryptography
- -Algorithms: McEliece Cryptosystem, BIKE (Bit Flipping Key Encapsulation), and HQC (Hamming Quasi-Cyclic).
- -Characteristics: Quantum-resistant based on decoding random linear codes, though public keys can be large.
- /SM590000 Multivariate Quadratic Equations
- -Algorithms: Unbalanced Oil and Vinegar (UOV), Rainbow.
- -Characteristics: Secure against solving systems of multivariate quadratic equations, generally efficient in signature generation but may involve larger key sizes.

## Power11 Implementation

Power11 processors support these quantum-safe algorithms through:

- /SM590000 Crypto Engines: Multiple engines per core enable efficient execution of cryptographic operations.
- /SM590000 Software Updates: The architecture allows updates to cryptographic libraries, ensuring the integration of new quantum-safe algorithms as they become standardized.

Power11's design and capabilities ensure robust security against future quantum threats by leveraging hardware acceleration and flexible software updates, maintaining high-security standards as the cryptographic landscape evolves.

## Encryption enablement in hardware

There are two options for accelerating encryption in an IBM Power11 processor-based server. The first option is to use the built in encryption acceleration built into the Power11 chip. In addition, IBM Power supports a PCIe based encryption accelerator.

## On-chip encryption support in Power11

The Power11 processor is designed to effectively support future encryption - including Fully Homomorphic Encryption (FHE) and Quantum-Safe Cryptography - in order to be ready for the Quantum age. The Power11 processor-chip instruction set architecture (ISA) is tailored for these solutions' software libraries, which are currently available or will soon be made available in the corresponding open source communities.

Workloads on Power11 benefit from cryptographic algorithm acceleration, enabling much higher per-core performance than Power10 processor-based servers for algorithms like Advanced Encryption Standard (AES), SHA2, and SHA3. Features like AIX Logical Volume Encryption can be activated with minimal performance overhead thanks to this performance enhancement.

With four times as many AES encryption engines, Power11 processor technology is designed to offer noticeably faster encryption performance. Power11 processor-based servers are more advanced than Power10 processor-based servers, with updates for the most stringent standards of today as well as future cryptographic standards including post-quantum and fully homomorphic encryption. It also introduces additional improvements to container security. Through the use of hardware features for a seamless user experience, transparent memory encryption aims to simplify encryption and support end-to-end security without compromising performance.

## 1.4.2  Protect, detect, and recover with Power Cyber Vault

Regulatory frameworks such as the U.S. Securities and Exchange Commission (SEC) cybersecurity disclosure rules and the Digital Operational Resilience Act (DORA) are no longer distant challenges-they are active requirements. More regulations are coming, each demanding higher levels of risk management and governance, mechanisms to promptly detect anomalous activities, and resilience practices to minimize the risk of data corruption or loss. The bar is rising, and the spotlight is now on every organization's security posture.

These frameworks go beyond checklists. They are signaling a shift: cybersecurity is no longer a compliance add-on, but a foundational element of operational continuity, customer trust, and business strategy.

## Traditional Siloed Security Solutions Can No Longer Keep Pace

Most organizations still operate with fragmented security architectures - collections of tools, processes, and teams that operate in silos. While some integration has occurred over time, loosely connecting security domains still leaves critical gaps in visibility, context, and response coordination.

## A siloed approach results in:

- -Limited understanding of threats across domains (e.g., linking identity misuse with network anomalies).
- -Reactive security postures measured by containment speed-not business outcomes.
- -Barriers to collaboration and innovation, as teams struggle to securely share and access data.
- -Missed opportunities to embed privacy and security into customer-facing products and services.

Even more concerning, lack of contextual awareness across tools and data streams can open the door to full-blown compromises - like ransomware attacks that paralyze operations.

Security was never meant to be siloed. It became that way out of necessity and legacy architectures. But that model is no longer sufficient. Forward-looking enterprises are rethinking their cybersecurity strategy not just as a defense mechanism, but as a business enabler. They are integrating tools, processes, and insights across domains-from access and identity to threat detection, data governance, and compliance. They are moving from isolated defenses to connected intelligence.

## Zero Trust: A Strategic Shift, Not Just a Technology Play

Many organizations are now adopting Zero Trust frameworks as the foundation for their next-generation security programs. Built on the principles of least privilege access, continuous verification, and assumed breach, Zero Trust offers a compelling blueprint.

But the blueprint alone is not enough. Applying Zero Trust effectively requires:

- -A deep understanding of your business's risk surface.
- -Alignment with regulatory and operational requirements.
- -Integration with your existing cloud and hybrid infrastructure.
- -Cultural and organizational readiness to shift to continuous governance and access control.

In essence, Zero Trust is not a product. It is an operating model for security that aligns with the way modern businesses function-dynamically, digitally, and distributed.

## The Reality of Today's Threats: Evolving, Disruptive, Costly

Cyber attacks today are fast-moving, complex, and deeply disruptive. Here's the typical incident flow:

1. Service disruption is noticed.
2. Immediate shutdowns are initiated to contain spread.
3. Search for uncompromised backups begins.
4. Slow and costly recovery efforts start.

According to recent reports, 78% of organizations take over 100 days to fully recover from a major cyber incident. That is not just downtime - it is lost trust, revenue, and opportunity.

IBM provides a unified cyber resilient solution which includes IBM Power11 processor-based servers, IBM FlashSystems storage, advanced software, and services from IBM Technology Expert Labs services to respond to evolving cyber threats and regulatory standards with IBM Power Cyber Vault.

## IBM Power Cyber Vault

IBM Power Cyber Vault keeps the business running by combining different features:

- /SM590000 Power Cyber Vault Protected Data

Automated immutable images stored in the Power Cyber Vault to protect clients' snapshots and backups.

- /SM590000 Continuous Threat Detection

Near real-time filesystem monitoring, FlashSystem inline detection, and recovery data scanning. Threat Detection Time as low as 60 Seconds.

- /SM590000 Rapid Clean Room Response

Automated response to attacks, creating Power Cyber Vault clean rooms for testing and validation. Accelerated Recovery Response.

- /SM590000 Cyber Vault Assisted Recovery

Automated multi-level testing and validation of Power Cyber Vault images to accelerate system recovery. Return to full operations in hours versus months.

## Power Cyber Vault Solution Components

IBM Power Cyber Vault is a complete Cyber Resilient Solution deployed in as little as 30 days. It is composed of various elements:

- /SM590000 IBM Power11 processor based servers

IBM Power11 processor based servers provide:

- -Industry leading threat resistance
- -PowerSC for security and compliance tracking
- -Secure boot and memory encryption built in
- -Scalable and flexible architecture from entry to enterprise servers
- /SM590000 IBM FlashSystems Storage

IBM FlashSystems provide:

- -Immutable Copies for Data Protection
- -Ransomware Detection for real-time threat detection
- -Flash-speed recovery for rapid restore of snapshot data
- -Storage Insights Pro for comprehensive reporting
- /SM590000 IBM Technology Expert Labs Power Cyber Vault Deployment

IBM Technology Expert Labs provides:

- -Design Workshop for client specific requirements
- -Power Cyber Vault Deployment to customize and enable the unified Power Cyber Vault to meet client needs
- /SM590000 IBM Expertise Connect

IBM Expertise Connect is:

- -Primary Technical Point of Contact for Power Cyber Vault
- -Dedicated to client and client success
- -Available after installation to help guide client's directions

## Power Cyber Vault Implementation

The implementation of the solution consists of several phases, each designed to support risk mitigation and ensure the fastest possible recovery

1. Identify - Assess Risks and Design Mitigation Solutions
- a. IBM Securityfi and Resilience Assessment

This assessment provides:

- A view of the organization, processes, and strategies regarding operational and cyber resilience
- A list of prioritized recommendations to improve overall resilience
- b. IBM Cyber Vault Solution and Design Workshop

This workshop provides:

- Create detailed solution based on customer cyber resiliency goals, infrastructure, and required services
2. Protect - Safeguarded Cyber Vault Copies and Backups

Client specific scheduled creation and protection of immutable snapshots and backups to meet RPO and RTO requirements:

- -Snapshots: FlashSystems and Storage Defender CSM save protected system snapshot state into the Cyber Vault on a regularly scheduled basis.
- -Orchestration for crash consistency
- -Application specific customization available
3. Detect - Real-Time Threat Detection

Monitoring real-time threats and cyber attack vectors to respond quickly as threats emerge via Power SC Real Time Compliance orchestration:

- -Zero Trust Execution to detect, prevent, and report unapproved executables and applications from running on Power11 based-processor servers
- -FlashSystem's integrated Ransomware Threat Detection - alerts on active attacks and reports to PowerSC via Storage Insights Pro
- -SEIM Integration to accept alerts and threat reports from SEIM systems
4. Respond and Recover - Real Time Threat Response

Routinely, or when threats are detected, a response phase is triggered to audit and recover the system:

- -Upon receiving an integrity alert, IBM Power Cyber Vault automatically initiates the validation process by creating an immutable backup of the VM within seconds and spinning up a copy of that VM in the clean room within minutes.
- -Various types of Integrity Checks are run in the clean room (including platform specific tests).
- -Power Cyber Vault reports corrupted copies, and the newest clean copies are found.
- -When approved by the administrator, recovery to production will commence to recover and help meet RPO and RTO requirements.

## 1.4.3  Quantum-safe compliance with one-click inventory discovery

IBM PowerSC is a comprehensive security and compliance solution designed specifically for virtualized environments running on IBM Power Systems with AIX, Linux, and IBM i. It integrates deeply with the Power platform to provide end-to-end protection through features such as Security and Compliance Automation, Trusted Boot, Trusted Firewall, and Trusted Logging.

PowerSC helps organizations meet stringent regulatory standards like PCI DSS, HIPAA, and SOX by automating the monitoring, auditing, and enforcement of security policies. It also includes advanced capabilities like multi-factor authentication, intrusion detection, and patch management. With tools like PowerSC Trusted Surveyor, it ensures consistent network configuration and compliance across dynamic virtual environments, helping reduce administrative overhead while maintaining strong security postures

IBM is actively involved in the development and adoption of quantum-safe cryptographic standards, including working with NIST on their Post-Quantum Cryptography Standardization project. PowerSC supports a seamless transition to NIST's forthcoming standards.

IBM PowerSC offers capabilities to help verify and manage quantum-safe encryption.

The Quantum Safety Analysis feature analyzes the quantum-safe status of your VMs to identify areas of concern, to ensure compliance with emerging quantum-safe cryptographic standards:

- /SM590000 PowerSC scans endpoints on your server to identify cryptographic artifacts and vulnerabilities related to quantum safety. These can also be scheduled.
- /SM590000 It creates an inventory of where different encryption algorithms are implemented on your system.
- /SM590000 The analysis estimates the strength of ciphers, certificates, and keys on your AIX endpoints and categorizes them as weak, strong, quantum safe, or unclassified. Note that PowerSC is a security and compliance management tool, not a cryptographic module validation tool.
- /SM590000 A Quantum Safety Analysis report in the PowerSC GUI lists the discovered cryptographic elements and their assessed strengths.

## 1.4.4  Zero planned downtime and end-to-end orchestration for maintenance events

Planned downtime remains one of the biggest challenges in IT operations - especially for critical infrastructure as Power11 is. IBM's approach is to deliver Autonomous IT by embedding extreme automation and AI-infused workflows directly into the platform. The result: a significant reduction in operational effort, minimized disruptions, and improved resilience.

Today's enterprise IT environments face increasing pressure to ensure infrastructure reliability and security without disrupting operations. Key challenges include:

- /SM590000 Operational Complexity: Maintaining firmware, I/O adapters, and Virtual I/O Servers (VIOS) is highly specialized and intricate.
- /SM590000 Service Interruptions: Routine maintenance often requires downtime, impacting application availability and business continuity.
- /SM590000 Security Risks: Delayed updates expose systems to vulnerabilities (CVEs), weakening compliance and increasing risk.
- /SM590000 Resource Drain: IT teams spend 20-40% of their time on maintenance tasks like patch evaluation, compatibility testing, and scheduling.
- /SM590000 High Downtime Costs: With downtime averaging $336,000 per hour, annual maintenance-related outages can exceed $2 million per system.

These issues force organizations into a difficult trade-off: stay current and risk downtime, or delay updates and increase exposure - an unsustainable model in today's always-on digital economy.

## What We Mean by 'Infrastructure'

In this context, infrastructure refers to the Power11 platform, encompassing firmware, I/O components, VIOS, and the operating system. To maintain performance and security, clients must:

- /SM590000 Apply patches and updates regularly.
- /SM590000 Remain on supported versions.
- /SM590000 Respond swiftly to security advisories.
- /SM590000 Conduct preventive maintenance.

However, these actions typically require planned downtime. The core dilemma: should businesses accept regular service interruptions to stay current, or defer updates and risk security and performance?

## IBM Power11: A Platform for Autonomous IT

IBM Power11 is designed to resolve this dilemma - enabling continuous business operations while keeping infrastructure secure, up-to-date, and high-performing. IBM Power11 introduces a transformative approach to infrastructure lifecycle management, purpose-built for Autonomous IT. The platform is designed to operate with minimal human intervention, leveraging embedded intelligence to monitor, diagnose, and remediate issues in real time. This enables infrastructure that is inherently resilient, adaptive, and capable of maintaining continuous service availability.

At the core of Power11 is the Automated Maintenance Framework, orchestrated by IBM Concert, an AI-driven engine that automates the full maintenance lifecycle. This includes:

- /SM590000 Unified update orchestration for system firmware, VIOS, and I/O adapters.
- /SM590000 Live Partition Mobility (LPM) to ensure uninterrupted application availability during updates.
- /SM590000 Pre-validation checks to confirm system readiness and compatibility.
- /SM590000 Flexible update sourcing from IBM repositories, SFTP, NFS, USB, or HMC-local filesystems.

Maintenance operations can be executed autonomously or manually via the Hardware Management Console (HMC), offering both automation and administrative control. This architecture transforms maintenance from a disruptive, manual process into a seamless, background operation.

## IBM Concert: AI-Powered Application Management for the Modern Enterprise

IBM Concert addresses the persistent challenge of siloed application data by harnessing the power of AI to deliver intelligent, prioritized recommendations. By streamlining issue identification and resolution, Concert significantly reduces Mean Time to Resolution (MTTR) for critical risk factors such as:

- /SM590000 Critical Vulnerability Exploits (CVEs)
- /SM590000 Certificate management issues
- /SM590000 Application compliance challenges

## What Sets IBM Concert Apart

Built on IBM watsonx, Concert uses advanced generative AI to analyze complex environments and deliver actionable insights.

- /SM590000 Data-Agnostic Integration

Designed to meet you where you are, Concert supports a wide range of data sources across networks, infrastructure, and application architectures.

- /SM590000 Hybrid Cloud by Design

Tailored for the realities of modern enterprise IT, Concert seamlessly supports hybrid cloud environments.

- /SM590000 AI-Infused Orchestration for Proactive Maintenance
- IBM Concert orchestrates maintenance operations through intelligent workflows and end-to-end automation, including:
- /SM590000 Inventory and Risk Discovery

Automatically identifies assets and uncovers hidden risks across the environment.

- /SM590000 Risk Analysis and Remediation Planning
- Delivers prioritized, actionable plans to address vulnerabilities and compliance gaps.
- /SM590000 Automated Risk Resolution

Enables triggering of automated actions to remediate issues, reducing manual effort and downtime.

Automation can be triggered directly from IBM Concert or from the HMC, ensuring flexibility and ease of use.

## Key Benefits of IBM Power11's Autonomous Maintenance

IBM Power11's integrated solution for Zero Planned Downtime (ZPD) delivers measurable benefits across operational, security, and financial dimensions:

- /SM590000 Business Continuity: Updates are applied without interrupting workloads, preserving uptime and service-level commitments.
- /SM590000 Operational Efficiency: Automation streamlines the maintenance lifecycle, reducing manual effort and administrative overhead.
- /SM590000 Risk Reduction: AI-driven orchestration ensures timely updates, minimizing exposure to vulnerabilities and compliance risks.
- /SM590000 Productivity Gains: IT staff benefit from a simplified, intelligent interface that reduces time-on-task from weeks to minutes.
- /SM590000 Cost Optimization: Eliminating downtime and manual labor significantly lowers the total cost of infrastructure operations.

Together, these capabilities position IBM Power11 as a foundational platform for secure, scalable, and self-managing enterprise IT infrastructure - aligned with the demands of modern digital transformation.

## The New Maintenance Model

From the IT administrator's perspective, maintenance is now part of a unified, intelligent flow:

- /SM590000 Inventory discovery and risk detection
- /SM590000 Smart planning
- /SM590000 Automated download and application of updates
- /SM590000 Validation
- /SM590000 VM evacuation and reintegration

This approach minimizes disruptions, ensures SLA compliance, and allows faster more frequent maintenance. Systems stay secure, stable, and aligned with evolving requirements - without downtime or performance impact.

Power11's built-in Automated Maintenance Tool enables platform maintenance without application impact. This automation can be triggered via IBM Concert's orchestration, or it can be

human triggered from the Hardware Management Console (HMC), completely independent of IBM Concert.

The automated maintenance tool provides:

- /SM590000 Ability to update System FW, VIOS and IO adapter from a single update flow
- -Supported for both concurrent and disruptive updates
- /SM590000 Validation for LPM and VIOS redundancy (VIOS maintenance readiness check)
- /SM590000 Ability to automatically migrate partitions and return as part of the update process
- -Option to choose to return to the source or leave on the target system
- -Option to evacuate all LPARs / choose a subset of LPARs and order of LPARs
- -Option to choose order of updates
- /SM590000 Option to either download only or download and update

Different sources of updates

- -IBM Website (preferred and recommended for end-to-end automation)
- -SFTP Server
- -HMC filesystem
- -NFS Server
- -USB

The new tool appears as a Licensed Capability of the server as shown in Figure 1-5.

Figure 1-5   Power System Licensed Capabilities

<!-- image -->

For more detail on using the new automated maintenance tool, see section 5.1.7, 'Using the Automated Maintenance tool' on page 109.

## 1.4.5  Automated data collection for faster error resolution

Enterprises are increasingly adopting hybrid cloud environments, with 82% reporting usage as of 2024 (IBM Cloud Survey). This shift introduces significant operational complexity, particularly in managing infrastructure and resolving issues. Manual error resolution continues to overwhelm IT teams, with 60% of organizations citing insufficient staffing for incident management. Moreover, the financial impact of downtime is escalating-over 90% of mid-size and large enterprises report that a single hour of downtime now exceeds $300,000 in cost.

To address these challenges, IBM has introduced enhanced support capabilities in the Power11 Hardware Management Console (HMC). This includes a streamlined case creation process and automated log collection via Call Home. Administrators can now initiate support cases directly from the HMC, with relevant FFDC (First Failure Data Capture) logs automatically gathered and transmitted to IBM Support.

## Use Case: Automated Support Workflow

Consider Louis, a system administrator at a large insurance company running critical backend applications on IBM Power infrastructure. During a routine maintenance task involving live partition mobility, Louis encounters a migration validation error. Previously, resolving such issues required manually opening a support case, coordinating with IBM Support, and collecting FFDC data-a process that could take 5 to 8 hours before investigation could begin.

With Power11, Louis can now open a support case directly from the HMC. The system automatically collects and submits the necessary diagnostic data, eliminating manual steps and reducing turnaround time. IBM Support leverages AI-enhanced diagnostic tools trained on Power processor-based servers to accelerate root cause analysis and resolution.

## Benefits of the Enhanced Support Model

Implementing this new automated data collection capability in conjunction with applied AI workflows in IBM Support results in:

- /SM590000 Reduced Time to Resolution: Automated log collection and AI-assisted diagnostics accelerate issue resolution.
- /SM590000 Improved Productivity: System administrators save hours by avoiding manual data gathering and coordination.
- /SM590000 Lower Operational Risk: Faster resolution minimizes downtime and its associated business impact.
- /SM590000 Enhanced Support Experience: A simplified, integrated workflow improves responsiveness and reduces stress for IT teams.

## 1.4.6  Smart energy scheduling options with increased efficiency and reduced energy consumption

In recent years, sustainability has become a central strategic priority in the IT industry due to regulatory pressure and business value. Over the past five years, there has been an 800% increase in climate commitments among companies, indicating broader awareness and the realization that sustainable practices can enhance profitability. Currently, 83% of organizations are investing in R&amp;D for low-carbon products and services, and products with sustainability attributes are achieving revenue growth rates over 25% higher than traditional ones.

Although 95% of companies have established operational sustainability goals, only 41% have achieved measurable progress. Many companies are facing challenges in starting the implementation process. This indicates a need for more explicit guidance, enhanced governance frameworks, and improved integration of sustainability metrics into core business operations.

Power processor-based servers have consistently improved energy efficiency per watt with each generation, and Power11 continues this trend. Power11 enhances efficiency, cutting energy use, carbon emissions, and the datacenter's energy footprint.

POWER9 clients upgrading to Power11 can reduce energy consumption by up to 60% for the same performance. Power11 outperforms x86 systems, providing twice the performance per watt. Compared to Power10, Power11 offers up to 33% better performance per watt, with the S1022 showing the largest gain.

Figure 1-6 illustrates natural IT efficiency improvements from POWER9 to Power11, offering up to a 3:1 consolidation ratio, reducing energy use and carbon emissions by approximately 60%, and cutting datacenter footprint by two-thirds.

Figure 1-6   POWER9 to Power11 energy consumption reduction

<!-- image -->

Along with the base energy savings delivered with Power11, there are some exciting new features that are introduced to address some of the other feedback received from clients.

## Partition level monitoring

In mid-2024, a new monitoring and reporting capability was introduced through HMC to track energy usage, carbon emissions, and other environmental factors. Power11 enhances this capability by providing insights at the partition level as shown in Figure 1-7.

Figure 1-7   Partition level monitoring

<!-- image -->

## Energy Efficient Mode

Power11 has introduced a new Energy Efficient mode that, when enabled, can reduce energy consumption by up to 30%, with only an approximate 10% reduction in performance. This impact will vary depending on the system and configuration. The power mode can be dynamically established through the HMC GUI, as illustrated in Figure 1-8.

Figure 1-8   Power modes

<!-- image -->

In the environmental dashboard, it is observed that in maximum performance mode, energy consumption remains almost constant even when core usage decreases.

Figure 1-9 shows the energy usage when the processor is set to maximum performance.

Figure 1-9   Energy consumption in max performance mode

<!-- image -->

Switching to energy efficiency mode, which is implemented instantaneously, results in only a 10% reduction in performance, while significantly decreasing energy consumption by up to 30%, as illustrated in Figure 1-10.

Figure 1-10   Energy efficient mode

<!-- image -->

## Power Mode Scheduling

If there is a pattern in system use, the new scheduling power mode feature can be utilized. This enables the system to activate Energy Efficient mode during designated times.

As illustrated in Figure 1-11, the Energy Efficient mode will be engaged over the weekend, starting from 8:00 PM on Friday until 5:00 AM on Monday.

Figure 1-11   Power mode scheduling

<!-- image -->

The new environmental dashboard enables energy savings whenever potential opportunities arise. The automated scheduling capabilities allow the system to transition into and out of various modes efficiently and appropriately.

## 1.4.7  Superior performance of up to 25% improvement for consolidation of Oracle 19c Workloads

At the time of writing, Oracle Database 19c is the latest long term release and version 19.12 is the latest Release Update for the AIX operating system. Oracle Database 19c includes many features over its previous database versions. Oracle Database versions for IBM AIX are upward compatible with the higher versions of AIX Technology Levels (TL) within the same major version. For certification information of Oracle Database with IBM AIX, refer My Oracle Support portal: https://support.oracle.com / in the ' Certification ' section.

Starting with Oracle Database 11g Release 2 (11.2.0.4), Oracle Database Instant Client is supported on Linux on Power (32-Bit and 64-Bit). The Instant Client version of the Oracle Database is also supported on the 19c version of the database. With Oracle Database 12c Release 1, support for little-endian was introduced for the Instant Client running on Linux on Power and with 12c Release 2 support is for little-endian only.

Oracle Database Instant Client is a light version of Oracle Database Client Binaries. Oracle Database Instant Client is available on Linux on IBM Power to allow an application deployed on Linux on Power to connect to the Oracle Database no matter what platform the database is deployed on (AIX/Power or any other platform).

Although servers based on IBM POWER8 processor technology have reached end of support, many customers have yet to benefit from a technology refresh. IBM has already demonstrated and published that migrating Oracle Database workloads from a server with IBM POWER8 processors to one with IBM Power11 processors can halve the processor core requirements. The reduction in cores required enables a potential reduction in Total Cost of Ownership (TCO), as it may be possible to reduce the number of required licenses. The IBM Power11 processor introduces significant performance advancements, enabling up to 25% improvement in the

consolidation of Oracle Database 19c workloads compared to the previous IBM Power10 processor generation. This performance gain allows organizations to further optimize infrastructure efficiency, reduce software licensing costs through better core utilization, and improve overall system throughput, all while having the best level of enterprise-grade reliability, availability, and serviceability (RAS) features.

Oracle is committed to release their latest products on IBM Power, to see the Oracle statement about support for 23ai on IBM Power see https://community.ibm.com/community/user/blogs/doug-bloom/2024/05/29/whats-the-late st-with-oracle-db-23ai-and-poweraix.

## 1.4.8  Day 1 availability of Power11 in PowerVS with Hybrid Billing

As hybrid cloud adoption reaches 82% among enterprises and the number of applications is projected to surpass 1 billion by 2028, organizations are under pressure to manage growing complexities, skill shortages, and escalating infrastructure costs. IBM Power11, in combination with IBM Power Virtual Server (PowerVS), offers a compelling solution to these challenges by delivering a secure, scalable, and high-performance hybrid cloud platform purpose-built for mission-critical workloads.

A key differentiator is the day-one availability of Power11 in both public and private PowerVS environments. This immediate access allows enterprises to deploy virtual servers for development, testing, production, and disaster recovery without delay. With provisioning times under 10 minutes, organizations can accelerate time-to-value and reduce migration risk through consistent architecture and user experience across on-premises and cloud environments.

Utilizing Power11 in PowerVS also unlocks advanced enterprise-grade security features. These include quantum-safe firmware signing, encrypted Live Partition Mobility (LPM), and a significantly lower number of hypervisor vulnerabilities (CVEs) compared to x86 alternatives. These capabilities are essential for organizations operating in regulated industries or managing sensitive data. Furthermore, PowerVS integrates natively with IBM watsonx services via Satellite™ Connector, enabling enterprises to seamlessly infuse AI into their workloads and accelerate digital transformation initiatives.

PowerVS now has three generations of Power available:

## /SM590000 POWER9

- -Soft landing for POWER9 clients coming to cloud
- Substantial support life remaining (through Oct. 2028)
- -Lowest cost
- Special pricing on existing capacity
- -Clients can continue to provision and use POWER9

## /SM590000 Power10

- -Work horse for production applications such as SAP RISE
- -Broad capacity availability across data centers
- -Excellent price/performance for AIX and IBM i workloads
- -Clients can continue to provision and use Power10

## /SM590000 Power11

- -No need to wait to receive Power11 on premises. Gain immediate access to Power11 via PowerVS
- -Provision in &lt;10 minutes
- -Leverage for dev/test, application qualification
- -First cloud environment to provide Power11 both in IBM Cloud and on-premises (Private)

From a business value perspective, PowerVS delivers up to 25% faster SAP workload migration compared to x86 platforms, thanks to architectural consistency and superior compute performance. Flexible consumption models, shared processor pools, and tiered storage options help reduce total cost of ownership (TCO), while automated disaster recovery, secure backup, and non-disruptive maintenance enhance operational efficiency. The platform also supports regulatory compliance with standards such as SOC2, HIPAA, and PCI-DSS.

Common use cases include application modernization through access to over 250 IBM Cloud services, enhancing business resiliency with warm disaster recovery environments, optimizing data center operations by consolidating or exiting physical infrastructure, and supporting SAP transformation initiatives with unmatched availability and scalability for large-scale HANA deployments.

PowerVS is currently available in 22 IBM Cloud data centers globally for public cloud use and can also be deployed in dedicated private cloud environments at client sites to meet stringent regulatory or operational requirements. Power11 will initially be available in four global data centers and will be fully enabled for automated disaster recovery operations. This broad availability ensures that organizations can adopt Power11 in a way that aligns with their specific business goals and compliance mandates.

## 1.4.9  Off-chip AI acceleration with IBM Spyre

IBM's Spyre™ accelerator card is a low-power, high-efficiency hardware module designed to complement Power11 processor-based servers for AI inference, analytics, and memory-intensive workloads. This feature is planned to be available on the Power E1150 in fourth quarter of 2025.

The card is a system on a chip architecture optimized for enterprise AI workloads, with 32 low-power AI cores, and supports multi-precision for inference and training: FP16/8, INT8/4.

The card is enabled for foundation models and it is planned to support the Red Hat software stack (OCP AI). It will also support popular AI Frameworks and libraries (PyTorch, vLLM). Implemented in 5nm technology, the IBM Spyre card offers high performance and low-power characteristics with only 75W of power consumed.

Figure 1-12   Spyre card physical view

<!-- image -->

Since each IBM Spyre card operates at only 75 watts, it significantly reduces the power footprint compared to traditional GPU-based acceleration, which often exceeds 300 watts per

device. IBM Spyre card is planned to be supported only inside the accelerator expansion drawer. Each accelerator expansion drawer supports a maximum of eight Spyre cards. Accelerator expansion drawer support on E1150 is as follows:

- /SM590000 At initial availability, a maximum of one accelerator expansion drawer will be supported on a Power11 processor-based server;
- /SM590000 Support for additional expansion drawers is planned in 2026.

Note: The Spyre card will require an updated fan-out card in the expansion drawer.

Unlike GPUs, which typically rely on high-bandwidth memory integrated on the card itself, the Spyre card is designed to access and leverage the system's main DDR5 memory modules. This enables the use of terabytes of high-performance memory available within the Power11 processor-based server. This architectural choice provides two main advantages:

- /SM590000 lower latency data access, thanks to the shared memory model;
- /SM590000 elimination of memory duplication, avoiding the need to copy data between host and device memory, as is common with discrete GPUs.

The use of system RAM not only simplifies software development and memory management but also contributes to a more energy-efficient and thermally balanced system architecture, an increasingly important factor in modern data centers and edge environments.

This design makes the Spyre card particularly well-suited for workloads that require frequent access to large memory spaces with minimal energy cost, such as real-time analytics, AI inferencing, and high-throughput data pre-processing.

Spyre card minimum supported levels of software stack for general availability and support on Power11 processor-based server in fourth quarter of 2025 are as follows:

- /SM590000 FW1110.10;
- /SM590000 HMC1111;
- /SM590000 RHEL 9.6;
- /SM590000 Spyre software stack container;
- /SM590000 OpenShift AI (Tech Preview 4Q 2025, GA 1Q 2026).

See Chapter 4, 'Artificial Intelligence Support' on page 93 for more detailed information about use cases and solutions with Spyre card.

## 1.4.10  Red Hat OpenShift with support for expanded software ecosystem

Red Hat OpenShift on Power provides a robust, enterprise-grade container platform that empowers financial institutions to modernize applications, deploy AI workloads, and integrate with a broad ecosystem of software. By running OpenShift on Power11 systems, organizations can take advantage of the platform's performance, scalability, and security while maintaining architectural consistency across hybrid cloud environments.

One of the key advantages of OpenShift on Power is its support for a significantly expanded software ecosystem. This includes IBM Cloud Paks such as Cloud Pak for Data and Cloud Pak for Integration, Red Hat Runtimes like Quarkus and Spring Boot, and a wide range of open-source AI/ML frameworks including PyTorch, TensorFlow, and ONNX. Additionally, many independent software vendor (ISV) applications are now certified for the Power architecture, allowing banks to run containerized workloads alongside traditional AIX or IBM i systems. This co-location reduces latency, improves data gravity, and simplifies integration.

OpenShift on IBM Power also supports seamless hybrid cloud integration, particularly when deployed on IBM Power Virtual Server (PowerVS). This enables access to IBM Cloud

services such as IBM Cloud Object Storage, IBM Key Protect, IBM Event Streams (Kafka), and IBM API Connectfi. These capabilities are essential for hybrid architectures where AI models may be trained on-premises and deployed in the cloud, or vice versa, depending on data locality and compliance requirements.

For developers and IT architects, OpenShift on Power offers a rich set of tools and resources. The OpenShift Container Catalog provides pre-built images and Helm charts optimized for Power, while infrastructure-as-code templates using Terraform simplify cluster provisioning on PowerVS. IBM also supports the ecosystem with community channels, technical workshops, and IBM Garage™ engagements to accelerate adoption.

By combining the strengths of IBM Power hardware with the flexibility of OpenShift and the breadth of the container ecosystem, financial institutions can build and scale AI-driven applications with confidence, agility, and enterprise-grade resilience.

## 1.5  IBM Power11 processor architecture

The Power11 processor is designed to deliver higher clock speeds and adds up to 25% more cores per processor chip than comparable IBM Power10 systems. The Power11 processor builds upon the key capabilities we delivered with Power10 including stronger reliability, availability, and serviceability (RAS) characteristics, better energy efficiency, improved energy management, and improved quantum-safe security.

Beyond the processor itself, we are introducing the following improvements to packaging, memory architecture and AI acceleration:

- /SM590000 Packaging Innovation

The Power11 processor will leverage new integrated stack capacitor (ISC) technology and advanced 2.5D packaging along with innovations in cooling such as improved heatsinks and more efficient fans to optimize energy delivery, improve thread and core strength and increase system capacity.

- /SM590000 Enhanced System Architecture:

The robust memory architecture for Power11 systems will be based on the recently released DDR5 DDIMMs and enhanced OMI interfaces which enable improved memory reliability, capacity, and bandwidth. Given that OMI is technology agnostic, the Power11 portfolio built on the Power11 processor will also support OMI DDR4 memory migrated from Power10 high-end systems, enabling clients to protect their investments in memory technology.

- /SM590000 AI Acceleration

We continue to support a range of emerging enterprise AI use cases with the MMA architecture. Improvements to Power11 processor core strength and system capacity will improve the performance of the MMA for inferencing workloads. Furthermore, IBM intends to incorporate the IBM Spyre accelerator into Power11 offerings to provide additional AI inferencing capabilities. Working together, IBM Power processors and the IBM Spyre accelerator will enable the next generation infrastructure to scale demanding AI workloads for businesses.

## New P11 Processor Details

Here are some details on the new Power11 processor design:

Here are some details on the new Power11 processor design:

## Technology and Packaging:

- -654 mm 2  7nm refined Samsung (30 Billion Devices)
- -Advanced 2.5D packaging technology optimizes space and improve ssignal integrity.
- -Integrated Stack Capacitor technology provides enhanced power delivery and performance by integrating capacitors directly into the chip package
- -Single-chip or Dual-chip sockets

## Computational Capabilities:

- -Up to 16 SMT8 Cores (2 MB L2 Cache / core)
- -Up to 128 MB L3 cache (low latency non-uniform cache architecture management)
- -Enterprise performance focus:
- 3x core performance relative to Power9
- 2x thread strength relative to Power9
- 4x L2 cache, 4xMMU/ core relative to Power9
- 4x crypto engine / core relative to Power9
- -AI computational Focus:
- 2x general SIMD / core relative to Power9
- 4x matrix SIMD / core relative to Power9
- New AI Instructions and data types

## Robust Data Plane:

- -2 TB/s raw (32GT/s) PowerAXON + OMI signaling
- -SMP interconnect for up to 16 sockets
- -2x OMI memory bandwidth relative to Power10
- -64 TB OMI DDR large system memory capacity

## Figure 1-13 shows the Power11 processor.

Figure 1-13   POWER11 Processor Diagram

<!-- image -->

Each of the cores have an L1 cache size with 64KB of data cache and 96KB of instruction cache.

## Comparing Power11, Power10, and POWER9 processors

Table 1-5 shows a comparison of Power11 to the previous generations.

Table 1-5 Comparison of the Power11 processor technology to prior processor generations

| Characteristics                                             | Power11                                                                                                              | Power10                                                                                                             | POWER9                                                                                            |
|-------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| Technology                                                  | 7 nm                                                                                                                 | 7 nm                                                                                                                | 14 nm                                                                                             |
| Die size                                                    | 654mm 2                                                                                                              | 602mm 2                                                                                                             | 693 mm 2                                                                                          |
| Processor module size                                       | 68.5 mmx 77.5 mm                                                                                                     | 68.5 mm x 77.5 mm                                                                                                   | 68.5 mm x 68.5 mm                                                                                 |
| Number of transistors                                       | 30 billion                                                                                                           | 18 billion                                                                                                          | 8 billion                                                                                         |
| Maximum cores                                               | 16                                                                                                                   | 15                                                                                                                  | 12                                                                                                |
| Maximum hardware threads per core                           | 8                                                                                                                    | 8                                                                                                                   | 8                                                                                                 |
| Maximum static frequency / high performance frequency range | 3.8 - 4.4 GHz                                                                                                        | 3.75 - 4.15 GHZ                                                                                                     | 3.9 - 4.0 GHz                                                                                     |
| L2 Cache per core                                           | 2048 KB                                                                                                              | 2048 KB                                                                                                             | 512 KB                                                                                            |
| L3 Cache                                                    | 8 MB of L3 cache per core with each core having access to the full 128 MB of L3 cache, on-chip high-efficiency SRAMa | 8 MB of L3 cache per core with each core having access to the full 120 MB of L3 cache, on-chip high-efficiency SRAM | 10 MBofL3cacheper core with each core having access to the full 120 MB of L3 cache, on-chip eDRAM |
| memory technology                                           | DDR5 and DDR4 a                                                                                                      | DDR4 and DDR5 b                                                                                                     | DDR4 and DDR3                                                                                     |
| I/O bus                                                     | PCIe Gen 5                                                                                                           | PCIe Gen 5                                                                                                          | PCIe Gen 4                                                                                        |

- a. DDR4 only as migration from Power10 during same serial number upgrade
- b. DDR5 support added after GA

## Chip packaging options

To provide flexibility and scalability, the Power11 chip is packaged in several ways. For the high end E1180, IBM utilizes a single chip module (SCM) which provides a single chip per socket and up to sixteen sockets per system. For the midrange E1150, the Power11 is packaged with two chips per socket in a Dual Chip Module (DCM) and supports up to four sockets per system, The DCM version is also used in the scale out server line in the S1122 and S1124, with up to two sockets per system. There is a third packaging option, the entry Single Core Module (eSCM), which is used in the entry level processor features of the S1122 (the 4 core and the 10 core options). The eSCM is a modified DCM where the second core in the module does not have any processor capability and only adds I/O functionality.

## The Dual Chip Module in the E1150

The IBM Power E1150 utilizes the advanced IBM Power11 processor architecture integrated as a Dual Chip Module (DCM). This DCM configuration allows each socket to house two processor chips, significantly increasing core density and performance. In the Power E1150, the DCM supports configurations ranging from 16 to 30 cores per socket, enabling high-throughput computing for enterprise workloads. This architecture is particularly beneficial for environments requiring robust virtualization, large-scale database processing, and AI model training, as it delivers enhanced parallel processing capabilities and energy efficiency.

The DCM enables the system to handle demanding workloads while maintaining a compact footprint, making it ideal for data centers with space and power constraints. The integration of Open Memory Interface (OMI) and Differential DIMMs (DDIMMs) further enhances memory performance and reliability, which is critical for mission-critical applications.

The E1150 utilizes the DCM to support IBM's broader goals of flexibility and efficiency in hybrid cloud environments. The Power11 chips in the DCM not only boost raw performance, but also support advanced features like transparent memory encryption and dynamic resource allocation. These capabilities are essential for modern IT infrastructures that demand both security and adaptability. Whether deployed in traditional data centers or as part of a hybrid cloud strategy, the DCM-equipped Power E1150 provides a solid foundation for scalable, secure, and high-performance computing. Figure 1-14 shows the logical layout of the Power11 DCM.

Figure 1-14   POWER11 DCM Diagram

<!-- image -->

## 1.6  Operating system support

The Power11 processor-based Power E1150 supports the following families of operating systems:

- /SM590000 AIX
- /SM590000 Linux
- /SM590000 Red Hat OpenShift

In addition, the Virtual I/O Server (VIOS) can be installed in special partitions that provide virtualization of I/O capabilities, such as network and storage connectivity. Multiple VIOS partitions can be installed to provide support and services to other partitions running AIX, or Linux, such as virtualized devices and Live Partition Mobility capabilities.

For more information about the Operating System and other software that is available on Power, see IBM Power. The minimum supported levels of IBM AIX, and Linux are described in the following sections.

Important: At the time of writing this Redbook, IBM i is not supported on the Power E1150 server. The operating system support mentioned above applies only to the specific versions detailed below.

IBM periodically announces support for newer versions of its operating systems. To take full advantage of the latest security enhancements, performance improvements, and new feature support, it is strongly recommended that you keep your operating systems up to date.

## 1.6.1  AIX operating system

Support for the AIX operating system is described in this section.

The Power E1150 server supports the following minimum levels of the AIX operating system when installed with virtual I/O:

- -AIX Version 7.3 with Technology Level 7300-02 and Service Pack 7300-02-02-2420
- -AIX Version 7.3 with Technology Level 7300-03 and Service Pack 7300-03-00-2446
- -AIX Version 7.2 with Technology Level 7200-05 and Service Pack 7200-05-08-2420

The Power E1150 servers support the following minimum levels of the AIX operating system when installed by using direct I/O connectivity:

- -AIX Version 7.3 with Technology Level 7300-02 and Service Pack 7300-02-04
- -AIX Version 7.3 with Technology Level 7300-03 and Service Pack 7300-03-01
- -AIX Version 7.3 with Technology Level 7300-04 and Service Pack 7300-04-00
- -AIX Version 7.2 with Technology Level 7200-05 and Service Pack 7200-05-10

Important: In addition, consider the following requirements:

- /SM590000 AIX 7.2 instances can use physical and virtual I/O adapters, but must run in an LPAR in IBM POWER9 compatibility mode.
- /SM590000 AIX 7.3 instances can use physical and virtual I/O adapters, and can run in an LPAR in native Power11 mode

## 1.6.2  Linux operating system

Support for the Linux operating system is described in this section for Power E1150. The Linux distributions that are described next are supported on the Power E1150 server model. Other distributions, including open source releases, can run on these servers, but do not include any formal Enterprise Grade support.

## Red Hat Enterprise Linux

The latest version of the Red Hat Enterprise Linux (RHEL) distribution from Red Hat is supported in native Power11 mode, which allows it to access all of the features of the Power11 processor and platform.

At announcement, the Power E1180 servers support the following minimum levels of the Red Hat Enterprise Linux operating system:

- /SM590000 Red Hat Enterprise Linux 8.10 for Power LE, or later (Power10 compatibility)
- /SM590000 Red Hat Enterprise Linux 9.4 for Power LE, or later (Power10 compatibility)

## 1.6.3  VIOS

- /SM590000 Red Hat Enterprise Linux 9.6 for Power LE, or later (Power11 native)
- /SM590000 Red Hat Enterprise Linux 10 for Power LE, version 10.0, or later
- /SM590000 Red Hat Enterprise Linux for SAP with Red Hat Enterprise Linux 9 for Power LE version 9.6, or later

## Red Hat OpenShift Container Platform

The Red Hat OpenShift Container Platform is supported to run on IBM Power servers, including the Power E1150. To run OCP in native mode on Power11, you need to run OCP 4.19 which is based off of Red Hat Enterprise Linux 9.6. Previous versions will run in Power10 compatibility mode.

The Power S1122 and S1124 servers support the following minimum levels of the operating systems that are supported for Red Hat OpenShift Container Platform:

- /SM590000 Red Hat Enterprise Linux CoreOS 4.18 (Power10 compatibility)
- /SM590000 Red Hat Enterprise Linux CoreOS 4.19 (Power11 native)

## SUSE Linux Enterprise Server

The latest version of the SUSE Linux Enterprise Server distribution of Linux from SUSE is supported in native Power11 mode, which allows it to access all of the features of the Power11 processor and platform.

At announcement, the Power E1180 servers support the following minimum levels of the SUSE Linux Enterprise Server operating system:

- /SM590000 SUSE Linux Enterprise Server for SAP with SUSE Linux Enterprise Server 15, Service Pack 6, or later
- /SM590000 SUSE Linux Enterprise Server 15, Service Pack 6, or later

The minimum required level of VIOS for the Power E1150 is VIOS 4.1.1.10 when running in Power11 mode. The following levels are supported:

- /SM590000 VIOS 4.1.1.10 (Service Pack)
- /SM590000 VIOS 4.1.0.40 (Service Pack) (P10 compatibility mode)
- /SM590000 VIOS 3.1.4.60 (Service Pack) (P9 compatibility mode).

## 1.7  Firmware and Hardware Management Console

The firmware on an IBM Power system is the foundational software layer that initializes hardware components, manages system resources, and provides the interface between the hardware and the operating system.

Known as the Power Firmware, it includes components like the Flexible Service Processor (FSP), PowerVM Hypervisor (PHYP), and system boot code. This firmware enables advanced features such as logical partitioning (LPARs), dynamic resource allocation, and secure boot processes. It also supports system diagnostics, error reporting, and remote management capabilities, especially when integrated with tools like the Hardware Management Console (HMC). Regular firmware updates from IBM ensure performance improvements, security patches, and compatibility with new hardware or software features.

An HMC is a dedicated appliance to configure and manage system resources on IBM Power. It can be delivered as a physical server or as a virtual appliance running in either Power VM or an x86 VM. A graphical user interface (GUI), a Command-line interface (CLI), or REST API interfaces are available.

## 1.7.1  HMC Requirements

Supported HMCs are the 7063-CR2 and the Virtual HMC appliances.

The minimum required HMC version for the Power E1150 is V11R1 M1110. V11R1 M1110 is supported on the 7063-CR2, and the Virtual HMC appliances only.

## Restriction:

- /SM590000 The 7063-CR1 and 7042 HMCs are not supported as an HMC for Power11 servers.
- /SM590000 The HMC code at V11R1M1110 cannot manage POWER7 processor-based systems

## 1.8  Rack support

The Power E1150 server fits a standard 19-inch rack. The server is certified and tested in the IBM Enterprise racks (7965-S42 and 7014-T42). Customers can choose to place the server in other racks if they are confident that those racks have the strength, rigidity, depth, and hole pattern characteristics that are needed. Contact IBM Support to determine whether other racks are suitable.

Note: It is a best practice that the Power E1150 server be ordered with an IBM 42U enterprise rack #ECR0 (7965-S42). This rack provides a high-quality environment for IBM Manufacturing system assembly and testing, and provides a complete package.

If a system is installed in a rack or cabinet that is not from IBM, help ensure that the rack meets the requirements.

Important: The customer is responsible for helping ensure the installation of the drawer in the preferred rack or cabinet results in a configuration that is stable, serviceable, safe, and compatible with the drawer requirements for power, cooling, cable management, weight, and rail security.

## 1.8.1  New rack considerations

Consider the following points when racks are ordered:

- /SM590000 The new IBM Enterprise 42U Slim Rack 7965-S42 offers 42 EIA units (U) of space in a slim footprint.
- /SM590000 The 7014-T42 rack is no longer available to purchase with a Power E1150 server. Installing a Power E1150 server in this rack is still supported.

Vertical PDUs: All PDUs that are installed in a rack that contains a Power E1150 server must be installed horizontally to allow for cable routing in the sides of the rack.

For more information on supported racks see section 2.3, 'Racks' on page 56.

<!-- image -->

Chapter 2.

2

## Power11 E1150

The IBM Power E1150 is a next-generation enterprise server built on the IBM POWER11 architecture, designed to deliver high performance, scalability, and efficiency for demanding workloads. It offers a range of processor options, allowing configurations with varying core counts and frequencies to match specific performance needs. The system supports DDR5-based DDIMMs, providing high memory bandwidth and capacity, with configurations that scale to support large in-memory databases and analytics workloads. The E1150 is engineered for dense compute environments, with advanced cooling and packaging technologies that enhance thermal efficiency and system reliability.

In terms of I/O and storage, the Power E1150 includes multiple PCIe Gen5 slots for high-speed connectivity to storage and networking. It also supports internal NVMe U.2 flash drives, offering low-latency, high-throughput storage options for performance-critical applications. For environments requiring additional expansion, the E1150 is compatible with external expansion drawers, such as the ENZ0 PCIe Gen4 I/O expansion drawer and the NED24 NVMe expansion drawer, which can house up to 24 NVMe devices. These options provide flexible scalability for both compute and storage, making the Power E1150 a robust platform for hybrid cloud, AI, and mission-critical enterprise workloads.

The following topics are covered in this chapter:

- /SM590000 Introducing the Power E1150
- /SM590000 Hardware Components
- /SM590000 Racks

## 2.1  Introducing the Power E1150

The Power11 processor represents the culmination of years of architectural innovation and performance enhancements. It powers the IBM Power E1150, a midrange server designed as the successor to the Power E1050. Engineered for both on-premises and cloud deployments, the Power E1150 delivers high performance, flexible capacity, and built-in virtualization capabilities, making it ideal for modern enterprise workloads.

The Power E1150 (model 9043-MRU) features a compact 4U rack-mounted chassis and supports configurations with two, three, or four Dual Chip Modules (DCMs). Each DCM contains up to 30 cores, with additional spare cores available for resiliency and capacity upgrades. The system includes PowerVM virtualization and supports AIX and Linux partitions, with or without the use of the Virtual I/O Server (VIOS), offering robust flexibility for diverse IT environments.

Restriction: IBM i is not supported on the Power E1150.

The base E1150 is configured with two sockets installed. Additional configurations available include three sockets and four sockets. Systems with two and three sockets can later be expanded by adding additional DCMs to the configuration.

There are three processor options that are available for this server:

- /SM590000 Sixteen cores (+2 spares) running at a typical 3.40 - 4.20 GHz (max) frequency range
- /SM590000 Twenty-four cores (+2 spares) running at a typical 3.05 - 4.15 GHz (max) frequency range
- /SM590000 Thirty cores (+2 spares) running at a typical 2.8 - 3.95 GHz (max) frequency range

A Power E1150 server with four 30-core DCMs offers the maximum of 120 cores and processor cores can run up to eight simultaneous threads to deliver greater throughput. All sockets must be populated with the same processor modules. Table 2-1 shows the number of cores and the number of threads available in the different configurations of the IBM Power E1150.

Table 2-1   Available cores and threads in the different E1150 configurations

| Number of sockets   | 16 Core (EPEX)   | 16 Core (EPEX)    | 24 Core (EPEY)   | 24 Core (EPEY)    | 30 Core (EPEZ)   | 30 Core (EPEZ)    |
|---------------------|------------------|-------------------|------------------|-------------------|------------------|-------------------|
| Number of sockets   | Number of cores  | Number of threads | Number of cores  | Number of threads | Number of cores  | Number of threads |
| 2                   | 32               | 256               | 48               | 384               | 30               | 480               |
| 3                   | 48               | 384               | 72               | 576               | 90               | 720               |
| 4                   | 64               | 512               | 96               | 768               | 120              | 960               |

## New features in the Power E1150

New in the Power E1150 are:

- /SM590000 Power11 DCM processor w/ 16, 24, or 30 cores/socket
- /SM590000 Spare cores for increased availability (2 spare cores per socket)
- /SM590000 Energy efficient mode with enhanced scheduling capabilities
- /SM590000 64 DDR5 DDIMM slots with 25% higher bandwidth that provides up to 16TB max memory
- /SM590000 Quantum safe encryption for secure boot and LPM (Logical Partition Migration)
- /SM590000 Serial Number preserving upgrade support from Power10 systems (4Q 2025 Availability)
- /SM590000 The E1150 is powered by a Titanium class 2300W power supply (PSU).

Power11 introduced a faster memory bus - the Open Memory Interface (OMI) - to the DDIMMs. The OMI now runs at max 32Gbps (with DDR5) compared with the OMI memory channel in the E1050 which runs at 25.6Gbps. The SMP xBus runs at 32Gbs.

Figure 2-1shows the front of the Power E1150.

Figure 2-1   Front view of the Power E1150

<!-- image -->

## Power E1150 compared to the E1050

With each Power Server evolution, new features and improvements are added,

- -more cores
- -more memory
- -better speed for memory or buses

Table 2-2 compares E1050 to E1150 showing the improvements that we have made to the new Power 1150.

Table 2-2   Comparison between the E1050 and the E1150

| Features                              | E1050                                                      | E1150                                                  |
|---------------------------------------|------------------------------------------------------------|--------------------------------------------------------|
| Processor module/max socket/max cores | P10 DCM/4S/96 Cores 74.5mm x 85.75mm; 5387 pin             | P11 DCM/4S/120 Cores 74.5mm x 85.75mm; 5387 pin        |
| Processor power                       | 455W                                                       | 545W                                                   |
| SMP xBus                              | xBus 32Gbs                                                 | xBus 32Gbs                                             |
| Memory Slot/System Capacity           | 64 DDIMM, 4U System Max 16TB                               | 64 DDIMM, 4USystem Max 16TB                            |
| Memory Bus                            | 2666or3200MbpsperOMI Channel 409 GB/s per CPU Socket (DCM) | 4000Mbps per OMI Channel 512 GB/s per CPU Socket (DCM) |
| Service Processor                     | eBMC                                                       | eBMC                                                   |
| System PCIe I/O                       | 176 lanes Gen4 OR 64 lanes Gen5 and 64 lanes Gen4          | 176 lanes Gen4 OR 64 lanes Gen5 and 64 lanes Gen4      |
| Cooling Fans and Cooling Domains      | 4 92mm fans, 1 cooling domain                              | 4 92mm fans, 1 cooling domain                          |
| Processor VRM N+1 Phase Redundancy    | Yes                                                        | Yes                                                    |

| Features                                                             | E1050                                      | E1150                                      |
|----------------------------------------------------------------------|--------------------------------------------|--------------------------------------------|
| Memory Voltage N+1 Phase Redundancy                                  | Yes                                        | Yes                                        |
| IO, Standby VRM N+1 Phase Redundancy                                 | Yes                                        | Yes                                        |
| Number of PCIe slots                                                 | 11                                         | 11                                         |
| Concurrent Maintenance Serviceability Enhancements for PCIe Adapters | Rear Blind Swap IO Cassette. Cable Bracket | Rear Blind Swap IO Cassette. Cable Bracket |
| Internal SAS Controllers/Cable                                       | None                                       | None                                       |
| Internal Storage                                                     | 10-NVMe PCIe Gen4 only                     | 10-NVMe PCIe Gen4 only                     |
| Concurrent Maintenance HDD/SSD/NVMe/PSU/Fan                          | Yes                                        | Yes                                        |
| Power Supplies                                                       | 4x Titanium Class 2300W                    | 4x Titanium Class 2300W                    |
| System Depth and Cable Management                                    | 35.5' + Cable Bracket                      | 35.5' + Cable Bracket                      |

## 2.1.1  Server configurations

The Power E1150 is a single machine type model: 9043-MRU delivered in a 4U enclosure. The minimum system has two sockets populated with one of the three available processor feature codes as shown in Table 2-3, Additional configurations are available with three or four sockets populated. The two socket system can be upgraded in the field to either three socket or four socket. The three socket system can be upgraded in the field to a four socket system.

## Processor options

There are three processor feature codes available at time of announcement. These features are differentiated by the number of cores and the frequency range of the cores. These characteristics are shown in Table 2-3.

Table 2-3   E1150 socket and core options

| Feature Code   | Feature Name   | Processor Type     | Max System Config   | Frequency Range   |
|----------------|----------------|--------------------|---------------------|-------------------|
| EPEZ           | 30W P11 DCM    | 30 Cores +2 Spare  | 120 Cores           | 3.0 to 4.1 GHZ    |
| EPEY           | 24W P11 DCM    | 24 Cores +2 Spares | 96 Cores            | 3.25 to 4.2 GHZ   |
| EPEX           | 16W P11 DCM    | 16 Cores +2 Spares | 64 Cores            | 3.5 to 4.2 GHZ    |

## SAP HANA support

There is no plan to release a separate SAP HANA model tor the E1150. Instead, SAP HANA will be sold on the E1150 with a special Linux price structure.

SAP HANA will be certified on the 24-core IBM Power11 processor, with support also planned for the 16-core, and 30-core Power11 configurations. IBM intends to support SAP HANA in production environments on the IBM Power System E1150 (9043-MRU), following the official certification of the platform. This support will include specific Linux operating systems that meet SAP's certification requirements, ensuring optimized performance and reliability for SAP HANA workloads on Power11 infrastructure.

- /SM590000 SUSE Linux Enterprise Server for SAP with SUSE Linux Enterprise Server 15 Service Pack 6,or later.
- /SM590000 Red Hat Enterprise Linux for SAP with Red Hat Enterprise Linux 9.6 for Power LE, or later.

## Memory configuration options

There are several memory feature options available in the E1150, as shown on Table 2-4

Table 2-4   E1150 memory options

| Feature Code   | Feature Name   | DDIMM Size     | Max System Config   | Frequency   | Peak Bandwidth /Socket   |
|----------------|----------------|----------------|---------------------|-------------|--------------------------|
| EM5P           | 64GB DDR5      | 2x 32GB DDIMM  | 2TB                 | 4000 MHZ    | 512 GBS                  |
| EM5Q           | 128GB DDR5     | 2x 64GB DDIMM  | 4TB                 | 4000 MHZ    | 1024 GBS                 |
| EM5R           | 256GB DDR5     | 2x 128GB DDIMM | 8TB                 | 4000 MHZ    | 1024 GBS                 |
| EM5S           | 512GB DDR5     | 2x 256GB DDIMM | 16TB                | 4000 MHZ    | 1024 GBS                 |

## 2.1.2  RAS Features

RAS stands for Reliability, Availability, and Serviceability. It encompasses features designed to ensure that a system operates consistently, remains available when needed, can be easily maintained, minimizes downtime, and enhances overall system performance.

System reliability begins at the component level, with devices and subsystems engineered for high dependability. Throughout the design and development phases, subsystems undergo rigorous verification and integration testing. During manufacturing, each system is subjected to comprehensive testing to ensure the highest standards of product quality.

## Key RAS Features

The Power11 processor-based E1150 system features an advanced suite of RAS (Reliability, Availability, and Serviceability) capabilities designed to enhance system uptime and streamline maintenance. Among the new enhancements is the inclusion of spare cores within each Dual Chip Module (DCM). These spare cores are automatically activated in the event of a core failure, providing an additional layer of redundancy. This design allows the system to continue operating without interruption, reducing the need for immediate maintenance outages and improving overall system resilience.

## Power11 Processor RAS

The Power11 processor has the ability to do processor instruction retry for some transient errors and core-contained checkstop for certain solid faults. The fabric bus design with CRC and retry persists in Power11 where a CRC code is used for checking data on the bus and has an ability to retry a faulty operation.

- /SM590000 Cache availability

The L2/L3 caches in the Power11 processor in the memory buffer chip are protected with double-bit detect, single-bit correct error detection code (ECC). In addition, a threshold of correctable errors detected on cache lines can result in the data in the cache lines being purged and the cache lines removed from further operation without requiring a reboot in the PowerVM environment.

Modified data would be handled through Special Uncorrectable Error handling. L1 data and instruction caches also have a retry capability for intermittent errors and a cache set delete mechanism for handling solid failures.

- /SM590000 Special Uncorrectable Error handling
- Special Uncorrectable Error (SUE) handling prevents an uncorrectable error in memory or cache from immediately causing the system to terminate. Rather, the system tags the data and determines whether it will ever be used again. If the error is irrelevant, it will not force a check stop. When and if data is used, I/O adapters controlled by an I/O hub controller would freeze if data were transferred to an I/O device, otherwise, termination may be limited to the program/kernel or if the data is not owned by the hypervisor.
- /SM590000 PCI extended error handling
- PCI extended error handling (EEH)-enabled adapters respond to a special data packet generated from the affected PCI slot hardware by calling system firmware, which will examine the affected bus, allow the device driver to reset it, and continue without a system reboot. For Linux, EEH support extends to the majority of frequently used devices, although some third-party PCI devices may not provide native EEH support.
- /SM590000 Uncorrectable error recovery

When the auto-restart option is enabled, the system can automatically restart following an unrecoverable software error, hardware failure, or environmentally induced (AC power) failure.

- /SM590000 Spare core

System configuration that allows two spare cores per DCM can maintain total processor core capacity after a predictive core failure.

- /SM590000 Runtime Processor Diagnostics

This capability provides a comprehensive exerciser to test processor cores by the PowerVM hypervisor without functionally impacting production workloads, enhancing system reliability. The service processor allows for different operation modes selection in the service processor ASMI menu.

## Service Processor

The E1150 system features a redesigned service processor built on a Baseboard Management Controller (BMC) architecture. It includes firmware accessible via open-source, industry-standard APIs such as Redfish. An enhanced ASMI (Advanced System Management Interface) web interface maintains essential RAS functions while offering a more intuitive user experience.

- /SM590000 Error Monitoring: Recoverable errors from the processor chipset are monitored by the system processor, while fatal errors are handled by the service processor.
- /SM590000 Independent Operation: The service processor operates on a separate power boundary, allowing it to function independently of the system processor.
- /SM590000 System Surveillance: It monitors connections to the Hardware Management Console (HMC) and system firmware (hypervisor), and supports remote power control, environmental monitoring, resets, restarts, and remote maintenance.

## Additional RAS Capabilities

The E1150 provides these additional RAS capabilities:

- /SM590000 Open Memory Interface with differential DIMMs.
- /SM590000 Enterprise-grade BMC for system management and service
- /SM590000 Active Memory Mirroring for the hypervisor
- /SM590000 Concurrent maintenance for NVMe drives and PCIe adapters
- /SM590000 Redundant, hot-plug cooling and power systems
- /SM590000 Redundant voltage regulators
- /SM590000 Concurrent maintenance for time-of-day battery
- /SM590000 Lightpath diagnostics with enclosure and FRU (Field Replaceable Unit) LEDs

- /SM590000 Clear service and FRU labeling
- /SM590000 Installation by client or IBM
- /SM590000 Proactive support with automated call-home capabilities
- /SM590000 Flexible servicing options (client or IBM)

## 2.1.3  HMC Requirements

The E1150 supports attachment to one or more HMCs or vHMCs for use when operating the system with PowerVM. This is the default configuration for servers supporting logical partitions with dedicated or virtual I/O. In this case, all servers have at least one logical partition.

The HMC is either a hardware appliance or virtual appliance that can be used to configure and manage your systems. The HMC connects to one or more managed systems and provides capabilities for following primary functions:

- /SM590000 Systems Management functions, such as Power Off, Power on, system settings, CoD, Enterprise Pools, shared processor pools, Performance and Capacity Monitoring, and starting Advanced System Management Interface (ASMI) for managed systems.
- /SM590000 Provide virtualization management capabilities including the creation, management, and deletion of LPARs; support for Live Partition Mobility (LPM) and Remote Restart; configuration of SR-IOV; management of Virtual I/O Servers; dynamic resource allocation; and access to operating system consoles.
- /SM590000 Acts as the service focal point for systems and supports service functions, including call home, dump management, guided repair and verify, concurrent firmware updates for managed systems, and around-the-clock error reporting with Electronic Service Agent (ESA) for faster support.
- /SM590000 Provides appliance management capabilities for configuring network, users on the HMC, and updating and upgrading the HMC.

An HMC is not required for the E1150, but is the recommended and default option for system management. There are two service strategies available if you choose not to implement an HMC in your environment:

- /SM590000 Full-system partition with PowerVM:
- A single partition owns all the server resources and only one operating system may be installed. The primary service interface is through the operating system and the service processor.
- /SM590000 Partitioned system with NovaLink:

In this configuration, the system can have more than one partition and can be running more than one operating system. The primary service interface is through the service processor.

## Supported consoles

Supported Hardware Management Console options for the Power E1150 are the 7063-CR2 and the Virtual HMC appliances.

## HMC 7063-CR2

The 7063-CR2 IBM Power HMC is a second-generation Power processor-based HMC.

The CR2 model includes the following features:

- /SM590000 6-core Power9 130W processor chip
- /SM590000 64 GB (4x16 GB) or 128 GB (4x32 GB) of memory RAM
- /SM590000 1.8 TB with RAID1 protection of internal disk capacity

- /SM590000 4-port 1 Gb Ethernet (RH-45), 2-port 10 Gb Ethernet (RJ-45), two USB 3.0 ports (front side) and two USB 3.0 ports (rear side), and 1 Gb IPMI Ethernet (RJ-45)
- /SM590000 Two 900W PSUs
- /SM590000 Remote Management Service: IPMI port (OpenBMC) and Redfish application programming interface (API)
- /SM590000 The Base Warranty is 1 year 9x5 with available optional upgrades

A USB Smart Drive is not included.

The 7063-CR2 is compatible with flat panel console kits 7316-TF3 1 , TF4 1 , and TF5.

## Virtual HMC

Originally, the IBM Hardware Management Console (HMC) was available exclusively as a physical hardware appliance, preinstalled with HMC firmware. However, IBM has since expanded its offerings to include a virtual HMC appliance. This virtual version can be deployed on ppc64le (PowerPCfi 64-bit Little Endian) architectures as well as on x86 platforms, providing greater flexibility for system administrators and data center environments.

Customers with a valid support contract can download the virtual HMC appliance from the Entitled Systems Support (ESS) website. Alternatively, it can be included as part of the initial order for a Power E1150 system.

The virtual HMC supports the following hypervisors:

- /SM590000 On x86 processor-based servers:
- -Xen
- -VMware
- -KVM
- /SM590000 On Power processor-based servers:
- -PowerVM

The following minimum requirements must be met to install the virtual HMC:

- /SM590000 16 GB of memory
- /SM590000 Four virtual processors
- /SM590000 Two network interfaces (maximum 4 allowed)
- /SM590000 One disk drive (500 GB available disk drive)

For an initial Power E1150 order with the IBM configurator (e-config), the HMC virtual appliance can be found by selecting Add software → Other System Offering s (as product selections) and then:

- /SM590000 5765-VHP for IBM HMC Virtual Appliance for Power V11
- /SM590000 5765-VHX for IBM HMC Virtual Appliance x86 V11

For more information about the Virtual HMC, see this web page.

For more information about how to install the virtual HMC appliance and all requirements, see IBM Documentation.

1   The 7316-TF3 and 7316-TF4 were withdrawn from marketing.

## 2.2  Hardware Components

In this section, we provide a detailed overview of the hardware components of the Power E1150 system, including its processors, memory, PCI slots, adapters, I/O drawers, and more.

## 2.2.1  Processors

The E1150 requires a minimum of two DCM processor modules be installed. It supports a maximum of four modules for a fully configured system. You can choose the number of processor modules on the initial configuration. If you start with less than four modules, you can add additional modules to the system later through an MES order. The MES upgrade requires scheduled downtime to install.

All processor modules in one server must be the same processor module Feature Code (F/Cfi) number. Processor F/C can't be mixed in E1150 server. On initial order and when adding additional modules via MES, all processor types must be the same. MES replacements of existing features is not supported. Table 2-5 shows the processor options available.

Table 2-5   E1150 Processor Features

| Feature                     | EPEZ   | EPEY   | EPEX   |
|-----------------------------|--------|--------|--------|
| Package                     | DCM    | DCM    | DCM    |
| #Cores Active per Module    | 30     | 24     | 16     |
| Spare Cores per Module      | 2      | 2      | 2      |
| Maximum Cores Per System    | 120    | 96     | 64     |
| Max DDR speed               | 4800*  | 4800*  | 4800*  |
| Power Save Freq             | 2.4GHz | 2.4GHz | 2.4GHz |
| Fixed Freq                  | 3000   | 3300   | 3500   |
| WOF Base Freq               | 3250   | 3550   | 3750   |
| MaxFreqwithall cores active | 4100   | 4200   | 4200   |
| Ultra Turbo Freq            | 4100   | 4200   | 4200   |
| Mod Max Power (@27C)        | 545W   | 535W   | 490W   |
| Processor Activation        | EPUZ   | EPUY   | EPUT   |

## Processor Activations

Permanent CoD processor core activations are required for the first processor module in the configuration and are optional for the second, third, and fourth modules.

Temporary CoD capabilities are optionally used for processor cores that are not permanently activated.

## Note: An HMC is required for temporary CoD

The E1150 can also be in a Power Enterprise Pool with other E1150 and E1050 systems when utilizing Power Enterprise Pools with Shared Utility Capacity. Shared Utility Capacity on Power E1150 systems provides enhanced multisystem resource sharing and by-the-minute tracking and consumption of compute resources across a collection of Power E1150 and E1050 systems within a single Power Enterprise Pools (2.0). Shared Utility Capacity provides a complete range of flexibility to tailor initial system configurations with the right mix of purchased and pay-for-use consumption of processor, memory, and software. Clients with existing Power Enterprise Pools 2.0 of Power E1050 systems can simply add one or more Power E1150 systems into their pool and migrate to them at the rate and pace of their choosing, as any Power E1150 and Power E1050 systems may seamlessly inter-operate and share compute resources within the same pool. Clients with Mobile Capacity on a Power E1150 may easily upgrade their system to support Power Enterprise Pools 2.0 to leverage Shared Utility Capacity resources.

## Workload Optimized Frequency (WOF)

All Power11 systems are shipped with Workload Optimized Frequency enabled.

The goals of WOF are:

- Increase peak performance
- Guarantee Turbo frequency
- Deliver Ultra Turbo frequency when possible
- Minimize run to run and part to part variations

There are several factors that are used to determine the maximum CPU frequency

- CPU Utilization - Lighter workloads will run at higher frequencies
- Number of Active Cores - Smaller number of active cores will run at higher frequencies
- Environmental Conditions - Lower ambient temperatures will run at higher frequencies

WOF takes advantage processor frequency and system capabilities, allowing fewer active workloads with fewer active cores to run at higher frequencies.

The frequency for a given workload is capped only by the maximum attainable frequency for the technology and the thermal design point (TDP) of the processor module supported by the system.   Each processor module has its own unique best performance capability, different from other modules. But as a system provider we would like to have consistency in the products that are offered.

The IBM Power11 architecture incorporates an advanced On-Chip Controller (OCC) that enables dynamic frequency scaling based on real-time system conditions. This controller continuously monitors key operational parameters - including voltage levels, current draw, and the number of active processor cores. Using these inputs, the OCC references pre-calibrated lookup tables to determine the optimal voltage-frequency operating point for each processor module.

When system workloads are light, the OCC increases processor frequencies to enhance performance. Conversely, as workloads intensify or thermal/power constraints are approached, frequencies are scaled down to maintain system stability and efficiency. This dynamic adjustment ensures that each system operates within its optimal performance envelope, contributing to uniform performance across a distributed infrastructure.

By intelligently managing power and performance, the OCC not only improves workload responsiveness but also supports energy-efficient computing - an increasingly critical requirement in modern data centers.

## 2.2.2  Memory Subsystem

The IBM Power E1150 server utilizes IBM's Open Memory Interface (OMI) for memory connectivity. OMI is a high-speed, low-latency memory interface developed by IBM to decouple memory technology from the processor interface. Unlike traditional memory interfaces that are tightly coupled to specific memory types (like DDR4 or DDR5), OMI provides a technology-agnostic approach. This allows IBM Power systems, such as the Power E1150, to support various memory technologies through Differential DIMMs (DDIMMs).

Some of the key benefits of OMI are:

- /SM590000 Flexibility: The ability to support multiple memory technologies on the same platform.
- /SM590000 Scalability: Enables high memory bandwidth and capacity - up to 16 TB in the Power E1150.
- /SM590000 Performance: Reduces latency and increases throughput by using a high-speed serial interface.
- /SM590000 Future-Proofing: Simplifies the adoption of next-generation memory technologies without redesigning the processor-memory interface.

OMI plays a crucial role in enhancing the performance and adaptability of IBM Power systems, especially in environments demanding high memory bandwidth and capacity, such as AI, analytics, and large-scale enterprise workloads.

The E1150 is designed to support DDR5 memory and the OMI interfaces are tuned for DDR5. Only DDR5 DDIMMs will be offered on new systems. However, IBM intends to support same serial number upgrades to existing Power10 based E1050 systems. In this case, the E1150 will support the integration of the existing memory on those E1050s, including existing DDR4 based DDIMMs.

## Memory Capabilities

Each Power11 DCM has 16 OMI busses or channels, 8 from each of the two chips. Each OMI channel connects to one 4U DDIMM.The Power E1150 supports up to 64 OMI slots when all four processor sockets are populated. This configuration allows for a maximum memory capacity of 16 TB, providing substantial scalability for memory-intensive workloads. Table 2-6 lists the available memory features for the E1150.

Table 2-6   E1150 4u DDIMM offering

| Feature Code a   | DIMM Capacity   | Size   | DDR Type   | DDR Speed   | OMI Speed b   |
|------------------|-----------------|--------|------------|-------------|---------------|
| EM5P/EMGT c      | 32 GB           | 4U     | DDR5       | 4000 MHz    | 32Gb/s        |
| EM5Q             | 64 GB           | 4U     | DDR5       | 4000 MHz    | 32Gb/s        |
| EMFL             | 256 GB          | 4U     | DDR5       | 4000 MHz    | 32Gb/s        |
| EM5S             | 256GB           | 4U     | DDR5       | 4000 MHz    | 32Gb/s        |

- a. Each feature code provides 2 DDIMMs
- b. DDIMM support up to 4800MHZ / 38.4 OMI. The E1150 system design is limited to 4000 MHz / 32G OMI

- c. Feature code for healthcare solution

Note: IBM intends to offer system upgrades from Power E1050 systems to corresponding next generation E1150 systems that will maintain the serial number of the existing system (same serial-number model upgrade).

While IBM intends that Power11 systems will be based on DDR5 technology, it is also intended that existing DDR4 and DDR5 memory DIMMs and activation can be retained or carried forward from E1050 systems to E1150 systems with a model upgrade.

## Minimum/Maximum Orderable System Memory Sizes

The following table lists the minimum and maximum of DDIMM and system memory sizes based on the number of sockets populated in the E1150.

Table 2-7   Memory minimums and maximums

| DDIMM and Sys Info    | 2S                | 3S                | 4S                 |
|-----------------------|-------------------|-------------------|--------------------|
| Min # of DDIMM        | 8                 | 12                | 16                 |
| Max # of DDIMM        | 32                | 48                | 64                 |
| DIMM Type             | 4U DDIMM          | 4U DDIMM          | 4U DDIMM           |
| Minimum DDIMM Size    | 32GB              | 32GB              | 32GB               |
| Maximum DDIMM Size    | 256GB             | 256GB             | 256GB              |
| Minimum System Memory | 8 x 32GB 256GB    | 12 x 32GB 384GB   | 16 x 32GB 512GB    |
| Maximum System Memory | 32 x 256GB 8192GB | 8 x 256GB 12288GB | 64 x 256GB 16348GB |

## OMI bus speed vs DDIMM Sizes

DDIMM must be of same size and speed behind each DCM socket. The actual OMI bus speed is set by firmware per DCM socket boundary based on the speed of the DDIMMs installed for that DCM socket.

## Memory subsystem RAS

The Power11 processor-based E1150 system introduces a new 4U tall differential DIMM (DDIMM), which has new open CAPI memory interface known as OMI for resilient and fast communication to the processor. This new memory subsystem design delivers solid RAS as described below.

- /SM590000 Memory Buffer: The DDIMM contains a memory buffer with key RAS features, including protection of critical data/address flows using CRC, ECC, and parity, a maintenance engine for background memory scrubbing and memory diagnostics, and a Fault Isolation Register (FIR) structure, which enables firmware attention-based fault isolation and diagnostics.
- /SM590000 OMI (Open Memory Interface): The OMI interface between the memory buffer and processor memory controller is protected by dynamic lane calibration, as well as a CRC retry/recovery facility to retransmit lost frames to survive intermittent bit flips. A complete lane fail can also be survived by triggering a dynamic lane reduction from 8 to 4, independently for both up and downstream directions. A key advantage of the OMI

interface is that it simplifies the number of critical signals that must cross connectors from processor to memory compared to a typical industry- standard DIMM design.

- /SM590000 Memory ECC: The DDIMM includes a robust 64-byte Memory ECC, with 8-bit symbols, capable of correcting up to five symbol errors (one x4 chip and one additional symbol), as well as retry for data and address uncorrectable errors.
- /SM590000 Dynamic row repair: To further extend the life of the DDIMM, the dynamic row repair feature can restore full use of a DRAM for a fault contained to a DRAM row, while system continues to operate.
- /SM590000 Spare temperature sensors: Each DDIMM provides spare temperature sensors, such that the failure of one does not require a DDIMM replacement.
- /SM590000 Spare DRAMs: 4U DDIMMs include two spare x4 memory modules (DRAMs) per rank. These can be substituted for failed DRAMs during runtime operation. Combined with ECC correction, the 2 spares allow the 4U DDIMM to continue to function with 3 bad DRAMs per rank, compared to 1 (single device data correct) or 2 (double device data correct) bad DRAMs in a typical industry standard DIMM design. This extends self-healing capabilities beyond what is provided with dynamic row repair capability.
- /SM590000 Spare power management integrated circuits: 4U DDIMMs include power management integrated circuits such that the failure of one power management integrated circuits does not require a DDIMM replacement.

## 2.2.3  Pervasive memory encryption

The Power11 processor integrates a Memory Controller Unit (MCU) that serves as the interface between the on-chip SMP interconnect fabric and the Open Memory Interface (OMI) links. This architecture makes the MCU an ideal location for implementing memory encryption logic.

The Power11 on-chip MCU encrypts and decrypts all traffic to and from system memory using Advanced Encryption Standard (AES) technology. This ensures that data in memory is protected from unauthorized access, both during operation and when memory modules are removed from the system.

## Supported AES Encryption Modes

There are two encryption modes built in to the MCU.

## AES XTS Mode (for Persistent Memory)

- /SM590000 XTS (Xor-Encrypt-Xor Tweaked Codebook with Ciphertext Stealing) is a strong block cipher mode ideal for encrypting persistent memory.
- /SM590000 Persistent DIMMs retain data even when power is off, posing a risk if physically removed.
- /SM590000 AES XTS protects against data theft from stolen or serviced DIMMs by encrypting memory contents.
- /SM590000 Although persistent memory is not yet standard in IBM Power systems, AES XTS support is built in for future readiness.

## AES CTR Mode (for Volatile Memory)

- /SM590000 CTR (Counter Mode) offers low-latency encryption, making it suitable for volatile memory.
- /SM590000 While not as strong as XTS, CTR mode is optimized for performance and is effective against physical attacks, especially in cloud environments.
- /SM590000 Each Power10/Power11 processor holds a 128-bit encryption key used by the MCU to encrypt data on DDIMMs connected via OMI links.

- /SM590000 The MCU cryptoengine is integrated into the data path, ensuring no performance degradation during encryption.
- /SM590000 Pervasive memory encryption using AES CTR is enabled by default and cannot be disabled via any administrative interface.

The pervasive memory encryption of the Power11 processor does not affect the encryption status of a system dump content. All data that is coming from the DDIMMs is decrypted by the MCU before it is passed onto the dump devices under the control of the dump program code. This statement applies to the traditional system dump under the OS control and the firmware assist dump utility.

The PowerVM LPM data encryption does not interfere with the pervasive memory encryption. Data transfer during an LPM operation uses the following general flow:

1. On the source server, the Mover Server Partition (MSP) provides the hypervisor with a buffer.
2. The hypervisor of the source system copies the partition memory into the buffer.
3. The MSP transmits the data over the network.
4. The data is received by the MSP on the target server and copied in to the related buffer.
5. The hypervisor of the target system copies the data from the buffer into the memory space of the target partition.

To facilitate LPM data compression and encryption, the hypervisor on the source system presents the LPM buffer to the on-chip NX unit as part of the processing in step 2. The reverse decryption and decompress operation is applied on the target server as part of the process in step 4.

The pervasive memory encryption logic of the MCU decrypts the memory data before it is compressed and encrypted by the NX unit on the source server. The logic also encrypts the data before it is written to memory but after it is decrypted and decompressed by the NX unit of the target server.

## Active Memory Mirroring

The Power E1050 server can mirror the PowerVM Hypervisor code across multiple memory DDIMMs. If a DDIMM that contains the hypervisor code develops an uncorrectable error, its mirrored partner enables the system to continue to operate uninterrupted.

Active Memory Mirroring (AMM) is an optional feature (#EM81).

The hypervisor code logical memory blocks are mirrored on distinct DDIMMs to enable more usable memory. There is no specific DDIMM that hosts the hypervisor memory blocks, so the mirroring is done at the logical memory block level, not at the DDIMM level. To enable the AMM feature, the server must have enough available memory to accommodate the mirrored memory blocks.

In addition to the hypervisor code itself, other components that are vital to the server operation are also mirrored:

- /SM590000 Hardware page tables (HPTs), which are responsible for tracking the state of the memory pages that are assigned to partitions
- /SM590000 Translation control entities (TCEs), which are responsible for providing I/O buffers for the partition's communications
- /SM590000 Memory that is used by the hypervisor to maintain partition configuration, I/O states, virtual I/O information, and partition state

Activating or deactivating AMM is done through the HMC. You can view the current status or modify the status from the Memory Mirroring section of the General Settings window for the E1150.

After a failure occurs on one of the DDIMMs that contains hypervisor data, all the server operations remain active and the enterprise Baseboard Management Controller (eBMC) service processor isolates the failing DDIMMs. The system stays in the partially mirrored state until the failing DDIMM is replaced.

Memory that is used to hold the contents of platform dumps is not mirrored, and AMM does not mirror partition data. AMM mirrors only the hypervisor code and its components to protect this data against a DDIMM failure. With AMM, uncorrectable errors in data that is owned by a partition or application are handled by the existing Special Uncorrectable Error (SUE) handling methods in the hardware, firmware, and OS.

SUE handling prevents an uncorrectable error in memory or cache from immediately causing the system to stop. Rather, the system tags the data and determines whether it will be used again. If the error is irrelevant, it does not force a checkstop. If the data is used, termination can be limited to the program/kernel or hypervisor owning the data, or freeze of the I/O adapters that are controlled by an I/O hub controller if data must be transferred to an I/O device.

## Active Memory Expansion

Active Memory Expansion (AME) is an advanced memory optimization feature available on IBM Power Systems running the AIX operating system. It allows the effective memory capacity of a system to exceed its physical memory by compressing memory contents in real time. This can result in memory expansion of up to 100% or more, enabling a partition to support more users or workloads without requiring additional physical memory. It also allows a server to run more logical partitions (LPARs), increasing overall system efficiency.

AME operates by using CPU resources to compress and decompress memory data. The effectiveness of this trade-off between memory capacity and processor cycles depends on how compressible the memory content is and whether there is sufficient spare CPU capacity available. To improve this process, Power11 processors include a dedicated hardware accelerator that enhances AME performance while reducing the load on CPU cores. This accelerator benefits from Power11's higher memory bandwidth and lower latency, making AME more efficient.

Administrators have fine-grained control over AME at the partition level. Each AIX partition can independently enable or disable AME, and configuration parameters allow users to specify the desired level of memory expansion. Enabling AME on a partition requires an Initial Program Load (IPL). Once active, AME performance can be monitored using standard AIX tools such as lparstat, vmstat, topas, and svmon.

To assist with planning, AIX includes a tool that analyzes actual workloads to estimate how much memory can be expanded and how much CPU resource will be required. This tool can run on any Power system. Additionally, IBM offers a one-time, 60-day AME trial through the Power Capacity on Demand website, allowing users to evaluate memory expansion and CPU usage more precisely.

AME is enabled through a chargeable hardware feature code (EMAM), which can be ordered with the system or as a Miscellaneous Equipment Specification (MES) order. A software key is provided for each system node when the feature is purchased. This key is permanent, specific to the system node, and does not require an IPL to activate. However, it cannot be transferred to another server. The CPU resources used for AME are part of the partition's assigned capacity and are subject to standard licensing requirements.

## OMI to DDIMM Wiring

Each of the two processor chips in the DCM has 8 of its 16 OMP PHY ports brought to the DCM pin. Note that the left (front) DCM0 and DCM3 are placed in 180 degrees rotation.

Figure 2-2 shows the OMI to DDIMM wiring.

Figure 2-2   OMI to DIMM Wiring

<!-- image -->

## OMI PHY Ports to DDIMM Location Codes Cross References

Figure 2-2 on page 47 provides cross-references for DDIMM location codes, offering a visual guide to help identify and map memory module positions within the system.

Note: CPx also means DCMx. They are used interchangeably

Figure 2-3   DDIMM Location Codes

<!-- image -->

## Memory Placement Rules

Each Power11 chip requires a minimum of two DDIMMs that are installed. Because each processor socket has a dual-chip module, four (2x2 DDIMMs) must be installed per socket. Because each Power E1150 server requires a minimum of two sockets that are populated, a minimum of eight DDIMMs must be installed. Using the smallest 32 GB DDIMMs, there is a minimum of 256 GB per server in a 2-socket configuration.

The OMI sockets that are numbered as P0-C22 - P0-C95 must be populated in a defined order. Figure 2-4 shows the plugging rules with colors. First, populate the green slots, then the pink slots, then the blue slots, and then the yellow slots.

Figure 2-4   Memory placement order

<!-- image -->

Table 2-8 provides a textual view of the memory placement order.

Table 2-8   Memory placement order

| Set                                       | Location codes       | Location codes       | Location codes       | Location codes       |
|-------------------------------------------|----------------------|----------------------|----------------------|----------------------|
|                                           | DCM0                 | DCM1                 | DCM2                 | DCM3                 |
| First set of DDIMMs (green in Figure 2-4) | P0-C86 P0-C87 P0-C88 | P0-C22 P0-C23 P0-C36 | P0-C38 P0-C39 P0-C52 | P0-C70 P0-C71 P0-C72 |
| Second set of DDIMMs (pink Figure 2-4)    | P0-C83 P0-C84        | P0-C25 P0-C26 P0-C33 | P0-C41 P0-C43        | P0-C66               |
| Second set of DDIMMs (pink Figure 2-4)    | P0-C89               | P0-C37               | P0-C53               | P0-C73               |
| in                                        |                      |                      |                      |                      |
| in                                        |                      |                      |                      | P0-C68               |
| in                                        | P0-C91               |                      | P0-C48               | P0-C75               |

| Set                                      | Location codes       | Location codes       | Location codes       | Location codes       |
|------------------------------------------|----------------------|----------------------|----------------------|----------------------|
|                                          | DCM0                 | DCM1                 | DCM2                 | DCM3                 |
| Third set of DDIMMs (blue in Figure 2-4) | P0-C80 P0-C81 P0-C93 | P0-C27 P0-C29 P0-C30 | P0-C42 P0-C45 P0-C46 | P0-C64 P0-C67 P0-C76 |
| Fourth set of                            | P0-C82 P0-C85        | P0-C24 P0-C28 P0-C32 | P0-C40 P0-C44        | P0-C65               |
| Fourth set of                            | P0-C95               | P0-C31               | P0-C49               | P0-C79               |
| DDIMMs (yellow in Figure 2-4)            |                      |                      |                      |                      |
| DDIMMs (yellow in Figure 2-4)            |                      |                      |                      | P0-C69               |
| DDIMMs (yellow in Figure 2-4)            | P0-C90               |                      | P0-C47               | P0-C74               |

## 2.3  Racks

The Power E1150 must be mounted in a supported rack for operation. The rack provides the ability to install and maintain the system unit and any attached expansion drawers, providing the power connections and allowing connectivity to other devices providing network connectivity and storage.

There are two IBM racks that are supported:

- /SM590000 IBM Enterprise 42U Slim Rack (7965-S42)

The IBM 7965-S42, also known as the Enterprise Slim Rack, is a compact rack designed for environments where space efficiency is a priority. The S42 offers 42U of rack space and supports a wide range of IBM systems, including Power servers and storage solutions. It is engineered for optimal thermal performance and includes features like front and rear door perforations for improved airflow. The S42 is particularly well-suited for modern data centers that require high-density computing in a smaller footprint, and it supports advanced cable routing and power management options to maintain a clean and efficient setup.

The S42 is supported for IBM integration of new orders and for migration of existing systems.

- /SM590000 IBM 42U Enterprise Rack Enclosure (7014-T42)

The IBM 7014-T42 rack is a robust, enterprise-grade enclosure designed to house a wide range of IBM Power Systems and associated IT equipment. Standing at 2.0 meters tall, the T42 provides 42 EIA units (42U) of usable vertical space, making it ideal for dense server and storage deployments. Its design accommodates high-power and high-heat systems, with features that support efficient airflow and cable management. The T42 is often used in data centers where scalability and reliability are critical, and it includes options for power distribution units (PDUs), side panels, and enhanced security features. Due to its height and weight, special handling and shipping considerations may apply.

The T42 rack has been withdrawn from marketing, but it is supported for field integration of the Power E1150 systems.

Other OEM racks are supported assuming that they meet the requirements specified in section 2.3.5, 'Original equipment manufacturer racks' on page 61.

## 2.3.1  IBM Enterprise 42U Slim Rack 7965-S42

The 2.0-meter (79-inch) Model 7965-S42 is compatible with past and present IBM Power servers and provides an excellent 19-inch rack enclosure for your data center. Its 600 mm (23.6 in.) width combined with its 1100 mm (43.3 in.) depth plus its 42 EIA enclosure capacity provides great footprint efficiency for your systems. It can be placed easily on standard 24-inch floor tiles.

Compared to the 7965-94Y Slim Rack, the Enterprise Slim Rack provides extra strength and provides additional shipping and installation flexibility.

The 7965-S42 rack includes space for up to four PDUs in side pockets. Extra PDUs beyond four are mounted horizontally and each uses 1U of rack space.

The Enterprise Slim Rack comes with options for the installed front door:

- /SM590000 Basic Black/Flat (#ECRM)
- /SM590000 High-End appearance (#ECRF)
- /SM590000 OEM Black (#ECRE)

All options include perforated steel, which provides ventilation, physical security, and visibility of indicator lights in the installed equipment within. All options come with a lock and mechanism included that is identical to the lock on the rear doors. Only one front door must be included for each rack ordered. The basic door (#ECRM) and OEM door (#ECRE) can be hinged on the left or right side.

Orientation: #ECRF must not be flipped because the IBM logo would be upside down.

At the rear of the rack, either a perforated steel rear door (#ECRG) or a Rear Door Heat Exchanger (RDHX) can be installed. The only supported RDHX is IBM Machine Type 1164-95X, which can remove up to 30,000 watts (102,000 BTU) of heat per hour by using chilled water. The no additional charge Feature Code #ECR2 is included with the Enterprise Slim Rack as an indicator when ordering the RDHX.

The basic door (#ECRG) can be hinged on the left or right side, and includes a lock and mechanism identical to the lock on the front door. Either the basic rear door (#ECRG) or the RDHX indicator (#ECR2) must be included with the order of a new Enterprise Slim Rack.

Due to the depth of the Power E1150 server model, the 5-inch rear rack extension (#ECRK) is required for the Enterprise Slim Rack to accommodate this system. This extension expands the space that is available for cable management and allows the rear door to close safely.

## Lifting considerations

Three to four service personnel are required to manually remove or insert a system unit into a rack because of its dimensions, weight, and content. To avoid the need for this many people to assemble at a client site for a service action, a lift tool can be useful. Similarly, if the client chose to install this customer setup (CSU) system, similar lifting considerations apply.

The Power E1150 server has a maximum weight of 70.3 kg (155 lb). However, by temporarily removing the power supplies, fans, and RAID assembly, the weight is easily reduced to a maximum of 55 kg (121 lb).

When lowering the Power E1150 server onto its rails in the rack, the server must be tilted on one end about 15 degrees so that the pins on the server enclosure fit onto the rails. This equates to lifting one end of the server 4 cm (1.6 in.). This task can be done by using a tip

plate on a lift tool, manually adjusting the load on a lift tool, or tilting during the manual lift. Consider the optional feature #EB2Z lift tool.

## 2.3.2  AC power distribution unit and rack content

The IBM high-function PDUs provide more electrical power per PDU compared to earlier PDUs, and they offer better PDU footprint efficiency. In addition, they are intelligent PDUs (iPDUs) that provide insight to actual power usage by receptacle and also provide remote power on/off capability for support by individual receptacle. The latest PDUs can be ordered as #ECJJ, #ECJL, #ECJN, and #ECJQ.

IBM Manufacturing integrates only the newer PDUs with the Power E1150 server. IBM Manufacturing does not support integrating earlier PDUs, such as #7188, #7109, or #7196. Clients can choose to use older IBM PDUs in their racks, but must install those earlier PDUs at their site.

Table 2-9 summarizes the high-function PDU Feature Codes for 7965-S42 followed by a descriptive list.

Table 2-9   High-function PDUs that are available with IBM Enterprise Slim Rack (7965-S42)

| PDUs                   | 1-phase or 3-phase depending on country wiring standards   | 3-phase 208 V depending on country wiring standards   |
|------------------------|------------------------------------------------------------|-------------------------------------------------------|
| Nine C19 receptacles a | #ECJJ                                                      | #ECJL                                                 |
| Twelve C13 receptacles | #ECJN                                                      | #ECJQ                                                 |

- a. The Power E1050 server has an AC power supply with a C19/C20 connector.

Power sockets: The Power E1150 server takes IEC 60320 C19/C20 mains power and not C13. Help ensure that the correct power cords and PDUs are ordered or available in the rack.

- /SM590000 High Function 9xC19 PDU plus (#ECJJ)
- This intelligent, switched 200 - 240 V AC PDU includes nine C19 receptacles on the front of the PDU. The PDU is mounted on the rear of the rack, which makes the nine C19 receptacles easily accessible. For comparison, this PDU is most like the earlier generation #EPTJ PDU.
- /SM590000 High Function 9xC19 PDU plus 3-Phase (#ECJL)

This intelligent, switched 208 V 3-phase AC PDU includes nine C19 receptacles on the front of the PDU. The PDU is mounted on the rear of the rack, which makes the nine C19 receptacles easily accessible. For comparison, this PDU is most like the earlier generation #EPTL PDU.

- /SM590000 High Function 12xC13 PDU plus (#ECJN)
- This intelligent, switched 200 - 240 V AC PDU includes 12 C13 receptacles on the front of the PDU. The PDU is mounted on the rear of the rack, which makes the 12 C13 receptacles easily accessible. For comparison, this PDU is most like the earlier generation #EPTN PDU.
- /SM590000 High Function 12xC13 PDU plus 3-Phase (#ECJQ)

This intelligent, switched 208 V 3-phase AC PDU includes 12 C13 receptacles on the front of the PDU. The PDU is mounted on the rear of the rack, which makes the 12 C13 receptacles easily accessible. For comparison, this PDU is most like the earlier generation #EPTQ PDU.

The PDU receives power through a UTG0247 power-line connector. Each PDU requires one PDU-to-wall power cord. Various power cord features are available for various countries and applications by varying the PDU-to-wall power cord, which must be ordered separately.

Each power cord provides the unique design characteristics for the specific power requirements. To match new power requirements and save previous investments, these power cords can be requested with an initial order of the rack or with a later upgrade of the rack features.

Table 2-10 shows the available wall power cord options for the PDU features, which must be ordered separately.

Table 2-10   PDU-to-wall power cord options for the PDU features

| Feature Code   | Wall plug              | Rated voltage (V AC)   |   Phase | Rated amperage    | Geography                                 |
|----------------|------------------------|------------------------|---------|-------------------|-------------------------------------------|
| #6489          | IEC309, 3P+N+G, 32 A   | 230                    |       3 | 32 amps per phase | Europe, Middle East, and Asia (EMEA)      |
| #6491          | IEC 309, P+N+G, 63 A   | 230                    |       1 | 63 amps           | EMEA                                      |
| #6492          | IEC 309, 2P+G, 60 A    | 200 - 208, 240         |       1 | 48 amps           | US, Canada, Latin America (LA), and Japan |
| #6653          | IEC 309, 3P+N+G, 16 A  | 230                    |       3 | 16 amps per phase | Internationally available                 |
| #6654          | NEMA L6-30             | 200 - 208, 240         |       1 | 24 amps           | US,Canada,LA, and Japan                   |
| #6655          | RS 3750DP (watertight) | 200 - 208, 240         |       1 | 24 amps           | US,Canada,LA, and Japan                   |
| #6656          | IEC 309, P+N+G, 32 A   | 230                    |       1 | 24 amps           | EMEA                                      |
| #6657          | PDL                    | 230 - 240              |       1 | 32 amps           | Australia and New Zealand                 |
| #6658          | Korean plug            | 220                    |       1 | 30 amps           | North and South Korea                     |
| #6667          | PDL                    | 380 - 415              |       3 | 32 amps           | Australia and New Zealand                 |

Notes: Ensure that the suitable power cord feature is configured to support the power that is being supplied. Based on the power cord that is used, the PDU can supply 4.8 - 19.2 kVA. The power of all the drawers that are plugged into the PDU must not exceed the power cord limitation.

The Universal PDUs are compatible with previous models. To better enable electrical redundancy, the Power E1150 server has four power supplies that must be connected to separate PDUs, which are not included in the base order. For maximum availability, a best practice is to connect power cords from the same system to two separate PDUs in the rack, and to connect each PDU to independent power sources.

For more information about the power requirements of and the power cord for the 7965-94Y rack, see IBM Documentation.

## 2.3.3  Rack-mounting rules

Consider the following primary rules when you mount the system into a rack:

- /SM590000 The system can be placed at any location in the rack. For rack stability, start filling the rack from the bottom.
- /SM590000 As as best practice, use an IBM approved lift tool for the installation of systems into any IBM or third-party rack.
- /SM590000 IBM does not support the installation of server nodes higher than the 29U position.
- /SM590000 Any remaining space in the rack can be used to install other systems or peripheral devices. Ensure that the maximum permissible weight of the rack is not exceeded and the installation rules for these devices are followed.
- /SM590000 Before placing the system into the service position, follow the rack manufacturer's safety instructions regarding rack stability.

Order information: The racking approach for the initial order must be 7965-S42 or #ECR0. If an extra rack is required for I/O expansion drawers, an MES to a system or an #0553 must be ordered.

## 2.3.4  Useful rack additions

This section highlights several rack addition solutions for IBM Power rack-based systems.

## IBM System Storage 7226 Model 1U3 Multi-Media Enclosure

The IBM System Storage 7226 Model 1U3 Multi-Media Enclosure can accommodate up to two tape drives, two RDX removable disk drive docking stations, or up to four DVD-RAM drives.

For more information on the 7726-1U3 see 'IBM System Storage 7226 Model 1U3 Multi-Media Enclosure' on page 90.

## Flat panel display options

The IBM 7316 Model TF5 is a rack-mountable flat panel console kit that also can be configured with the tray pulled forward and the monitor folded up, which provides full viewing and keying capability for the HMC operator.

The Model TF5 is a follow-on product to the Model TF4 and offers the following features:

- /SM590000 A slim, sleek, and lightweight monitor design that occupies only 1U (1.75 in.) in a 19-inch standard rack.

- /SM590000 A 18.5 inch (409.8 mm x 230.4 mm) flat panel TFT monitor with truly accurate images and virtually no distortion.
- /SM590000 The ability to mount the IBM Travel Keyboard in the 7316-TF5 rack keyboard tray.
- /SM590000 Support for the IBM 1x8 Rack Console Switch (#4283) IBM Keyboard/Video/Mouse (KVM) switches.

The #4283 is a 1x8 Console Switch that fits in the 1U space behind the TF5. It is a CAT5-based switch. It contains eight analog rack interface (ARI) ports for connecting PS/2 or USB console switch cables. It supports chaining of servers that use an IBM Conversion Options switch cable (#4269). This feature provides four cables that connect a KVM switch to a system, or can be used in a daisy-chain scenario to connect up to 128 systems to a single KVM switch. It also supports server-side USB attachments.

## 2.3.5  Original equipment manufacturer racks

The Power E1150 can be installed in a suitable OEM rack if that the rack conforms to the EIA-310-D standard for 19-inch racks. This standard is published by the Electrical Industries Alliance. For more information, see IBM Documentation.

IBM Documentation provides the general rack specifications, including the following information:

- /SM590000 The rack or cabinet must meet the EIA Standard EIA-310-D for 19-inch racks, which was published August 24, 1992. The EIA-310-D standard specifies internal dimensions, for example, the width of the rack opening (width of the chassis), the width of the module mounting flanges, and the mounting hole spacing.
- /SM590000 The front rack opening must be a minimum of 450 mm (17.72 in.) wide, and the rail-mounting holes must be 465 mm plus or minus 1.6 mm (18.3 in. plus or minus 0.06 in.) apart on center (horizontal width between vertical columns of holes on the two front-mounting flanges and on the two rear-mounting flanges).

Figure 2-5 is a top view showing the rack specification dimensions.

Figure 2-5   Rack specifications (top-down view)

<!-- image -->

- /SM590000 The vertical distance between mounting holes must consist of sets of three holes that are spaced (from bottom to top) 15.9 mm (0.625 in.), 15.9 mm (0.625 in.), and 12.7 mm (0.5 in.) on center, which makes each three-hole set of vertical hole spacing 44.45 mm (1.75 in.) apart on center.

Figure 2-6 shows the vertical distances between the mounting holes.

Figure 2-6   Vertical distances between mounting holes

<!-- image -->

- /SM590000 The following rack hole sizes are supported for racks where IBM hardware is mounted:
- -7.1 mm (0.28 in.) plus or minus 0.1 mm (round)
- -9.5 mm (0.37 in.) plus or minus 0.1 mm (square)

The rack or cabinet must be capable of supporting an average load of 20 kg (44 lb.) of product weight per EIA unit. For example, a four EIA drawer has a maximum drawer weight of 80 kg (176 lb.).

<!-- image -->

Chapter 3.

3

## I/O Subsystem

This chapter explores the robust I/O subsystem within IBM Power Systems, highlighting its design for mission-critical applications. We'll delve into how its modular architecture and PowerVM virtualization facilitate massive, dynamic scalability of I/O resources, including network and storage. The discussion also covers the inherent reliability features, such as NPIV and VIOS, along with advanced error handling, ensuring continuous operation. Finally, we'll examine how direct I/O assignments and optimized shared I/O through VIOS contribute to exceptional performance for demanding, data-intensive workloads.This chapter includes the following topics:

- /SM590000 Internal I/O
- /SM590000 Enhancing I/O Scalability with Expansion Drawers
- /SM590000 List of Supported Adapters on IBM Power E1150
- /SM590000 Other device support

## 3.1  Internal I/O

The I/O subsystem in IBM Power Systems is designed from the ground up to deliver exceptional scalability, reliability, and performance for mission-critical workloads. Scalability is achieved through a highly modular and flexible architecture. This allows for massive expansion of I/O resources, including a vast number of PCIe slots, high-speed network adapters (Ethernet, InfiniBand), and a large capacity for internal and external storage, including NVMe drives. PowerVM virtualization, a cornerstone of Power Systems, enables granular allocation and sharing of physical I/O resources across multiple logical partitions (LPARs), allowing administrators to dynamically scale I/O to meet the changing demands of virtualized workloads without physical reconfiguration.

Reliability is paramount in the IBM Power I/O subsystem, built upon decades of experience in mission-critical computing. Features like N\_Port ID Virtualization (NPIV) provide highly available Fibre Channel connectivity by allowing multiple LPARs to share a single physical adapter while maintaining independent storage access. Virtual I/O Server (VIOS) plays a crucial role, providing shared I/O capabilities and enabling failover mechanisms for network and storage adapters. Advanced error detection and correction (RAS features), such as predictive failure analysis and dynamic component sparing for I/O resources, are deeply integrated into the hardware and firmware.

The ability to directly assign I/O adapters to LPARs (dedicated I/O) or use hardware-assisted virtualization like SR-IOV for near-native performance further optimizes throughput and reduces CPU overhead. For shared I/O, the Virtual I/O Server (VIOS) is continuously optimized to handle high volumes of virtualized traffic, with best practices and tuning options available to maximize performance for various workloads, including those with intensive network and storage demands. The overall architecture is designed to handle immense I/O concurrency, making Power Systems ideal for data-intensive applications such as databases, analytics, and AI workloads that require rapid access to large datasets

## 3.1.1  Internal PCIe Gen 5 subsystem

The E1150 system provides a total of eleven internal PCIe slots, labeled C1 through C11. All eleven slots are located in the rear. Each of the eleven slots is a general-purpose PCIe slot and is physically x16 in size. In addition to the PCIe subsystem, the E1150 reserves additional PCIe Gen4 slots for connectivity for up to ten NVMe drives that are installed in the system. The internal NVMe configuration is described in section 3.1.4, 'Internal NVMe storage subsystem' on page 67.

Slot configurations are as follows:

- -six slots (C2, C3, C4, C5, C8, and C11) support x8 Gen5 or x16 Gen4 operation
- -two slots (C7 and C10) support x8 Gen5
- -three slots (C1, C6, and C9) support x8 Gen4.

Note: Slots C1, C2, C3, C6, C7, and C8 are designated as OpenCAPI (OC) slots. However, OpenCAPI adapters are not concurrently maintainable, and currently, there are no OpenCAPI adapters planned for the E1150.

Figure 3-1 shows the PCIe subsystem in the E1150.

Figure 3-1   Overview of the PCIe subsystem of the E1150

<!-- image -->

E1150 has 10 out of 11 IO slots from the right 2 dual chip modules (DCM1 and DCM2) while all 10 NVMe are attached to the two 2 left dual chip modules (DCM0 and DCM3). This means that systems with only two sockets populated are limited to seven usable PCIe slots (P0-C1 and P0-C6 to P0-C11) and six NVMe drives. To make all the slots available, at least three processor sockets must be populated and to enable all of the NVMe drives, all of the sockets must be populated.

Table 3-1 lists the number of PCIe slots and NVMe drives available based on the number of sockets populated.

Table 3-1   Available PCIe slots and NVMe drives

|   Number of sockets populated |   PCIe slots a |   NVMe drives |
|-------------------------------|----------------|---------------|
|                             2 |              7 |             6 |
|                             3 |             11 |             6 |
|                             4 |             11 |            10 |

- a. One slot is dedicated for network card installation.

The Power11 chip design achieves full IO bandwidth from any IO slot in a flat-8 SMP (1-hop) design.

Slot C0 is a special-purpose slot reserved for the eBMC Service Processor Card. This slot is not a standard PCIe slot and is not concurrently maintainable. The eBMC card is installed in a dedicated I/O cassette. The BMC card is described in section 8.4, 'eBMC card' on page 154.

There is no ethernet connectivity on the E1150 system board, so Slot C1, while a general-purpose x8 PCIe slot, is typically used for the base Ethernet card. The default Ethernet adapter for the E1150 is the 2-Port 25/10 Gb NIC &amp; ROCE SR/Cu PCIe 3.0 Adapter (Feature Code EC2U), but other adapters can be utilized.

## 3.1.2  PCIe slot properties

The internal I/O subsystem of the Power E1150 server is connected to the PCIe Express controllers on a Power11 chip in the system. A Power11 chip has two PCI Express controllers (PECs) of 16 lanes each for a total of 32 Gen5/Gen4 lanes per chip and 64 Gen5/Gen4 lanes per DCM.

Each PEC supports up to three PCI host bridges (PHBs) that directly connect to PCIe slots or devices. Both PEC0 and PEC1 can be configured as follows:

- /SM590000 One x16 Gen4 PHB or one x8 Gen5 PHB
- /SM590000 One x8 Gen5 and one x8 Gen4 PHB
- /SM590000 One x8 Gen5 PHB and two x4 Gen4 PHBs

Table 3-2 shows the PCIe port capabilities and is listed by physical position from right to left of the rear view.

Table 3-2   PCIe slot characteristics

| Name   | Width        | P11 CP   | PCIe bus   |
|--------|--------------|----------|------------|
| C11    | x8 G5/x16 G4 | DCM1/C0  | E0         |
| C10    | x8 G5/G4     | DCM1/C0  | E1A        |
| C9     | x8 G4        | DCM1/C0  | E1B        |
| C8     | x8 G5/x16 G4 | DCM1/C1  | E0         |
| C7     | x8 G5/G4     | DCM1/C1  | E1A        |
| C6     | x8 G4        | DCM1/C1  | E1B        |
| C5     | x8 G5/x16 G4 | DCM2/C0  | E0         |
| C4     | x8 G5/x16 G4 | DCM2/C0  | E1         |
| C3     | x8 G5/x16 G4 | DCM2/C1  | E0         |
| C2     | x8 G5/x16 G4 | DCM2/C1  | E1         |
| C1     | x8 G4        | DCM0/C0  | E0A        |

Note: The eBMC service processor card occupies slot C0. Slot C1 is generally reserved for a network adapter.

The x16 slots can provide up to twice the bandwidth of x8 slots because they offer twice as many PCIe lanes. PCIe Gen5 slots can support up to twice the bandwidth of a PCIe Gen4 slot, and PCIe Gen4 slots can support up to twice the bandwidth of a PCIe Gen3 slot, assuming an equivalent number of PCIe lanes.

Note: Although some slots provide a x8 connection only, all slots have an x16 connector.

All PCIe slots support hot-plug adapter installation and maintenance and enhanced error handling (EEH). PCIe EEH-enabled adapters respond to a special data packet that is generated from the affected PCIe slot hardware by calling system firmware, which examines the affected bus, allows the device driver to reset it, and continues without a system restart. For Linux, EEH support extends to the most devices, although some third-party PCI devices might not provide native EEH support.

All PCIe adapter slots support hardware-backed network virtualization through single-root IO virtualization (SR-IOV) technology. Configuring an SR-IOV adapter into SR-IOV shared mode might require more hypervisor memory. If sufficient hypervisor memory is not available, the request to move to SR-IOV shared mode fails. The user is instructed to free extra memory and try the operation again.

The Power E1150 server is smarter about energy efficiency when cooling the PCIe adapter environment. It senses which IBM PCIe adapters are installed in their PCIe slots, and if an adapter requires higher levels of cooling, they automatically speed up fans to increase airflow across the PCIe adapters. Faster fans increase the sound level of the server.

## 3.1.3  PCIe cassette

The E1150 PCIe subsystem uses a cassette to facilitate the installation and concurrent maintenance of the PCIe adapters. PCIe adapters are installed into the I/O cassette before the cassette is inserted into the system from the rear. Each I/O cassette includes a single x16 PCIe connector. The PCIe cassettes are shipped with the system and every slot from C1 to C11 contains an I/O cassette, either populated with an adapter or with a blank. All standard PCIe slots (C1 through C11) support concurrent maintenance via the I/O cassette mechanism, while Slot C0 does not. The PCIe cassette is shown in Figure 3-2.

Figure 3-2   PCIe cassette

<!-- image -->

## 3.1.4  Internal NVMe storage subsystem

The E1150 system is equipped with an NVMe U.2 backplane. Drives that support both U.3 and U.2 interfaces will operate in NVMe U.2 mode only, as the E1150 does not support the SAS protocol. Additionally, the system is limited to single-port NVMe mode and does not support dual-port NVMe configurations.

The E1150 includes ten internal NVMe drive bays, integrated into a backplane assembly. This backplane is a standard component in all E1150 systems. In addition to the ten NVMe bays, the NVMe backplane also houses the Operator Panel Base, Operator Panel LCD, and two USB 3.0 ports.

This is shown in Figure 3-3.

Note: Although ten NVMe drive bays are available, the number of NVMe drives supported is dependent on the number of sockets populated in the E1150.

- -For 2 and 3 socket systems: 6 NVMe devices are supported
- -For 4 socket systems: 10 NVMe devices are supported

Figure 3-3   Front view of E1150 showing NVMe drive slots

<!-- image -->

The NVMe backplane connects directly to the system planar using three connectors and a dedicated power connector. There are no cables between the backplane and the planar, ensuring a clean and reliable connection.

The wiring strategy and materials used in the backplane are specifically selected to support PCIe Gen4 signaling across all NVMe drives. Signal Integrity (SI) analysis has confirmed that neither re-timers nor re-drivers are required to maintain Gen4 performance. The NVMe connectors used are compliant with Gen4 PCIe specifications. It is expected that Gen5 NVMe will be available during the E1150 lifetime and the E1150 will accept Gen5 drive. However, NVMe devices installed in the Power E1150 will always run at Gen4 speed or lower.

The E1150 supports NVMe drives with either 7mm or 15mm height, utilizing a single universal carrier for both sizes. While the system will be offered with 15mm drives by default, it also supports the use of 7mm drives when installed in the 15mm carrier, allowing for flexible drive migration.

Table 3-3 provides a cross reference between the NVMe drive number and its location code.

Table 3-3   Location codes for NVMe drives

| NVMe drive   | Location Code   |
|--------------|-----------------|
| NVMe0        | P1-C0           |
| NVMe1        | P1-C1           |
| NVMe2        | P1-C2           |
| NVMe3        | P1-C3           |

| NVMe drive   | Location Code   |
|--------------|-----------------|
| NVMe4        | P1-C4           |
| NVMe5        | P1-C5           |
| NVMe6        | P1-C6           |
| NVMe7        | P1-C7           |
| NVMe8        | P1-C8           |
| NVMe9        | P1-C9           |

## Plug rules in the E1150 system unit

The following NVMe plug rules are recommended to provide the most redundancy in hardware for operating system mirror support.

Table 3-4

| Number of NVMe drives a   | Location                                                    |
|---------------------------|-------------------------------------------------------------|
| 2                         | NVMe3 NVMe4 or NVMe8 NVMe9                                  |
| 4                         | NVMe3 NVMe4 NVMe8 NVMe9                                     |
| 6                         | NVMe3 NVMe4 NVMe8 NVMe9 NVMe2 NVMe7                         |
| More than 6 b             | NVMe3 NVMe4 NVMe8 NVMe9 NVMe2 NVMe7 NVMe5 NVMe6 NVMe0 NVMe1 |

- a. If odd number of NVMe just install by order shown.
- b. Only available in four socket configuration

## NVMe availability considerations

When you load either VIOS or an operating system on the internal NVMe drives, and you intend to mirror the drives, we have recommended plug rules to attempt to ensure separate hardware paths for the mirrored pairs of disk E1150 NVMe thermal design supports 18W for 15mm NVMe and 12W for 7mm NVMe.

The NVMe drives can be in an OS-controlled RAID0, RAID1 array. Hardware RAID is not supported on the NVMe drives.

Note: It is recommended, though not required, that the two NVMe drives used in a mirrored pair be of the same capacity. When drives of different capacities are used, the effective usable capacity of the mirror is limited to the smaller of the two drives. In other words, the mirror (secondary) drive must have a capacity equal to or greater than that of the primary drive to ensure proper mirroring functionality without data loss.

## For mirrored OS support:

- -NVMe3 and NVMe4
- -NVMe8 and NVMe9

For dual VIOS configurations:

- /SM590000 VIOS1
- -NVMe 3 and NVMe9
- /SM590000 VIOS2
- -NVMe4 and NVMe8

Note: When there are multiple running partitions in the system, the mirror pairs selections are dependent on drives available and allocated to each partition.

## 3.2  Enhancing I/O Scalability with Expansion Drawers

Adding I/O drawers to an IBM Power E1150 server significantly enhances the system's scalability, flexibility, and overall performance in enterprise environments. The Power E1150 supports PCIe Gen4 I/O Expansion Drawers, which provide increased throughput and bandwidth for connected devices, enabling the system to handle more demanding workloads and a broader range of peripherals.

These drawers expand the available PCIe slots beyond what is natively supported on the system board, allowing for additional network adapters, storage controllers, and accelerators to be integrated without compromising existing configurations. This is particularly beneficial for data-intensive applications, such as AI inferencing, high-speed networking, and large-scale database operations. Furthermore, the modular nature of I/O expansion supports a pay-as-you-grow model, aligning with dynamic business needs and reducing upfront capital expenditure.

By expanding the number and type of adapters that can be installed, these drawers enable more flexible system configurations. This is especially beneficial in virtualized environments where multiple virtual machines or containers require dedicated I/O resources. The additional capacity supports higher VM density and more granular resource allocation, improving overall system utilization. IBM's expansion drawers are designed with enterprise-grade reliability features, including hot-plug capabilities, redundant paths, and integration with PowerVM and the Hardware Management Console (HMC). This ensures that I/O resources can be added or serviced without system downtime, supporting continuous operations and minimizing the risk of service disruptions.

Finally, the I/O expansion capability aligns with the Power E1150's broader design goals of resilience, security, and performance. The system's architecture, including its I/O subsystem, is built to support mission-critical workloads with features like redundant paths, error recovery, and secure boot. The drawers integrate seamlessly with IBM's Hardware Management

Console (HMC) and PowerVM virtualization, ensuring centralized control and simplified management. This makes the Power E1150 with I/O drawers an ideal platform for enterprises seeking to modernize their infrastructure while maintaining high availability and robust performance.

## 3.2.1  Supported I/O Drawers

The E1150 supports the following I/O expansion drawers:

- /SM590000 PCIe Gen4 I/O Expansion Drawer (#ENZ0)
- The PCIe Gen4 I/O Expansion Drawer (#ENZ0) is a 4U high, 19-inch wide, PCIe Gen4 based rack mountable I/O drawer that is available as a feature of Power11 Servers. The PCIe Gen4 I/O Expansion Drawer (#ENZ0) replaces the PCIe Gen3 I/O Expansion Drawer (#EMX0). There is no upgrade path from PCIe Gen3 I/O Expansion Drawer (#EMX0) to PCIe Gen4 I/O Expansion Drawer (#ENZ0).
- /SM590000 NED24 NVMe Expansion Drawer (#ESR0)

The NED24 NVMe Expansion Drawer (#ESR0) is a storage expansion enclosure with 24 U.2 NVMe bays. It supports up to 24 U.2 NVMe devices in 15 mm Gen3 carriers. The 15 mm carriers can accommodate either 7 mm or 15 mm NVME devices.

Within the Power E1150 server the PCIe slots are enabled to support the PCIe x16 to CXP Converter Card (FC EJ2A) that is used to attach expansion drawers. Two PCIe x16 to CXP Converter Cards are used to attach each expansion drawer. Depending on the number of sockets populated in the E1150 you purchase, the maximum number of supported drawers are as detailed in Table 3-5.

Table 3-5   Maximum IO Drawer configuration

| Configuration          |   Max IO Drawers | Max NED24   | Max ENZ0   | Comments                        |
|------------------------|------------------|-------------|------------|---------------------------------|
| E1150 (3 or 4 Sockets) |                4 | 2 a         | 4 b        | Slots C7 and C10 are Gen 5 x8 c |
| E1150 (2 Sockets)      |                2 | 1 d         | 2 e        | Slots C7 and C10 are Gen 5 x8 c |

- a. E1150 Slots for NED24: C8/C11, C2/C4, or C3/C5
- b. E1150 Slots for ENZ0: C2/C4, C3/C5, C8/C11
- c. Using the x8 slots yields performance that is similar to the performance of the EMX0 PCIe3 expansion drawer. Using x16 slots yields improved performance.
- d. E1150 (2 Socket) for NED24: C8/C11
- e. E1150 (2 Socket) for Nimitz: C8/C11, C7/C10

## 3.2.2  Non-supported drawers

With Power11 E1150, the following drawers are no longer supported:

- /SM590000 EMX0:

PCI Gen 3 I/O Expansion Drawer (#EMX0) is no longer supported with Power11. None of the Fanout Modules EMXF/EMXG/EMXH are supported within the new PCI Gen 4 I/O Expansion Drawer (#ENZ0).

- /SM590000 IBM EXP24SX:

The EXP24SX SAS storage enclosure (#ESLS/ ESLL) is no longer supported with Power11. For additional internal storage a new NED24 NVMe Expansion Drawer (#ESR0) is available, populated with up to 24 NVMe drives.

## 3.2.3  PCIe Gen 4 I/O Expansion Drawer

The 19-inch 4 EIA (4U) PCIe Gen 4I/O Expansion Drawer (#ENZ0) and two PCIe Fanout Modules (#ENZF) provide 12 PCIe I/O full-length, full-height Gen4 PCI slots. One Fanout Module provides 6 PCIe slots that are labeled C0 - C5. C0 through C3 are x16 slots, C4 and C5 are x8 slots. PCIe Gen1, Gen2, and Gen3 full-high adapters are also supported.

Important: PCI Gen 3 I/O Expansion Drawer (#EMX0) is no longer supported with Power11. Non of the Fanout Modules EMXF/G nor EMXH are supported within the new PCI Gen 3 I/O Expansion Drawer (#ENZ0).

A blind swap cassette (BSC) houses the full-high adapters that go into these slots. The BSC is the same as the one used with the previous generation EMX0 drawer. The drawer is shipped with a full set of BSCs.

A PCIe CXP converter adapter (#EJ24) that occupies one of the PCIe Gen5 slots in the system node and a pair of Active Optical Cables (AOCs) or Copper Cable are used for system node to Fanout Module connection. Both Fanout Modules are completely independent PCIe domains. They can be serviced independently to one another.A minimum of one Fanout Module is required in ENZ0 drawer in location P0 being placed at the left side of the drawer when viewed from behind. Each PCIe Gen 4 I/O Expansion Drawer has two power supplies.

Drawers can be added to a server dynamically. Concurrent repair and adding or removing expansion drawers and PCIe adapters is done through HMC-guided menus or by operating system support utilities.

Careful balancing of I/O, assigning adapters through redundant ENZ0 expansion drawers, and connectivity to different system nodes can help ensure high availability for I/O resources that are assigned to LPARs.

Figure 3-4 shows a PCIe Gen 4 I/O Expansion Drawer front view.

Figure 3-4   PCIe Gen 4I /O Expansion Drawer front view

<!-- image -->

Figure 3-5 shows the rear view of the PCIe Gen 4 I/O Expansion Drawer.

Figure 3-5   Rear view of a PCIe Gen 4 I/O Expansion Drawer

<!-- image -->

## Supported PCIe Adapters for ENZ0 expansion drawer

Table 3-6 lists the PCIe adapters supported on the ENZ0 expansion drawer.

Table 3-6   Adapters supported on the ENZ0 expansion drawer.

| Feature code   | CCIN   | Description                                            | FRU     |
|----------------|--------|--------------------------------------------------------|---------|
| EC2U           | 58FB   | PCIe3 2-port 25/10 Gb NIC & RoCE SFP28 adapter         | 01FT753 |
| EC6K           | 590F   | PCIe2 2-Port USB 3.0 Adapter                           | 02JD518 |
| EC72           | 2CF9   | PCIe4 2-port 25/10/1 Gb RoCE SFP28 adapter             | 03HD074 |
| EC74           | 2CF8   | PCIe4 2-port 25/10/1 Gb RoCE SFP28 with Crypto adapter | 03HD078 |
| EJ2B           | 57F2   | PCIe3 x8 SAS quad-port 12 Gb tape adapter              | 03MT000 |
| EJ37           | C0AF   | 4769-001 Cryptographic Coprocessor                     | 02JD572 |
| EN1A           | 578F   | PCIe3 x8 2-port Fibre Channel (32 Gb/s)                | 01FT704 |
| EN1J           | 579C   | PCIe4 x8 2-port Fibre Channel (32 Gb/s)                | 02CM909 |
| EN1L           | 2CFC   | PCIe4 x8 4-port Fibre Channel (32 Gb/s)                | 03HD014 |
| EN1N           | 2CFD   | PCIe4 x8 2-port Fibre Channel (64 Gb/s)                | 03HD020 |
| EN26           | EC2A   | PCIe4 x16 4-port 25/10/1 GbE RoCE SFP28 adapter        | 03HD066 |
| EN2L           | 2F06   | PCIe4 x16 4-port Fibre Channel (32 Gb/s)               | 03JP004 |
| EN2N           | 2F05   | PCIe4 x8 2-port Fibre Channel (64 Gb/s)                | 03JP010 |
| EN2W           | 2F04   | PCIe3 4-port 10 GbE adapter                            | 03JP016 |
| EPG6           | C138   | 4770 Cryptographic Coprocessor                         | 03JP117 |

Note: For additional requirements regarding the ENZ0 expansion drawer, supported adapters, and operating system levels, please refer to the Sales Manual.

## NED24 NVMe Expansion Drawer

IBM continues to provide industry-leading I/O capabilities with a PCIe direct-attached expansion drawer that supports NVMe drive attachment. The NED24 NVMe Expansion Drawer (#ESR0) is a storage expansion enclosure with 24 U.2 NVMe bays.

Important: The EXP24SX SAS Storage Enclosure is no longer supported with Power11.

Figure 3-6 shows the front view of a NED24 drawer with 4 NVMe Drives at slots C8-C11.

Figure 3-6   NED24 NVMe drawer front view

<!-- image -->

Figure 3-7 shows the rear view of the NED24 drawer with two power supplies and two Expansion Service Manager (ESM)

Figure 3-7   NED24 NVME drawer rear view

<!-- image -->

Each of the 24 NVMe bays in the NED24 drawer is separately addressable and can be assigned to a specific LPAR or Virtual I/O Server (VIOS) to provide native boot support for up to 24 partitions. At the time of writing, each drawer can support up to 153 TB.

Up to 24 U.2 NVMe devices can be installed in the NED24 drawer by using 15 mm Gen3 carriers. The 15-mm carriers can accommodate either 7 mm or 15 mm NVMe devices.

The NED24 drawer is supported in the Power E1150 by using the same interconnect card that is used for the PCIe Gen 4 expansion drawer. A maximum of two NED24 NVMe expansion drawers is supported per system when the E1150 has three or four sockets populated. Due to the reduced number of PCIe slots available in the two socket E1150, the two socket configuration only supports one NED24 drawer.

When mixing the different expansion drawers, the maximum number of drawers that are supported is based on the number of EJ24 fanout cards that are supported.

## Drive types supported

The IBM Power E1150 supports two main drive types for NVMe U.2 SSDs, each optimized for different workload profiles:

- /SM590000 Enterprise NVMe SSDs
- -Designed for high-performance, high-endurance workloads such as databases, analytics, and virtualization.
- -PCIe Gen4 NVMe U.2
- -15mm form factor
- -Higher write endurance (measured in Drive Writes Per Day - DWPD)

The enterprise NVMe feature codes are EC5V, EKF3, EKF5, EKF7, ES1E, ES3B, and ES4D,

- -Capacities: 800 GB, 1.6 TB, 3.2 TB, 6.4 TB
- -Operating System Support: AIX and Linux
- /SM590000 Mainstream NVMe SSDs
- -Optimized for read-intensive or mixed workloads, such as boot drives, application servers, or general-purpose storage.
- -PCIe Gen3 or Gen4 NVMe U.2
- -7mm form factor (EC5x and EC7T)
- -Lower write endurance, suitable for workloads with fewer write cycles.

The mainstream NVMe are Feature Codes EC5X, EC7T, and ECT9

- -Capacities: 800 GB, 15.3 TB
- -Operating System Support: AIX and Linux

## Additional Notes:

- /SM590000 The Carrier Conversion Kit (EC7X) allows 7mm drives to be used in 15mm bays, ensuring compatibility across the system.
- /SM590000 All NVMe drives are hot-pluggable.
- /SM590000 Concurrent Maintenance: is supported for NVMe drives, enabling non-disruptive replacement or upgrades.

Table 3-7 shows the NVMe drives supported in the E1150 and the NED24 (ESR0) NVMe expansion drawer

Table 3-7   Supported NVMe drives

| Feature Code   | Capacity   | Drive Type            | Drive Size   | Operating System Support   |
|----------------|------------|-----------------------|--------------|----------------------------|
| EC5V           | 6.4 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| EC5X           | 800 GB     | Mainstream PCIe3 NVMe | 7mm          | AIX/Linux                  |
| EC7T           | 800 GB     | Mainstream PCIe3 NVMe | 7mm          | AIX/Linux                  |
| ECT9           | 15.3 TB    | Mainstream PCIe4 NVMe | 15mm         | AIX/Linux                  |
| EKF3           | 1.6 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| EKF5           | 3.2 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| EKF7           | 6.4 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| EKF9           | 800 GB     | Enterprise PCIe4 NVMe | 7mm          | AIX/Linux                  |
| ES1E           | 1.6 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |

| Feature Code   | Capacity   | Drive Type            | Drive Size   | Operating System Support   |
|----------------|------------|-----------------------|--------------|----------------------------|
| ES1G           | 3.2 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES3B           | 1.6 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES3D           | 3.2 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES3F           | 6.4 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES3H           | 800 GB     | Enterprise PCIe4 NVMe | 7mm          | AIX/Linux                  |
| ES4B           | 1.6 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES4D           | 3.2 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES4F           | 6.4 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES5A           | 800 GB     | Enterprise PCIe4 NVMe | 7mm          | AIX/Linux                  |
| ES5C           | 1.6 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES5E           | 3.2 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |
| ES5G           | 6.4 TB     | Enterprise PCIe4 NVMe | 15mm         | AIX/Linux                  |

## 3.2.4  PCIe expansion card

The PCIe4 Expansion Cable Adapter (FC: EJ2A, CCIN: 6B99) is a full-height PCIe Gen4 cable adapter supported on the IBM Power E1150 server. It provides two ports for connecting expansion drawer cables. A single adapter supports either one Enclosure Services Manager (ESM) in an NED24 NVMe expansion drawer or one PCIe4 6-slot fanout module in an ENZ0 PCIe4 expansion drawer.

The supported cable adapter for IBM Power E1150 server is listed in Table 3-8. Figure 3-8 shows the PCIe Gen4 cable adapter (#EJ24).

Figure 3-8   PCIe Gen4 cable adapter with FC EJ24

| Feature code   | CCIN   | Description                          | FRU     |
|----------------|--------|--------------------------------------|---------|
| EJ2A           | 6B92   | PCIe Gen 4 PCIe Expansion Cable Card | 02WF534 |

Table 3-8   Supported cable adapters on the IBM Power E1150 server.

<!-- image -->

The EJ2A adapter card (Feature Code EJ2A, CCIN 6B99) is a PCIe Gen4 x16 cable adapter designed to provide high-speed connectivity between the system and supported I/O expansion drawers. It is a full-height, half-length card featuring two ports for connecting expansion drawer cables. The adapter can be installed in either PCIe Gen4 x16 or x8 slots; however, while using x8 slots allows for greater flexibility in connecting more drawers, it delivers lower performance compared to installation in full x16 slots. See Table 3-5 on page 71 for the recommended adapter locations depending on the expansion drawer being attached.

This adapter supports the attachment of:

- -One Enclosure Services Manager (ESM) in an NED24 NVMe expansion drawer
- -One PCIe4 6-slot fanout module in an ENZ0 PCIe4 expansion drawer

The EJ2A adapter is a key component for extending the PCIe bus from the IBM Power E1080 system to external I/O resources, enabling scalable configurations for high-performance workloads. It is equipped with two status LEDs: a green LED that indicates link status and an amber LED used for identifying the adapter during maintenance activities.

Each EJ2A adapter supports two CXP interface ports, which can be used with either optical or copper cables to connect to PCIe Gen4 expansion drawers. A single adapter connects to one fan-out module in the Gen4 expansion drawer or to one of the ESMs within the NED24 drawer. To utilize both fan-out modules in a Gen4 expansion drawer, two EJ2A adapters are required. Two adapters are also required when connecting a NED24 expansion drawer,

Internally, the EJ2A includes a built-in PCIe switch, which enables it to extend the E1080's internal PCIe Gen4 bus to external I/O drawers while maintaining bandwidth and performance integrity. The adapter's physical layout and connectivity are illustrated in Figure 3-9.

Figure 3-9   PCIe Expansion card

<!-- image -->

The number of supported cable card adapters for Power E1150 systems are listed in Table 3-5 on page 71.

## Cables

Two cables are required to attach each PCIe expansion card to the expansion drawer. Cables are available as either copper or active optical (AOC) with different lengths available. The two cables used to connect a card to an expansion drawer must be the same length.

Cables are ordered with a single feature code which includes two physical cables, either copper or optical. Table 3-9 lists the supported cables for the ENZ0 PCIe Gen4 I/O expansion drawer and IBM Power11 systems.

Restriction: You cannot mix copper and optical cables on the same expansion drawer. Either both fan-out modules use copper cables or both use optical cables.

Table 3-9   Supported cables for ENZ0 PCIe Gen4 I/O expansion drawer.

| FC   | CCIN   | FRU     | Description                             |
|------|--------|---------|-----------------------------------------|
| ECLS | C1B0   | 03NG620 | 3-meter expansion drawer cable (copper) |
| ECLX | C1B3   | 78P7688 | 3-meter active optical cable (AOC)      |
| ECLY | C1B2   | 78P7689 | 10-meter active optical cable (AOC)     |

The supported cables for the NED24 NVMe expansion drawer and IBM Power11 systems are listed in Table 3-10

Table 3-10   Supported cables for NED24 NVMe expansion drawer.

| FC   | CCIN   | FRU     | Description                             |
|------|--------|---------|-----------------------------------------|
| ECLS | C1B0   | 03NG620 | 3-meter expansion drawer cable (copper) |
| ECLX | C1B3   | 78P7688 | 3-meter active optical cable (AOC)      |
| ECLY | C1B2   | 78P7689 | 10-meter active optical cable (AOC)     |

The cable feature codes are:

- /SM590000 (#ECLS) - 3.0M CXP x16 Copper Cable Pair for PCIe4 Expansion Drawer
- This 3.0 meter cable pair connects a PCIe4 fan-out module in the PCIe Gen4 I/O Expansion Drawer to a PCIe4 Optical Converter Adapter in the system unit. There are two identical copper cables in the cable pair, each with two CXP connectors. One of the cables attaches to the top CXP port of the PCIe4 fan-out module and to the top CXP port of the PCIe4 Optical Converter Adapter. The other cable attaches to the bottom CXP ports.
- /SM590000 (#ECLX) - 3.0M Active Optical Cable x16 Pair for PCIe4 Expansion Drawer
- The 3.0 meter active optical cable (AOC) x16 pair connects a PCIe4 module in the PCIe Gen4 I/O Expansion Drawer to a PCIe4 Optical Converter Adapter in the system unit. There are two identical cables in the cable pair, each with two CXP connectors. One of the cables attaches to the top CXP port of the PCIe4 module and to the top CXP port of the PCIe4 Optical Converter Adapter. The other cable attaches to the bottom CXP ports.
- /SM590000 (#ECLY) - 10M Active Optical Cable x16 Pair for PCIe4 Expansion Drawer
- The 10 meter active optical cable (AOC) x16 pair connects a PCIe4 module in the PCIe Gen4 I/O Expansion Drawer to a PCIe4 Optical Converter Adapter in the system unit. There are two identical cables in the cable pair, each with two CXP connectors. One of the cables attaches to the top CXP port of the PCIe4 module and to the top CXP port of the PCIe4 Optical Converter Adapter. The other cable attaches to the bottom CXP ports.

The choice of copper or optical cables is dependent on your specific configuration and length of cables required. Use optical AOC cables features for cables which are much thinner and can be longer such as the features ECLX (3M optical), ECLY (10M optical).

## Active Optical Cables (AOC)

The AOC is constructed of a fiber cable and two active electrical-to-optical converter modules combined into one assembly, also known as CXP converters. The AOC has a minimum allowed bend radius of 1 in (25 mm).

Figure 3-10 shows the AOC connector.

Figure 3-10   AOC connector

<!-- image -->

Note: Use the 3 m cables for intra-rack installations. Use the 10 m cables for inter-rack installations.

## Connectivity to PCIe Expansion Drawer

A PCIe Gen4 I/O expansion drawer with two I/O fanout modules is connected to one host system node via two PCIe4 cable adapters with four expansion drawer cables (two expansion drawer cable pairs). One pair is used for each of the PCIe4 6-slot fanout modules.

Figure 3-1illustrates the connection of two expansion drawer cable pairs for two PCIe4 6-slot fanout modules.

Figure 3-11   Cabling setup for PCIe Gen4 expansion drawer

<!-- image -->

## Connectivity to NED24 expansion drawer

Figure 3-12 shows the connectivity to the NED24 expansion drawer. Note that both connections to the drawer need to be populated and be from the same system.

Figure 3-12   Connectivity to NED24 drawers

<!-- image -->

## 3.3  List of Supported Adapters on IBM Power E1150

This section discusses the various types and functions of the PCIe adapters that are supported by the IBM Power E1150 server.

This list is subject to change as more PCIe adapters are tested and certified, or listed adapters are no longer available.

The following sections describe the supported adapters and provide tables of orderable and supported feature numbers. The tables indicate operating system support (AIX and Linux) for each of the adapters.

The Order type table column in the following subsections is defined as:

Supported

Denotes that the feature is supported, but no longer orderable with a new system.

Both

Denotes the orderability of a feature as part of new and MES upgrade purchases.

The adapters supported for the E1150 server are listed in the Table 3-11.

Table 3-11 includes each adapter's Feature Code (FC), description, Customer Card Identification Number (CCIN) and FRU number.

Table 3-11   Adapters supported on the IBM Power E1150 server.

| Feature code   | CCIN   | Description                                     | FRU     | Order type   |
|----------------|--------|-------------------------------------------------|---------|--------------|
| EC2U           | 58FB   | PCIe3 2-port 25/10 Gb NIC & RoCE SFP28 adapter  | 01FT753 | Supported    |
| EC76           | 2CFB   | PCIe4 2-port 100 GbE RoCE x16 adapter           | 02CM921 | Both         |
| EC72           | 2CF9   | PCIe4 2-port 25/10/1 Gb RoCE SFP28 adapter      | 03HD074 | Both         |
| EN26           | EC2A   | PCIe4 x16 4-port 25/10/1 GbE RoCE SFP28 adapter | 03HD066 | Both         |

| Feature code   | CCIN   | Description                              | FRU     | Order type   |
|----------------|--------|------------------------------------------|---------|--------------|
| EC86           | EC2C   | PCIe5 x16 2-port 200 GbE RoCE adapter    | 03HD082 | Both         |
| EN2W           | 2F04   | PCIe3 4-port 10 GbE adapter              | 03JP016 | Both         |
| EN1J           | 579C   | PCIe4 x8 2-port Fibre Channel (32 Gb/s)  | 02CM909 | Both         |
| EN1A           | 578F   | PCIe3 x8 2-port Fibre Channel (32 Gb/s)  | 01FT704 | Both         |
| EN1L           | 2CFC   | PCIe4 x8 4-port Fibre Channel (32 Gb/s)  | 03HD014 | Both         |
| EN1N           | 2CFD   | PCIe4 x8 2-port Fibre Channel (64 Gb/s)  | 03HD020 | Both         |
| EN2L           | 2F06   | PCIe4 x16 4-port Fibre Channel (32 Gb/s) | 03JP004 | Both         |
| EN2N           | 2F05   | PCIe4 x8 2-port Fibre Channel (64 Gb/s)  | 03JP010 | Both         |
| EC6K           | 590F   | PCIe2 2-Port USB 3.0 Adapter             | 02JD518 | Both         |
| EJ35           | C0AF   | 4769-001 Cryptographic Coprocessor       | 02JD572 | Supported    |
| EJ37           | C0AF   | PCIe3 Crypto Coprocessor BSC-Gen3 4769   | 02JD572 | Supported    |
| EPG5           | C138   | 4770 Cryptographic Coprocessor           | 03JP117 | Both         |
| EJ2A           | 6B99   | PCIe4 cable adapter                      | 02WF534 | Both         |

## Transceiver (SFP) Replacement Support for Fibre Channel and Ethernet Adapter Features

IBM supports the replacement of transceivers (SFPs) for certain Fibre Channel and Ethernet adapter features. However, replacement is not supported for all features. In particular, Ethernet adapters designed specifically for copper media do not support conversion to optical transceivers.

Additionally, some Fibre Channel adapter features do not support SFP replacement, even if the transceivers appear physically removable. For these adapters, a separate SFP part is not available and cannot be ordered.

For the latest information on adapters that support replacement of the transceivers and those that do not see the following IBM support articles:

- /SM590000 Transceiver component (SFP) not replaceable for POWER Fibre Channel and Ethernet adapter features.
- /SM590000 Transceiver component (SFP) is replaceable for following POWER Fibre Channel and Ethernet adapter features.

## 3.3.1  Fibre Channel adapters

IBM Power11 processor-based Enterprise Midrange servers support connectivity to Fibre Channel (FC) devices, either directly or through a storage area network (SAN). A variety of PCIe-attached FC adapters are available, offered in both low-profile and full-height form factor

All supported Fibre Channel (FC) adapters use LC-type connectors. If you are connecting to a switch or device that uses an SC-type fiber connector, you will need either an LC-to-SC 50-micron fiber converter cable (feature code #2456) or an LC-to-SC 62.5-micron fiber converter cable (feature code #2459), depending on the fiber specification.

Table 3-12 lists the Fibre Channel (FC) adapters supported on the IBM Power E1150 server.

Table 3-12   Fibre Channel adapters supported on the IBM Power E1150 server.

| Feature code   | Description                                                                                             | OS support a   |
|----------------|---------------------------------------------------------------------------------------------------------|----------------|
| EN1J           | PCIe4 x8 2-port Fibre Channel (32Gb/s);(FCEN1JandFCEN1K;CCIN 579C); Adapter part number: 02CM909        | AIX and Linux  |
| EN1A           | PCIe3 x8 2-port Fibre Channel (32 Gb/s); (FC EN1A and EN1B); CCIN 578F); Adapter part number: 01FT704   | AIX and Linux  |
| EN1L           | PCIe4 x8 4-port Fibre Channel (32 Gb/s); (FC EN1L and FC EN1M; CCIN 2CFC); Adapter part number: 03HD014 | AIX and Linux  |
| EN1N           | PCIe4 x8 2-port Fibre Channel (64 Gb/s); (FC EN1N and FC EN1P; CCIN 2CFD); Adapter part number: 03HD020 | AIX and Linux  |
| EN2L           | PCIe4 x16 4-port Fibre Channel (32 Gb/s); (FC EN2L and EN2M; CCIN 2F06); Adapter part number: 03JP004   | AIX and Linux  |
| EN2N           | PCIe4 x8 2-port Fibre Channel (64 Gb/s); (FC EN2N and FC EN2P; CCIN 2F05); Adapter part number: 03JP010 | AIX and Linux  |

- a. Check for specific AIX or Linux OS requirements in the E1150 Sales Manual

## 3.3.2  Network adapters

To connect IBM Power E1150 server models to a local area network (LAN), supported LAN adapters can be installed in the system's PCIe slots. A variety of connection speeds and physical interfaces are available, depending on the selected adapter.

Table 3-13 lists the LAN adapters supported on IBM Power E1150 server.

Table 3-13   List of supported LAN adapters compatible with the IBM Power E1150 server.

| Feature code   | Description                                                                                                    | OS support a   |
|----------------|----------------------------------------------------------------------------------------------------------------|----------------|
| EC2U           | PCIe3 2-port 25/10 Gb NIC & RoCE SFP28 adapter (FC EC2T and EC2U; CCIN 58FB); Adapter part number: 01FT753     | AIX and Linux  |
| EC76           | PCIe4 2-port 100 GbE RoCE x16 adapter (FC EC75 and FC EC76; CCIN 2CFB); Adapter part number: 02CM921           | AIX and Linux  |
| EC72           | PCIe4 2-port 25/10/1GbRoCESFP28adapter(FCEC71and FCEC72; CCIN 2CF9); Adapter part number: 03HD074              | AIX and Linux  |
| EN26           | PCIe4 x16 4-port 25/10/1 GbE RoCE SFP28 adapter (FC EN24 and FC EN26; CCIN EC2A); Adapter part number: 03HD066 | AIX and Linux  |
| EC86           | PCIe5 x16 2-port 200 GbE RoCE adapter (FC EC85 and FC EC86; CCIN EC2C); Adapter part number: 03HD082           | AIX and Linux  |
| EN2W           | PCIe3 4-port 10 GbE adapter (FC EN2W and EN2X; CCIN 2F04); Adapter part number: 03JP016                        | AIX and Linux  |

- a. Check for specific AIX or Linux OS requirements in the E1150 Sales Manual

## 3.3.3  SAS and Tape adapters

The internal storage in IBM Power11 processor-based Enterprise Midrange servers is based on Non-Volatile Memory Express (NVMe) devices that are directly connected over PCIe. Additional storage expansion drawers can be connected to the system using Serial Attached SCSI (SAS) connections.

Table 3-14 lists the SAS adapters that are supported only on IBM Power E1150 servers when connected via the PCIe expansion drawer (ENZ0).

Table 3-14   Supported SAS adapters on the ENZ0 expansion drawer

| Feature code   | Description                               | OS support    |
|----------------|-------------------------------------------|---------------|
| EJ2B           | PCIe3 x8 SAS quad-port 12 Gb tape adapter | AIX and Linux |

## 3.3.4  NVMe options

This section lists the supported NVMe drives supported on the IBM Power E1150. Table 3-15 lists the Feature Codes and provides the size and OS support.

Table 3-15   List of supported NVMe drives compatible with the IBM Power E1150 server.

| Feature code   | Description                                  | FRUs                          | OS support    | Order types   |
|----------------|----------------------------------------------|-------------------------------|---------------|---------------|
| EC5X           | Mainstream 800 GB SSD PCIe3 NVMe U.2 module; | 02YC615                       | AIX and Linux | Supported     |
| EC7T           | 800GBMainstreamNVMeU.2SSD 4k;                | 02YC661                       | AIX and Linux | Supported     |
| ES5A           | Enterprise 800GB SSD PCIe4 NVMe U.2 module;  | TBD*                          | AIX and Linux | Both          |
| ES5C           | Enterprise 1.6 TB SSD PCIe4 NVMe U.2 module; | TBD*                          | AIX and Linux | Both          |
| ES4B           | Enterprise 1.6 TB SSD PCIe4 NVMe U.2 module; | 02YC896 or 02YC912*           | AIX and Linux | Both          |
| ES1E           | Enterprise 1.6 TB SSD PCIe4 NVMe U.2 module; | 01LU967                       | AIX and Linux | Supported     |
| ES3B           | Enterprise 1.6 TB SSD PCIe4 NVMe U.2 module; | 02YC725 or 02YC739* (FC EKF3) | AIX and Linux | Supported     |
| ES5E           | Enterprise 3.2 TB SSD PCIe4 NVMe U.2 module; | TBD*                          | AIX and Linux | Both          |
| ES4D           | Enterprise 3.2 TB SSD PCIe4 NVMe U.2 module; | 02YC897 or 02YC913*           | AIXandLinux   | Both          |
| ES1G           | Enterprise 3.2 TB SSD PCIe4 NVMe U.2 module; | 01LU968                       | AIX and Linux | Supported     |

| Feature code   | Description                                  | FRUs                          | OS support    | Order types   |
|----------------|----------------------------------------------|-------------------------------|---------------|---------------|
| ES3D           | Enterprise 3.2 TB SSD PCIe4 NVMe U.2 module; | 02YC726 or 02YC740* (FC EKF5) | AIX and Linux | Supported     |
| ES5G           | Enterprise 6.4 TB SSD PCIe4 NVMe U.2 module; | TBD                           | AIX and Linux | Both          |
| ES4F           | Enterprise 6.4 TB SSD PCIe4 NVMe U.2 module; | 02YC898 or 02YC914*           | AIX and Linux | Both          |
| EC5V           | Enterprise 6.4 TB SSD PCIe4 NVMe U.2 module; | 01LU969                       | AIX and Linux | Supported     |
| ES3F           | Enterprise 6.4 TB SSD PCIe4 NVMe U.2 module; | 02YC727 or 02YC741*           | AIX and Linux | Supported     |
| ECT9           | 15.3 TB Mainstream NVMe U.2 SSD 4k;          | 02YC749 or 02YC940* (FC EKF7) | AIX and Linux | Both          |

## Differences between mainstream and enterprise SSD

It is important to note that some of the NVMe options listed in Table 3-15 on page 83 are listed as mainstream drives and some are listed as enterprise drives. In this section, we explore the key differences between enterprise SSDs and mainstream SSDs (previously referred to as read-intensive SSDs). It is important to match the NVMe drive to your workload requirements, specifically the number of writes anticipated to be made to the device.

## /SM590000 Endurance and Use Case

Enterprise SSDs are designed for high-endurance workloads, supporting up to 10 Drive Writes Per Day (DWPD), typically using durable MLC NAND (formerly called eMLC). Mainstream SSDs, by contrast, usually support ~1 DWPD, making them suitable for read-heavy or less write-intensive applications.

## /SM590000 Over-Provisioning and Performance

Enterprise SSDs have more over-provisioned NAND capacity, which helps manage garbage collection, reduces write amplification, and improves random write performance and endurance. Mainstream SSDs have less over-provisioning, which reduces cost but impacts performance and lifespan during write-heavy operations.

## /SM590000 Cost

Mainstream SSDs are more cost-effective due to lower over-provisioning and endurance specs, offering lower cost per GB but reduced performance in write-intensive tasks.

## /SM590000 Mixing in RAID Arrays

IBM systems do not allow mixing of mainstream and enterprise SSDs in RAID arrays. The data striping performed by the PCIe SAS adapter assumes uniform endurance across all drives.

## /SM590000 Monitoring and Lifecycle Management

Mainstream SSDs must be monitored for end-of-life (EOL) indicators. When nearing EOL, a Predictive Failure Analysis (PFA) alert is triggered. The drive should be replaced promptly to avoid degraded performance or write failures.

## /SM590000 Warranty Considerations

Mainstream SSDs are not covered under warranty if they exceed their rated write lifecycle. Usage beyond rated endurance may result in degraded or failed write operations.

For more information refer to the following link:

https://www.ibm.com/docs/en/power11/9043-MRU?topic=SSYZWG/p11hal/p11hal\_read\_inten sive\_ssd.html

## 3.3.5  Internal USB ports and USB adapters

This section discusses the available USB ports and USB adapters as well as detailing the available USB devices.

## USB ports

In the IBM Power E1150 server, the first DCM (DCM0) also hosts the USB controller, which is connected via four PHBs, although only a single lane is utilized. DCM0 provides a total of four USB 3.0 ports - two located on the front and two on the rear of the system.

The two front USB ports are routed from the USB controller located on the TPM card, through the DASD backplane, and then to the system backplane, where the front USB cable is connected. These front ports support up to 1.5 A of current, primarily to accommodate devices such as the external USB DVD drive (Feature Code EUA5).

The two rear USB ports are routed from the same USB controller on the TPM card, passing through the system planar and the eBMC card, and finally to the eBMC tailstocks. Each of the rear USB ports supports up to 0.9 A of current.

Note: The USB controller is placed on the trusted platform module (TPM) card due to space reasons.

Some customers require that USB ports must be deactivated for security reasons. You can achieve this task by using the ASMI menu. For more information, see section 8.5, 'Managing the system by using the ASMI GUI' on page 154. The procedure is shown in Figure 8-6 on page 160.

## Devices supported

The stand-alone USB DVD drive (FC EUA5) is an optional, stand-alone external USB-DVD device. It requires high current at 5 V and must use the front USB 3.0 port on the system or a USB port on a USB 3.0 adapter.

## Media Compatibility:

- /SM590000 Read support: CD-ROM, CD-R, CD-RW, DVD-R, DVD+R, DVD-RW, DVD+RW, DVD-ROM, and DVD-RAM discs
- /SM590000 Write support: 4.7 GB DVD-RAM discs
- /SM590000 Read speeds:
- CD: up to 24X
- DVD-ROM: up to 8X
- DVD-RAM: up to 5X

- /SM590000 Write speed (DVD-RAM): up to 5X
- /SM590000 Buffer size: 0.75 MB (non-configurable)

## Specifications:

- /SM590000 Interface: USB
- /SM590000 Connector: USB 2.0
- /SM590000 Loading tray: supports 12 cm and 8 cm discs
- /SM590000 Operating positions: horizontal only (on a flat stable surface or floor)
- /SM590000 Form factor: Stand-alone USB DVD drive
- /SM590000 DVD video playback: not supported

Note: A USB extension cable (P/N 32N1311) is included. It is intended for use when there are no safe, flat surfaces available within the rack. The extension allows the drive to reach the floor. Alternative or additional extension cables are not supported, as the total USB cable length must not exceed 3 meters.

## PCIe2 2-Port USB 3.0 adapter

The PCIe2 2-Port USB 3.0 adapter is a high-performance, PCI Express (PCIe) Generation 2 expansion adapter. Feature Code (FC) EC6J is a low-profile adapter, while FC EC6K is a full-height adapter. Only the full-height adapter (FC EC6K) is supported on the E1150 server. This adapter is listed in Table 3-16. The adapters provides the following features and capabilities:

- /SM590000 Compliant with the PCIe Base Specification Revision 2.0
- /SM590000 Single-lane (x1) PCI Express interface with a throughput of up to 5 Gbps
- /SM590000 Single-slot, half-height, half-length PCIe2 form factor
- /SM590000 FCC Class A compliant
- /SM590000 Provides two external, downstream SuperSpeed USB 3.0 ports with Type A connectors
- /SM590000 Backward compatible with USB 2.0 and USB 1.1 devices
- /SM590000 Supports simultaneous operation of multiple USB 3.0, USB 2.0, and USB 1.1 devices

Table 3-16   List of supported USB adapters compatible with the IBM Power E1150 server.

| Feature code   | Description                                                                                 | OS support    |
|----------------|---------------------------------------------------------------------------------------------|---------------|
| EC6K           | PCIe2 2-Port USB 3.0 Adapter (FC EC6J and FC EC6K; CCIN 590F); Adapter part number: 02JD518 | AIX and Linux |

## 3.3.6  IBM PCIe Cryptographic Coprocessor

The IBM 4769 and 4770 Cryptographic Coprocessors, based on PCI Express (PCIe) Gen3 x4 adapters, are supported on Power E1150 servers. These adapters utilize the IBM Common Cryptographic Architecture (CCA) to accelerate cryptographic workloads. For more information about these cryptographic coprocessors, the associated software, and the IBM CCA, please refer to the IBM Systems Cryptographic HSMs website.

## PCIe Gen3 4770 Cryptographic Coprocessor

The 4770 is the latest and fastest generation of PCIe-based HSMs. It delivers top-tier security processing and high-speed cryptographic performance, providing high throughput to minimize latency and eliminate bottlenecks. Additionally, the 4770 supports FPGA updates and offers acceleration for Dilithium-based post-quantum cryptography.

- /SM590000 Enhanced Hardware to perform asymmetric, symmetric and hashing algorithms (300+).
- -Hardware Support for Quantum Safe Algorithms (CRYSTALS-Dilithium)

- /SM590000 Quantum Safe protected firmware using parallel signatures (ECDSA + CRYSTALS-Dilithium)

The Secure Key Adapter combines both cryptographic coprocessor and accelerator functionalities in a single PCIe card. The 4770 PCIe Cryptographic Coprocessor is designed for applications requiring high-speed, security-sensitive cryptographic operations, such as RSA acceleration, data encryption, and digital signing.

This adapter is also well-suited for secure key management and custom cryptographic applications. It provides tamper-resistant, secure hardware storage for cryptographic keys and is designed to comply with FIPS 140-2 Level 4 security requirements.

Note: Next Gen HSM (4770) adapter operates in dedicated mode only.

This adapter is available in both low-profile and full-height form factors. Feature Code (FC) EPG4 corresponds to a low-profile adapter, while EPG5 and EPG6 are full-height adapters. The different feature codes also indicate whether a blind-swap cassette is used, and the specific type of cassette.

- /SM590000 FC EPG5 is a full-height adapter without a blind-swap cassette.
- /SM590000 FC EPG6 is a full-height adapter that includes a Generation 3 blind-swap cassette.

## PCIe Gen3 4769 Cryptographic Coprocessor

The 4769 Cryptographic Coprocessor is a PCI Express (PCIe) Gen3 x4 adapter that provides both cryptographic coprocessor and cryptographic accelerator functions in a single card. It is designed for applications requiring high-speed, security-sensitive cryptographic operations, such as RSA acceleration, data encryption, and digital signing.

This adapter is also well-suited for secure management and usage of cryptographic keys, as well as for custom cryptographic applications. It offers secure key storage within a tamper-resistant hardware security module, engineered to meet FIPS 140-2 Level 4 security requirements.

Note: The 4769 operates exclusively in dedicated mode.

Feature codes EJ35 and EJ37 refer to identical adapter cards with the same CCIN (C0AF). The difference between them lies in the cassette configuration:

- /SM590000 FC EJ35 is provided without a blind-swap cassette.
- /SM590000 FC EJ37 includes a Generation 3 blind-swap cassette.

Table 3-17 summarizes the supported cryptographic coprocessor and accelerator adapters for IBM Power E1150 servers.

Table 3-17   Cryptographic Adapter Features for IBM Power E1150 server.

| Feature code   | Description                                                                                                                                    | OS support    |
|----------------|------------------------------------------------------------------------------------------------------------------------------------------------|---------------|
| EJ35           | 4769-001 Cryptographic Coprocessor (FC EJ35 and EJ37 for BSC; CCIN C0AF); Adapter part number: 02JD572                                         | AIX and Linux |
| EPG5           | 4770 Cryptographic Coprocessor (FC EPG4, FC EPG5, and FC EPG6; CCIN C138); Adapter part number: FC EPG4: 03JP116, FC EPG5 and FC EPG6: 03JP117 | AIX and Linux |

## 3.3.7  PCIe adapters that require increased cooling

Some PCIe adapters generate higher levels of heat and are often designated as 'hot adapters'. These adapters often have strict temperature limits. The Power E1150 server supports a variety of such adapters, and to ensure adequate cooling, the fan speed floor is increased when these adapters are installed. The minimum fan RPM varies depending on the specific adapter and the ambient temperature.

Note: Higher fan speed floors can result in a noticeable increase in overall system noise. Fan activity is often triggered by temperature readings from sensors located on the operator panel.

The IBM Power E1150 server is equipped with three sensors located on the operator panel to measure ambient temperature. The current readings from these sensors can be viewed through the ASMI (eBMC) interface by navigating to Hardware Status and selecting Sensors, as shown in Figure 3-13.

Figure 3-13   Ambient temperature readings viewed through the ASMI GUI (eBMC).

<!-- image -->

The firmware uses an internal algorithm to determine which ambient sensor value to use for fan control. For example, if the readings are as follows (see Figure 3-13):

- /SM590000 Ambient 0 Temp = 25.062°C
- /SM590000 Ambient 1 Temp = 25.422°C
- /SM590000 Ambient 2 Temp = 25.022°C

The firmware will select Ambient 0 Temp as the controlling value (25.062°C), as it represents the median reading among the three sensors.

On the E1150 server, if the intake air temperature (measured by the front-mounted sensors) exceeds 25°C but remains below 30°C, as shown in Figure 3-14 on page 89, the fan speed target is set to 5000 RPM as illustrated in Figure 3-14 on page 89, assuming no hot adapters are installed.

Note: The fan speed target and the actual speed readings (in RPM), as displayed in the ASMI GUI (eBMC) under the Sensors tab, may vary slightly from the expected values.

If hot adapters are present, the fan speed behavior is adjusted based on the Hot PCIe Adapters List and the corresponding ambient temperature range, available at:

https://github.com/openbmc/phosphor-fan-presence/blob/master/control/config\_files/ p10bmc/com.ibm.Hardware.Chassis.Model.Fuji/events.json

Figure 3-14   Fan speed readings in RPM as viewed through the ASMI GUI (eBMC) under the Sensors tab.

<!-- image -->

Note: PCIe4 cable adapters (FC EJ2A and EJ24) are not treated as standard PCIe hot adapters. Their presence in the system does not automatically trigger higher fan floor speeds. Instead, these adapters are equipped with temperature sensors, and the system firmware uses predefined thresholds to determine when to increase fan speeds to ensure adequate cooling.

Table 3-18 lists the PCIe adapters that require increased cooling and will result in higher fan speeds on the IBM Power E1150.

Note: The Hot Adapter List is continuously updated. To access the latest version, visit the public GitHub repository linked below. Navigate to your system specific 'com.ibm.Hardware.Chassis.Model.xxxxxx' directory and refer to the 'pcie\_cards.json' file for the most up-to-date list:

https://github.com/openbmc/phosphor-fan-presence/tree/master/control/config\_files/

Table 3-18   PCIe adapters that require increased cooling.

| Feature code   | CCIN   | Description                                    | FRU     |
|----------------|--------|------------------------------------------------|---------|
| EJ2A           | 6B99   | PCIe4 cable adapter                            | 02WF534 |
| EC2S           | 58FA   | PCIe3 2-port 10 Gb NIC & RoCE SR/Cu adapter    | 01FT759 |
| EC2U           | 58FB   | PCIe3 2-port 25/10 Gb NIC & RoCE SFP28 adapter | 01FT753 |
| EC66           | 2CF3   | PCIe4 2-port 100 GbE RoCE x16 adapter          | 01FT742 |
| EC76           | 2CFB   | PCIe4 2-port 100 GbE RoCE x16 adapter          | 02CM921 |
| EJ35           | C0AF   | 4769-001 Cryptographic Coprocessor             | 02JD572 |
| EJ37           | C0AF   | 4769-001 Cryptographic Coprocessor             | 02JD572 |

| Feature code   | CCIN   | Description                                                    | FRU     |
|----------------|--------|----------------------------------------------------------------|---------|
| EC86           | EC2C   | PCIe5 x16 2-port 200 GbE RoCE adapter                          | 03HD082 |
| EN2N           | 2F05   | PCIe4 x8 2-port Fibre Channel (64 Gb/s)                        | 03JP010 |
| EN2L           | 2F06   | PCIe4 x16 4-port Fibre Channel (32 Gb/s)                       | 03JP004 |
| EN26           | EC2A   | PCIe4 x16 4-port 25/10/1 GbE RoCE SFP28 adapter                | 03HD066 |
| EN1L           | 2CFC   | PCIe4 x8 4-port Fibre Channel (32 Gb/s)                        | 03HD014 |
| EN1N           | 2CFD   | PCIe4 x8 2-port Fibre Channel (64 Gb/s)                        | 03HD020 |
| EC63           | 2CF1   | PCIe4 x16 1-Port EDR 100 GB IB ConnectX-5 CAPI Capable Adapter | 00WT179 |
| EC65           | 2CF2   | PCIe4 x16 2-Port EDR 100 GB IB ConnectX-5 CAPI Capable Adapter | 00WT176 |
| EC3M           | 2CEC   | PCIe3 2-port 100 GbE NIC & RoCE QSFP28 Adapter                 | 00WT078 |

Note: The availability of PCIe adapters listed in this table may vary, and not all are offered for the IBM Power E1150 server.

## 3.4  Other device support

This section provides an overview of the available media features with Power E1150

## IBM System Storage 7226 Model 1U3 Multi-Media Enclosure

The IBM System Storage 7226 Model 1U3 Multi-Media Enclosure can accommodate up to two tape drives, or up to four DVD-RAM drives.

The IBM System Storage 7226 Multi-Media Enclosure offers a customer-replaceable unit (CRU) maintenance service to help make the installation or replacement of new drives efficient. Other 7226 components are also designed for CRU maintenance.

The IBM System Storage 7226 Multi-Media Enclosure is compatible with most Power8, Power9, Power10 and Power11 processor-based systems that offer current level AIX, and Linux operating systems.

Figure 3-15 shows the 7226 configured with two DVDs and one Tape Drive

Figure 3-15   7226-1U3 media drawer

<!-- image -->

The IBM System Storage 7226 Multi-Media Enclosure supports LTO Ultrium and DAT160 Tape technology, DVD-RAM, and RDX removable storage requirements on the following IBM systems:

- /SM590000 IBM Power9 processor-based systems
- /SM590000 IBM Power10 processor-based systems
- /SM590000 IBM Power11 processor-based systems

The IBM System Storage 7226 Multi-Media Enclosure offers an expansive list of drive feature options, as listed in Table 3-19.

Table 3-19   Supported drive features for the 7226-1U3

| Feature Code   | Description                                    | Status    |
|----------------|------------------------------------------------|-----------|
| #1420          | DVD-RAM SAS Optical Drive                      | Available |
| #1422          | DVD-RAM Slim SAS Optical Drive                 | Available |
| #5762          | DVD-RAM USB Optical Drive                      | Available |
| #5763          | DVD Front USB Port Sled with DVD-RAM USB Drive | Available |
| #5757          | DVD RAM Slim USB Optical Drive                 | Available |
| #8348          | LTO Ultrium 6 Half High Fibre Tape Drive       | Available |
| #8341          | LTO Ultrium 6 Half High SAS Tape Drive         | Available |
| #8441          | LTO Ultrium 7 Half High SAS Tape Drive         | Available |
| #8546          | LTO Ultrium 8 Half High Fibre Tape Drive       | Available |
| #EU03          | RDX 3.0 Removable Disk Docking Station         | Available |

The following options are available:

- /SM590000 LTO Ultrium 6 Half-High 2.5 TB SAS and FC Tape Drive: With a data transfer rate up to 320 MBps (assuming a 2.5:1 compression), the LTO Ultrium 6 drive is read/write compatible with LTO Ultrium 6 and 5 media, and read-only compatibility with LTO Ultrium 4. By using data compression, an LTO-6 cartridge can store up to 6.25 TB of data.
- /SM590000 The LTO Ultrium 7 drive offers a data rate of up to 300 MBps with compression. It also provides read/write compatibility with Ultrium 7 and Ultrium 6 media formats, and read-only compatibility with Ultrium 5 media formats. By using data compression, an LTO-7 cartridge can store up to 15 TB of data.
- /SM590000 The LTO Ultrium 8 drive offers a data rate of up to 300 MBps with compression. It also provides read/write compatibility with Ultrium 8 and Ultrium 7 media formats. It is not read/write compatible with other Ultrium media formats. By using data compression, an LTO-8 cartridge can store up to 30 TB of data.
- /SM590000 DVD-RAM: The 9.4 GB SAS Slim Optical Drive with an SAS and USB interface option is compatible with most standard DVD disks.

For a complete list of host software versions and release levels that support the IBM System Storage 7226 Multi-Media Enclosure, see IBM System Storage Interoperation Center (SSIC).

Note: Any of the existing 7216-1U2, 7216-1U3, and 7214-1U2 multimedia drawers are also supported.

For more information and technical description reference these sales manual pages.

## RDX Removable Hard Disk Cartridge

The RDX removable Hard Disk Cartridge is not supported with Power E1150. It can not be purchased to be used with this server.

There are alternative solutions to an RDX based Backup strategy that are being offered. Clients should consider:

- /SM590000 Cloud based Backup Services or
- /SM590000 On-Premise Entry Tape Drives and Libraries

<!-- image -->

Chapter 4.

4

## Artificial Intelligence Support

Artificial intelligence (AI) is becoming a cornerstone of digital transformation across industries, enabling organizations to automate processes, gain deeper insights, and deliver more personalized experiences. From predictive analytics and natural language processing to computer vision and generative AI, the demand for AI-driven solutions is rapidly growing. To support these workloads effectively, enterprises need infrastructure that can handle the computational intensity and data throughput AI requires - this is where IBM Power processor-based servers play a critical role.

IBM Power is purpose-built to support AI workloads with its high-performance architecture, including the latest Power11 processors that feature integrated AI acceleration. These processors are designed to handle low-precision arithmetic operations commonly used in AI models, significantly boosting inferencing speed without compromising accuracy. Additionally, IBM Power supports advanced technologies like IBMfi Spyre™ Accelerator, a PCIe-attached AI card optimized for enterprise AI workloads, and Power11's on-chip AI inferencing, which enables real-time decision-making at scale. These capabilities make Power processor-based servers ideal for deploying AI models in production environments where performance, reliability, and scalability are essential.

Beyond hardware, IBM Power integrates seamlessly with OpenShift AI, enabling organizations to build, train, and deploy AI models within a containerized, hybrid cloud environment. This combination allows for consistent DevOps practices, efficient resource utilization, and simplified management of AI workflows. With support for popular AI frameworks like PyTorch, TensorFlow, and vLLM, IBM Power provides a complete, enterprise-ready platform for operationalizing AI. Whether running AI at the edge, in the data center, or across hybrid cloud environments, IBM Power delivers the performance and flexibility needed to turn AI potential into real-world impact.

This chapter contains the following topics:

- /SM590000 On chip support
- /SM590000 AI Acceleration with IBM Spyre adapter
- /SM590000 AI Solutions on IBM Power11

## 4.1  On chip support

With the upcoming availability of the Spyre card on IBM Power-based servers, starting with systems powered by the new IBM Power11 processor, organizations will be able to take advantage of a new class of hardware acceleration designed to support a wide spectrum of enterprise workloads. The Spyre card will deliver significant performance benefits, particularly for compute-intensive and AI-driven use cases, while aligning with the architectural strengths of the IBM Power platform.The Spyre card is not a GPU, but a dedicated hardware developed by IBM Research Lab to provide superior acceleration.

A common misconception among IT decision-makers today is that one or more GPUs are always required for any workload involving artificial intelligence. This perception often leads to infrastructure decisions that prioritize GPU integration by default, regardless of the nature or scope of the AI tasks to be executed. However, GPUs are not a prerequisite for all AI use cases. Their inclusion in every server configuration may result in disproportionate acquisition costs, elevated power consumption, and increased complexity in thermal and workload management, without guaranteed benefits for the workload at hand.

Beginning with IBM Power10 chip and improved in IBM Power11, each core integrates four Matrix Math Accelerators (MMAs) capable of supporting a wide range of AI inference workloads directly on the CPU. This innovation enables customers to run AI models natively on IBM Power cores without requiring a discrete GPU. The architecture is particularly well-suited to 'traditional AI' use cases, such as fraud detection, text extraction, document analysis, domain adaptation via Retrieval-Augmented Generation (RAG), pattern recognition, forecasting, and image/video/audio processing.

The IBM Power11 processor further enhances these capabilities. While not always matching the raw throughput of high-end GPUs in certain generative AI (GenAI) scenarios, IBM Power11 technology provides excellent performance for tasks such as entity extraction, translation, summarization, and classification, all while offering lower energy consumption and improved data protection. By enabling AI workloads to run closer to where the data resides, IBM Power11 processor-based systems support secure, efficient, and scalable deployment of AI without unnecessary data movement.

Moreover, when the IBM Spyre card becomes available on IBM Power11 processor-based systems, it will introduce additional acceleration capabilities, complementing the on-chip features. IBM Power11 processor-based systems with their integrated AI accelerators and future support for IBM Spyre card present a highly optimized, energy-aware, and secure platform for deploying AI across a broad range of use cases, without the default dependency on discrete GPUs.

## 4.2  AI Acceleration with IBM Spyre adapter

Improvements to Power11 processor core strength and system capacity boost the performance of the MMA (Matrix-Math Accelerator) for inferencing workloads. Furthermore, with IBM Spyre accelerator into Power11 provide additional AI inferencing capabilities. Working together, IBM Power processors and the IBM Spyre accelerator will enable the next generation infrastructure to scale demanding AI workloads for businesses.

The IBM Spyre card extends the capabilities of Power11 processor-based systems by offering a low-power and high-efficiency acceleration path for workloads that demand frequent memory access and streamlined data movement.

Thanks to the advanced virtualization and workload consolidation capabilities of IBM Power processor-based servers, Spyre-based applications can be co-located with other mission-critical services within the same physical server. This allows AI inference engines, real-time analytics, or data preprocessing workloads that leverage IBM Spyre card to run in close proximity to databases or transactional systems hosted in LPARs or containers on the same server. This architectural proximity reduces latency, improves throughput, and eliminates the overhead typically associated with cross-node or cross-platform communication.

The IBM Spyre card will be uniquely positioned as the one of the only AI solutions that combines:

- -Data privacy: Data &amp; AI sovereignty on reliable, trusted on-premises infrastructure.
- -Skills: Ready-to-consume enterprise AI services.
- -Complexity: Accelerated plug &amp; play AI for business workflows.

Important: Support for the Spyre adapter on IBM Power is expected to be announced and available late in 2025. Statements regarding IBM's future direction and intent are subject to change or withdrawal without notice and represent goals and objectives only.

## 4.2.1  Deploying AI in the Enterprise

Artificial intelligence (AI) is transforming enterprise operations across industries, enabling organizations to optimize decision-making, streamline processes, and deliver personalized experiences at scale. However, the path to successful AI integration in enterprise environments is far from trivial. It demands not only the right algorithms and models but also a robust, secure, and scalable infrastructure tailored to diverse AI workloads. IBM's AI-optimized Power platform provides a unique approach to meet this challenge, offering both on-chip and off-chip acceleration capabilities for different levels of AI maturity.

Most enterprise AI adoption begins with experimentation and initial use cases, such as proof-of-concept models for customer segmentation, log analysis, or anomaly detection. These workloads typically require limited computational resources and can be efficiently handled by the on-chip AI acceleration embedded in IBM Power11 processors. This includes Matrix Math Assist (MMA) engines and SIMD vector instructions, combined with high memory bandwidth-a critical component for feeding data into AI models rapidly. On-chip acceleration enables real-time inference close to the data source, minimizing latency and reducing the need for additional hardware. It is particularly effective for traditional machine learning models, data warehouse analytics, and vector database operations (like RAG-style queries).

As enterprise AI use cases evolve to include more complex deep learning models-such as image classification, video processing, and time-series forecasting-the demands on compute performance and memory bandwidth increase significantly. At this stage, enterprises need a more flexible and powerful acceleration option, particularly as workloads move from experimentation to production.

## 4.2.2  Fit for Purpose Al Acceleration

To address this, IBM introduced IBM Spyre, a dedicated off-chip accelerator designed specifically for AI-intensive workloads. Compared to the on-chip accelerators in Power11, Spyre offers significantly higher throughput, parallelism, and model capacity, thanks to its dedicated memory architecture, optimized data paths, and ability to offload processing from the CPU. Spyre is ideal for large-scale transformer models, such as those used in generative AI applications-translation, summarization, sentiment analysis, and more. This architecture supports massive parallel compute operations with higher efficiency than general-purpose CPUs.

A key advantage of the IBM Power platform is its ability to combine both acceleration strategies-on-chip and off-chip-within a unified, enterprise-grade ecosystem. Early AI workloads can begin on existing Power processor-based servers with no added hardware, leveraging the built-in accelerators for cost-efficiency. As demands grow, the Spyre accelerators can be seamlessly integrated to boost performance without requiring a platform change or software rewrite. This flexibility protects existing investments while enabling future scaling this is shown in Figure 4-1.

Figure 4-1   IBM's fit for purpose AI architecture for Power

<!-- image -->

In high-value use cases like AI assistants and autonomous agents, where real-time interaction, low-latency inference, and contextual awareness are essential, off-chip acceleration becomes a requirement. These applications often rely on large language models (LLMs) and need to manage vast knowledge graphs or context windows, which exceed the practical limits of CPU-based inference alone. IBM Spyre is built precisely for such scenarios, and when paired with Power11 it provides a hybrid architecture that is secure, performant, and manageable within enterprise IT constraints.

Additionally, this fit-for-purpose infrastructure aligns well with existing on-premises environments, especially those where data sovereignty, latency, and compliance are non-negotiable. Unlike cloud-only solutions, the Power + Spyre combination gives enterprises full control over their data and compute stack, all while supporting AI workloads that rival hyperscale offerings in performance.

In conclusion, the approach to enterprise AI infrastructure stands out for its modularity and adaptability. Whether you're starting with lightweight models or scaling to multimodal AI agents, IBM Power processor-based servers allow you to deploy AI on your terms. With on-chip acceleration for efficient starting points and Spyre off-chip accelerators for high-performance growth, businesses gain the ability to move from AI exploration to enterprise-wide transformation-all on a platform built for the future of AI.

## 4.2.3  IBM Spyre adapter

The IBM Spyre Accelerator is a purpose-built enterprise-grade accelerator offering scalable capabilities for complex AI models and generative AI use cases. The new accelerator features 32 individual accelerator cores onboard, and each Spyre is mounted on a PCIe card. Jointly designed by IBM Research and IBM Infrastructure, Spyre's architecture is designed for more efficient AI computation. Notably, the chip will send data directly from one compute engine to the next, leading to an efficient use of energy. This family of processors also uses a range of lower precision numeric formats (such as int4 and int8), to make running an AI model more energy efficient and far less memory intensive.

IBM Spyre Is designed with a system-on-a-chip architecture optimized for enterprise AI workloads. Spyre is implemented in 5nm technology with a high performance and low-power design with 75W consumption and provides the following capabilities:

- -32 low-power AI cores.
- -Supports multi-precision for inference and training: FP16/8, INT8/4.
- -Enabled for Foundation Models and enabled in the Red Hat software stack (OCP AI)
- -Supports popular AI Framework and libraries (PyTorch, vLLM)

Figure 4-2 shows the IBM Spyre adapter.

Figure 4-2   The IBM Spyre adapter

<!-- image -->

This dedicated, enterprise-grade AI acceleration chip sits on a 75W PCIe adapter, surrounded by 128 GB of LPDDR5 memory - which is large memory to hold a wide variety of LLMs in support of the heterogeneous workloads that are typically seen on IBM Power. As a single Spyre adapter will not provide enough compute capacity for most use cases, the IBM solution will utilize current I/O expansion technology to attach a cluster of eight Spyre adapters in a single I/O expansion drawer, creating a logical cluster. The firmware on those eight cards will coordinate the distribution of compute, and transfer of data amongst the cards. Making the clusters appear to the software as one high-performance compute engine, with 1 TB of memory and 1.6TB/s of memory bandwidth.

## This is shown in Figure 4-3

Figure 4-3   Spyre cluster in an expansion drawer

<!-- image -->

This expansion drawer with Spyre adapters can be attached to any of the announced IBM Power11 servers, the number of supported drawers is dependent on the Power11 server that it is attached to.

At announcement, Spyre is planned to be available in a fixed configuration which includes eight Spyre adapters installed in the PCIe Gen4 I/O expansion drawer. The Sypre expansion drawer can be attached to any Power11 server: S1122, S1124, E1150, E1180. One expansion drawer will be supported on all models except the E1180 which will support up to two. New I/O drawer components such as power supplies and fan-out modules will be utilized to support the additional power requirements of the Spyre adapter configuration.

The initial supported software and hardware will be:

- -FW1110.10
- -HMC1111
- -RHEL 9.6
- -Spyre software stack container
- -OpenShift AI (Tech Preview 4Q 2025, GA 1Q 2026)

## 4.3  AI Solutions on IBM Power11

Artificial Intelligence is no longer an emerging trend, but it is a foundational pillar of competitive advantage. According to IBM Institute for Business Value, at the time of writing, about 72% of top-performing CEOs identify advanced generative AI as essential to their future success. However, only a fraction of enterprises have successfully moved beyond pilots and proofs of concept (PoCs) into full production deployments.

IBM Power11 processor-based server provides the AI-optimized infrastructure and integrated software stack required to scale AI workloads, secure enterprise data, and modernize mission-critical processes.

Organizations across industries are realizing tangible benefits from deploying AI on IBM Power:

- /SM590000 Logistics service providers can reduce order processing time by embedding GenAI into their ERP system;
- /SM590000 Financial institutions can accelerate anomaly detection by four times, while reducing TCO;
- /SM590000 Utility providers can reduce equipment downtime, and lower energy consumption using AI-powered visual inspection.

These outcomes highlight the value of deploying generative AI in real business workflows using the trusted IBM Power platform.

IBM Power is optimized for a wide range of AI workloads spanning key sectors:

- /SM590000 Finance and ERP:
- Fraud detection, AML, order processing, invoice compliance;
- /SM590000 Healthcare:

Medical transcription, claims-EHR matching, diagnostic imaging, assistant bots;

- /SM590000 IT and Development:

Predictive ITOps, code assistants (RPG, Ansible), transcription, documentation;

- /SM590000 Cross-Industry:

Document digitization, summarization, KYC, visual quality inspection, customer churn prediction, business intelligence;

Each use case leverages the inherent strengths of IBM Power, such as resilient compute, secure data access, and scalable architecture.

## 4.3.1  Barriers to scaling AI and the IBM Power technology advantage

While the promise of AI is great, common enterprise challenges persist, as described in the following table:

Table 4-1   challenges to scale AI

| Barrier                          | Description                                                              |
|----------------------------------|--------------------------------------------------------------------------|
| Data complexity                  | Fragmented formats, distributed sources, lack of standardized connectors |
| AI integration difficulty        | Legacy workflows, lack of AI skills, software maturity                   |
| Security and privacy             | Concerns around data leakage, adversarial threats, and model protection  |
| Infrastructure and cost concerns | High cost of AI development and fears of redesigning core systems        |
| Talent and skill gaps            | Shortage of data scientists and AI engineers                             |

IBM Power addresses these challenges by offering a complete AI platform, integrated into existing systems, with hardware acceleration, data fabric, and AI assistants.

## 4.3.2  IBM Power AI Portfolio: from Data to Business Value

To scale AI in production, IBM Power provides an AI-ready portfolio including:

- /SM590000 AI-Ready Infrastructure:

Built-in accelerators, support for hybrid and cloud-native deployment (e.g., PowerVS);

- /SM590000 Optimized software stack:

Red Hat, IBM, open source, and certified ISV AI solutions;

- /SM590000 data fabric:

Secure, unified access to enterprise data across silos;

- /SM590000 AI assistants and agents:

Tools to enhance developer productivity, automate ITOps, and streamline business workflows.

This stack empowers organizations to infuse AI into core business processes while maintaining governance, performance, and security.

## 4.3.3  Strategic outcomes enabled by IBM Power

The following table describes outcome examples enabled by the implementation of AI-solutions on IBM Power processor-based server:

Table 4-2   outcomes examples enabled by IBM Power

| Objective           | Outcome example                                                        |
|---------------------|------------------------------------------------------------------------|
| Transform processes | Accelerate ERP or financial workflows integrated with AI services      |
| Boost productivity  | Roll out new application features faster with GenAI code assistants    |
| Optimize data use   | Reduce TCO and improve resource efficiency via unified data governance |

IBM Power enables clients to put AI to work today securely, efficiently, and at enterprise scale.

## 4.3.4  Uniqueness of the platform

With its AI-optimized infrastructure and enterprise-proven architecture, IBM Power is uniquely positioned to help organizations move from experimentation to real AI-driven transformation. Its scalable ecosystem (spanning hardware, software, data, and automation) makes it a reliable platform for deploying production-grade AI at scale.

A key differentiator of the IBM Power11 processor-based platform lies in its ability to deliver AI capabilities without requiring external GPUs. Unlike conventional assumptions that all AI workloads demand dedicated GPU acceleration, IBM Power11 leverages advanced on-core accelerators integrated directly within each processor core, combined with a system-wide infrastructure design optimized for AI and data-intensive tasks.

This architectural approach enables the deployment of a wide range of AI use cases, such as inferencing, predictive analytics, and cognitive automation, while maintaining enterprise-grade attributes including resilience, reliability, performance, security, and sustainability. By eliminating or reducing dependency on external GPUs, organizations

benefit from simplified infrastructure, reduced energy consumption, and lower total cost of ownership (TCO), all without compromising AI readiness. The IBM Power11 platform is purpose-built to support scalable and secure AI solutions across diverse industry environments.

<!-- image -->

5

Chapter 5.

## Automation and Management

Managing IBM Power Systems effectively requires a combination of tools that provide automation, orchestration, and visibility across the infrastructure. At the core of this management stack is the Hardware Management Console (HMC), which serves as the central point for configuring and monitoring Power servers. HMC enables administrators to manage logical partitions (LPARs), perform firmware updates, and monitor system health. It provides both a graphical interface and a REST API, allowing integration with automation tools for more scalable operations.

Ansible and Terraform bring Infrastructure as Code (IaC) capabilities to IBM Power environments. Ansible is widely used for configuration management and automation of tasks such as patching, user management, and software deployment. It supports Power Systems through modules and collections tailored for AIX, IBM i, and Linux on Power. Terraform complements Ansible by enabling declarative provisioning of infrastructure components, including Power Virtual Servers and PowerVC-managed resources. Together, they allow teams to automate the full lifecycle of infrastructure - from provisioning to configuration ensuring consistency and reducing manual effort.

PowerVC (Power Virtualization Center) plays a crucial role in managing virtualized environments on IBM Power. Built on OpenStack, PowerVC provides advanced virtualization management capabilities such as image management, dynamic resource allocation, and integration with cloud platforms. It supports automation through REST APIs and integrates with both Ansible and Terraform, enabling seamless orchestration of virtual machines and workloads. By combining HMC, Ansible, Terraform, and PowerVC, organizations can build a robust, automated, and scalable management framework for their IBM Power infrastructure.

This chapter contains the following topics:

- /SM590000 Hardware Management Console overview
- /SM590000 Ansible
- /SM590000 Terraform
- /SM590000 PowerVC

## 5.1  Hardware Management Console overview

The hardware management console (HMC) is a hardware or virtual appliance that is used to configure and manage your systems. The HMC connects to one or more managed systems and provides capabilities for the following primary functions:

- /SM590000 Provide systems management functions, including the following examples:
- -Power off
- -Power on
- -System settings
- -Capacity
- -on Demand
- -Enterprise
- -Pools
- -Shared Processor Pools
- -Performance and Capacity Monitoring
- /SM590000 Starting Advanced System Management Interface (ASMI) for managed systems
- /SM590000 Deliver virtualization management through support for creating, managing, and deleting Logical Partitions, Live Partition Mobility, Remote Restart, configuring SRIOV, managing Virtual IO Servers, dynamic resource allocation, and operating system terminals.
- /SM590000 Acts as the service focal point for systems and supports service functions, including call home, dump management, guided repair and verify, concurrent firmware updates for managed systems, and around-the-clock error reporting with Electronic Service Agent for faster support.
- /SM590000 Provides appliance management capabilities for configuring network, users on the HMC, and updating and upgrading the HMC.

## 5.1.1  HMC Options

Power11 servers can be connected to either the 7063-CR2 HMC or to a Virtual HMC.

Restriction: The 7063-CR1 is not supported by the Power11 servers.

## HMC 7063-CR2

The 7063-CR2 IBM Power HMC (see Figure 2-1) is a second-generation Power processor-based HMC. It includes the following features:

- -6-core IBM Power9 130W processor chip
- -64 GB (4x16 GB) or 128 GB (4x32 GB) of memory
- -1.8 TB of internal disk capacity with RAID1 protection
- -4-ports 1 Gbps Ethernet (RJ-45), 2-ports 10 Gbps Ethernet (RJ-45), two USB 3.0 ports (front side) and two USB 3.0 ports (rear side), and 1 Gbps IPMI Ethernet (RJ-45)
- -Two 900W power supply units
- -Remote Management Service: IPMI port (OpenBMC) and Redfish application programming interface (API)

The base Warranty is 1-year 9x5 with available optional upgrades. A USB Smart Drive is not included.

Figure 5-1 is a picture of a 7063-CR2 HMC.

Figure 5-1   HMC 7063-CR2

<!-- image -->

The 7063-CR2 is compatible with flat panel console kits 7316-TF3, TF4, and TF5.

Note: The 7316-TF3 and TF4 are withdrawn from marketing

## Virtual HMC

Initially, the HMC was sold only as a hardware appliance, including the HMC firmware installed. However, IBM extended this offering to allow the purchase of the hardware appliance or a virtual appliance that can be deployed on ppc64le architectures or x86 platforms.

Any customer with a valid contract can download the HMC from the Entitled System Support , or it can be included within an initial Power S1122 or S1124 order.

The virtual HMC supports the following hypervisors:

- /SM590000 On x86 processor-based servers
- -KVM
- -Xen
- -VMware
- /SM590000 On Power processor-based servers
- -IBM PowerVM

The following minimum requirements must be met to install the virtual HMC:

- -16 GB of Memory
- -4 virtual processors
- -2 network interfaces (maximum 4 allowed)
- -1 disk drive (500 GB available disk drive)

For an initial Power S1122 or S1124 order with the IBM configurator (e-config), the HMC virtual appliance can be found by selecting Add software → Other System Offerings (as product selections) and then choosing:

- -5765-VHP for IBM HMC Virtual Appliance for Power V10
- -5765-VHX for IBM HMC Virtual Appliance x86 V10

For more information and an overview of the Virtual HMC, see Virtual HMC appliance (vHMC) overview . For more information about how to install the virtual HMC appliance and all requirements, see Installing the HMC virtual appliance .

## 5.1.2 BMC network connectivity rules for 7063-CR2 HMC

The 7063-CR2 HMC features a baseboard management controller (BMC), which is a specialized service processor that monitors the physical state of the system by using sensors. OpenBMC that is used on 7063-CR2 provides a graphical user interface (GUI) that can be accessed from a workstation that includes network connectivity to the BMC. This connection requires an Ethernet port to be configured for use by the BMC.

The 7063-CR2 provides two network interfaces (eth0 and eth1) for configuring network connectivity for BMC on the appliance.

Each interface maps to a different physical port on the system. Different management tools name  the  interfaces  differently.  The  HMC  task Console  Management → Console Settings → Change BMC/IPMI Network Settings modifies only the Dedicated interface.

The BMC ports are listed in Table 5-1.

Table 5-1 BMC ports

| Management tool                            | Logical port   | Shared/Dedicated   | CR2 physical port    |
|--------------------------------------------|----------------|--------------------|----------------------|
| OpenBMC UI                                 | eth0           | Shared             | eth0                 |
| OpenBMC UI                                 | eth1           | Dedicated          | Management port only |
| ipmitool                                   | lan1           | Shared             | eth0                 |
| ipmitool                                   | lan2           | Dedicated          | Management port only |
| HMC task (change BMC/IPMI Network settings | lan2           | Dedicated          | Management port only |

Figure 1-15 shows the BMC interfaces of the HMC.

Figure 5-2   BMC interfaces

<!-- image -->

The main difference is that the shared and dedicated interface to the BMC can coexist. Each has its own LAN number and physical port. Ideally, the customer configures one port, but both can be configured. The rules for connecting Power Systems to the HMC remain the same as for previous versions.

## 5.1.3 High availability HMC configuration

For the best manageability and redundancy, a dual HMC configuration is suggested. This configuration can be two hardware appliances, or one hardware appliance and one virtual appliance or two virtual appliances.

The following requirements must be met:

- -Two HMCs are at the same version.
- -The HMCs use different subnets to connect to the BMCs.
- -The HMCs can communicate with the servers' partitions over a public network to allow for full synchronization and function.

## 5.1.4 HMC code level requirements

The minimum required HMC version for the Power E1150 is V11R1 M1110. V11R1 M1110 is supported on 7063-CR2, and Virtual HMC appliances only. It is not supported on the 7063-CR1 or the 7042 machine types.

Note: HMC with V11R1M1110 cannot manage POWER7 processor-based systems.

An HMC that is running V11R1M1110 includes the following features:

- /SM590000 Support for managing Power11 Systems
- /SM590000 Support for new I/O adapters
- /SM590000 VIOS Management Enhancements:
- -Resource Groups
- -IBM i Secure Boot
- -Increase in platform keystore size
- -Remove support for vTPM 1.2
- -Quantum safe LPM
- -Minimum Affinity Score and actions
- /SM590000 Console Management and User Experience Improvements
- -User experience improvements spanning
- Network Topology
- Trusted Keystore
- Import Certificate
- Multi-factor Authentication Allow list
- -Ability to advertise device information via LLDP
- /SM590000 Power Infrastructure Maintenance and Automated Power Platform Updates
- -Execute Power platform updates with minimal touchpoints to enhance simplicity and reduce the risk of human error
- -Experience seamless platform updates with one touch solutions
- -Automatic operational recovery and resiliency

- /SM590000 Autonomous Error Resolution
- -Ability to collect platform logs (FW, hypervisor, HMC, VIOS) from a single interface
- -Ability to create a case and upload logs to the case from the HMC
- /SM590000 Sustainability
- -New energy efficiency mode - higher performance/ watt
- -Partition level energy monitoring
- Real time monitoring and reporting of energy and carbon emissions at the VM/partition level
- -Scheduling of energy modes

## 5.1.5 HMC currency

In recent years, cybersecurity emerged as a national security issue and an increasingly critical concern for CIOs and enterprise IT managers.

The IBM Power processor-based architecture has always ranked highly in terms of end-to-end security, which is why it remains a platform of choice for mission-critical enterprise workloads.

A key aspect of maintaining a secure Power environment is ensuring that the HMC (or virtual HMC) is current and fully supported (including hardware, software, and Power firmware updates).

Outdated or unsupported HMCs represent a technology risk that can quickly and easily be mitigated by upgrading to a current release.

## 5.1.6  New features

The minimum level of the HMC required to support Power11 is V11 R1 M1110. V11 of the HMC will not be supported on the 7063-CR1. HMC V11 will run on the 7063-CR2 or the virtual HMC. Additionally, HMC V11 will not support POWER8 or earlier servers.

HMC V11.1.1110 was GA (generally available) in July 2025. Specific new features include support for managing Power11 systems and support for new I/O adapters.

## Virtualization Management new features include:

- /SM590000 Resource Groups
- /SM590000 IBM i Secure Boot
- /SM590000 Increase in platform keystore size
- /SM590000 Remove support for vTPM 1.2
- /SM590000 Quantum safe LPM
- /SM590000 Minimum Affinity Score and Actions

## Console Management and User Experience Improvements include:

- /SM590000 User experience improvements spanning
- -Network topology
- -Trusted Keystore
- -Import certificate
- -Multi-factor authentication allow list
- /SM590000 Ability to advertise device information via LLDP

## Power Infrastructure Maintenance and automated Power Platform Updates

- /SM590000 Execute Power platform updates with minimal touchpoints to enhance simplicity and reduce the risk of human error. Includes the ability to update system firmware, VIOS and I/O adapters from a single update flow.
- /SM590000 Experience seamless platform updates with our advanced one-touch solutions, designed for both evacuation and return or in-place updates, each has automatic operational recovery and resiliency
- /SM590000 Validation for LPM and VIOS redundancy (VIOS maintenance readiness check)
- /SM590000 Ability to automatically migrate partitions and return as part of the update process.

## Autonomous Error Resolution

- /SM590000 Reduce problem resolution time with ability to collect platform logs (FW, Hypervisor, HMC, VIOS) from a single Interface
- /SM590000 Ability to create a case and upload logs to the case from the HMC

## Sustainability

- /SM590000 New Energy Efficiency mode - Higher Performance/Watt
- /SM590000 Partition Level Energy Monitoring - Real-time monitoring and reporting of energy and carbon emissions at the VM/partition level.
- /SM590000 Scheduling of energy modes

## 5.1.7  Using the Automated Maintenance tool

This section shows using the new automated maintenance tool provided in the IBM Power11 HMC.

## Launch Point

Prior to launching the new automated maintenance, the system should have the latest 11.10 firmware installed.

1. To start the process, we choose the option ' Update system, VIOS, adapter levels '. which is a new action menu introduced with Firmware 11.10. This is shown in Figure 5-3.

Figure 5-3   Start update

<!-- image -->

This will start a wizard that will guide us through all the process:

- -Import update files option: 'Import files to HMC filesystem and perform update' or 'Import files only'
- -Check System readiness - System needs to be in Ready state in order to proceed with update process
- -Select the source file location
2. Select ' Update/Upgrade ' type as shown in Figure 5-4.
3. In order to preserve the system availability, for disruptive upgrades/updates, the partitions migration panel will be shown as in Figure 5-5.

Figure 5-4   Upgrade type

<!-- image -->

Figure 5-5   Choose Process

<!-- image -->

4. These are the options that can be selected:
2. -Remote HMC switch enables migrating the partitions to a system managed by remote HMC. When selected partitions are not automatically migrated back to the system after the update.
3. -By default, all the partitions are selected for migration. Partitions can be individually selected to migrate by toggling Server evacuation switch.
4. -In case of selected partitions for migration, partition migration sequence can be specified for a set of partitions that needs to be migrated initially.

Finally, all the update process is automatically done, and we can see the results as shown in Figure 5-6.

Figure 5-6   Figure 5 - Update status

<!-- image -->

In conclusion this solution provides an unmatched Business Resiliency with Zero Planned Downtime delivered through platform automation capabilities integrated with IBM Concert's AI-infused workloads. It enables faster and more frequent maintenance updates, keeping systems secure, stable, and compatible with evolving software and hardware requirements, lowering risks of performance degradation, security breaches or unplanned downtimes.

## 5.2  Ansible

Ansible is an open-source, cross-platform tool for resource provisioning automation that DevOps professionals use for continuous deployment (CD) of software code by leveraging an IaC approach. The Ansible automation platform has evolved to deliver sophisticated automation solutions for operators, administrators, and IT decision-makers across various technical disciplines. It is an enterprise automation solution with flourishing open-source software. It operates on several UNIX like platforms, and can manage systems like UNIX and Microsoft architectures. It comes with descriptive language for describing system settings.

Because of the broad acceptance of the Ansible platform, its open-source design, and its wide support for many devices and platforms, it is becoming a dominant tool in the market. However, it is also common to use other automation tools with Ansible to do more complex automation. For example, many companies use Ansible with Terraform to provide automatic provisioning of their infrastructure.

## Ansible architecture

As shown in Figure 5-7, the Ansible architecture consists of an Ansible Controller and one or more Ansible client hosts. The controller runs automation tasks and houses Ansible collections, which contain modules, plug-ins, and roles defining the actions Ansible can perform on client nodes.

Figure 5-7   Simplified Ansible architecture

<!-- image -->

## Playbooks

The heart of Ansible Automation Ansible playbooks are YAML files that define sequences of tasks to run on remote hosts. These tasks can range from installing packages to configuring services or copying files. Playbooks enable IT teams to automate infrastructure provisioning, configuration management, application deployment, and more.

## Why choose Ansible

Ansible offers numerous benefits for IT professionals seeking to improve efficiency, scalability, and consistency in their infrastructure. Here are some key advantages:

- /SM590000 Versatility: Ansible supports a wide range of devices and can scale to accommodate growing environments and automation needs.
- /SM590000 Agentless architecture: Ansible manages devices by using Secure Shell (SSH), which eliminates the need for agents on target systems.
- /SM590000 Flexibility: Ansible can be used for simple CLI tasks and complex workflows that are defined in playbooks.
- /SM590000 Extensive module library: Ansible provides a rich collection of modules for managing various systems, cloud infrastructures, and OpenStack.

- /SM590000 Declarative approach: With the Ansible declarative syntax, you can define the state of a system, and Ansible takes the necessary steps to achieve it.
- /SM590000 Ease of learning: The Ansible YAML syntax and minimal learning curve make it accessible to IT professionals at all levels.

Ansible is a powerful automation tool that can help organizations improve efficiency, scalability, and reliability in their IT infrastructure. By leveraging Ansible playbooks, IT teams can streamline routine tasks, automate complex workflows, and help ensure consistent configurations across their environments.

## Options for implementing Ansible

As you decide to implement Ansible for IT management, it is essential to select the correct product and support level to meet your organization's needs. This section describes some of the options that are available to you.

## Ansible Community

The community versions of Ansible primarily include the following ones:

- /SM590000 Ansible Core

Ansible Core is a fundamental part of Ansible. It provides the core automation engine. It is an open-source tool that includes the basic functions for configuration management, application deployment, and task automation. Ansible Core includes modules, plug-ins, and the CLI that is needed to run playbooks and manage configurations.

- /SM590000 AWX

AWX is the upstream, open-source project that serves as the community version of Red Hat Ansible Tower. AWX provides a web-based UI, Representational State Transfer (REST) API, and task engine for managing Ansible automation at scale. AWX offers role-based access control (RBAC), job scheduling, graphical inventory management, and more. It helps users manage and scale automation efforts.

- /SM590000 Ansible Collections

Ansible Collections are prepackaged modules, roles, and plug-ins that are created and shared by the community. With Collections, users can extend Ansible functions with more content that is often maintained by the community or specific organizations. Collections can be downloaded from Ansible Galaxy, a community hub for sharing and discovering Ansible content.

- /SM590000 Ansible Galaxy

Ansible Galaxy is a repository for sharing and discovering Ansible roles and collections. It is a community-driven platform where users can find reusable Ansible content to simplify automation tasks. It provides a searchable repository of roles and collections that are created by the Ansible community, which can be integrated into your automation workflows.

These community versions are suitable for individual users, small teams, and development environments but lack the formal support and advanced features that are provided by Red Hat Ansible Automation Platform.

## Ansible Automation Platform

Ansible Automation Platform is a subscription-based enterprise solution that combines over 20 community projects into a fully supported automation platform. Ansible Automation Platform provides curated, certified, and validated Ansible Collections and roles from partners like IBM, Juniper, Cisco, and public cloud providers.

Here are the key considerations for choosing Ansible Automation Platform:

- /SM590000 Support level: Ansible Automation Platform offers enterprise-grade support, which includes SLAs for security, compatibility, and upgrades. Community options might have limited support.
- /SM590000 Features: Ansible Automation Platform includes features beyond Ansible Core, such as a web interface and integration with other tools.
- /SM590000 Cost: Ansible Automation Platform is a subscription-based product, but community options are available at no charge. Scale and complexity: For large organizations with complex automation needs, Ansible Automation Platform might be the better choice due to its enterprise-grade features and support.

By carefully evaluating these factors, you can select the Ansible offering that best aligns with your organization's goals, budget, and support requirements.

## Using Ansible to automate your IBM Power infrastructure

Ansible is a powerful automation tool that brings significant value to managing IBM Power Systems. Here are several key use cases where Ansible enhances efficiency, consistency, and scalability in Power environments:

1. System Configuration and Provisioning

Ansible automates the setup and configuration of AIX, IBM i, and Linux on Power systems. This includes tasks like user and group management, network configuration, software installation, and system tuning. By using Ansible playbooks, administrators can ensure consistent configurations across multiple systems, reducing manual errors and speeding up provisioning.

2. Patch Management and Compliance

Keeping systems up to date is critical for security and stability. Ansible can automate the patching process for AIX and IBM i, including downloading patches, applying them, and verifying system health post-update. It also supports compliance checks by validating system configurations against predefined baselines, helping organizations meet regulatory and security standards.

3. Integration with HMC and PowerVC

Ansible collections for IBM Power include modules that interact with the Hardware Management Console (HMC) and PowerVC. This enables automation of tasks such as LPAR creation, VIOS configuration, and virtual machine lifecycle management. Combined with dynamic inventory capabilities, Ansible can discover and manage Power infrastructure components in real time.

For more information on implementing automation with Ansible in an IBM Power environment refer to this IBM Redbook: Using Ansible for Automation in IBM Power Environments , SG24-8551

## 5.3  Terraform

Terraform is an open source tool originally developed by HashiCorp and now owned y IBM. It is written in the Go programming language and compiles down into an executable named Terraform. Terraform is an infrastructure as code tool that lets you build, change, and version cloud and on-premises resources safely and efficiently. Terraform provides a mechanism to access any API for any cloud provider to manage infrastructure as a service (IaaS).

Figure 5-8 shows the process involved in calling the API. The definition of which APIs to call is defined in configuration files. These configuration files are the code in that is referenced in Infrastructure as code.

Figure 5-8   Terraform functionality

<!-- image -->

Terraform allows users to define and provision infrastructure using a high-level configuration language called HashiCorp Configuration Language (HCL). With Terraform, infrastructure components such as servers, databases, networking, and storage can be described in code, enabling version control, collaboration, and repeatability. This approach eliminates the need for manual setup and reduces the risk of configuration drift across environments.

Terraform operates through a workflow that includes writing configuration files, initializing the working directory, planning changes, and applying them. The terraform init command sets up the environment by downloading necessary provider plugins. The terraform plan command then creates an execution plan, showing what actions Terraform will take to reach the desired state. Finally, terraform apply executes the plan, making the actual changes to the infrastructure. Terraform maintains a state file that tracks the current state of the infrastructure, which is essential for determining what changes need to be made during future runs.

One of Terraform's key strengths is its provider ecosystem, which allows it to manage resources across a wide range of platforms, including AWS, Azure, Google Cloud, Kubernetes, and many others. This makes it a powerful tool for managing hybrid and multi-cloud environments. Additionally, Terraform supports modules, which are reusable configurations that promote consistency and reduce duplication. By codifying infrastructure, Terraform enables DevOps practices such as continuous integration and delivery (CI/CD), infrastructure testing, and automated deployments.

Figure 5-9 on page 116 shows how Terraform works through defining your intended end point through configuration files which are then placed into a plan and finally applied through infrastructure providers.

Figure 5-9   Terraform process

<!-- image -->

## Track your infrastructure

Terraform generates a plan and prompts you for your approval before modifying your infrastructure. The state of your infrastructure is kept in a file named "terraform.tfstate", which can be held in Git, Gitlab or HCP Terraform to version, encrypt, and securely share it with your team. This acts as a single source of truth for your environment.

## Automate Changes

Terraform configuration files are declarative, describing the end state. So are easy to automate with tools like Ansible.

## Standardize configurations

Terraform provides standardization in modules. A module consists of a collection of .tf and .tf.json files kept together in a directory. Modules are the main way to package and reuse resource configurations with Terraform.

## Collaborate

Since Terraform can be distributed as configuration files and version controlled in applications like git, GitHub and HCP Terraform, these are ideal locations to share and collaborate.

## Combining Ansible and Terraform

Terraform connects to any provider, like Ansible, to manage you infrastructure. Browse the Terraform registry for providers. The Terraform Provider for Ansible provides a more straightforward and robust means of executing Ansible automation from Terraform rather a than local-exec 1 . Figure 5-10 shows the Ansible provider entry in the registry.

Figure 5-10   Terraform Ansible provider

<!-- image -->

The prerequisites for using the Ansible provider are as follows;

1. Install Go

For installation instructions, refer to the official installation guide .

2. Install Terraform:

Install the ppc64le version for operation on IBM Power. Installation instructions are found on the GitHub registry.

3. Install Ansible

To install Ansible refer to the Ansible official installation guide

## Track your infrastructure

Terraform generates a plan and prompts you for your approval before modifying your infrastructure. The state of your infrastructure is kept in a file named "terraform.tfstate", which can be held in Git, Gitlab or HCP Terraform to version, encrypt, and securely share it with your team. This acts as a single source of truth for your environment.

## Automate Changes

Terraform configuration files are declarative, describing the end state. So are easy to automate with tools like Ansible.

1   https://developer.hashicorp.com/terraform/language/resources/provisioners/local-exec

## Standardize configurations

Terraform provides standardization in modules. A module consists of a collection of .tf and .tf.json files kept together in a directory. Modules are the main way to package and reuse resource configurations with Terraform.

## Collaborate

Since Terraform can be distributed as configuration files and version controlled in applications like git, GitHub and HCP Terraform, these are ideal locations to share and collaborate.

For more information on Terraform in the IBM Power environment, refer to the IBM Redbook publication Modernization Techniques for IBM Power , SG24-8582

## 5.4  PowerVC

IBM PowerVC is an advanced virtualization and cloud management platform built on OpenStack, specifically designed for managing virtualized workloads on IBM Power Systems. It supports AIX, IBM i, and Linux operating systems and provides a unified interface for deploying, managing, and automating virtual machines (VMs) across Power environments. PowerVC simplifies the creation of private clouds and integrates seamlessly with higher-level cloud orchestrators, such as Ansible, Terraform, and Red Hat OpenShift, enabling hybrid cloud strategies.

PowerVC offers two editions: Standard Edition and Private Cloud Edition. The Standard Edition includes core virtualization management features such as:

- /SM590000 VM image capture, import/export, and deployment
- /SM590000 Policy-based VM placement for optimized resource utilization
- /SM590000 Snapshots and cloning for backup and testing
- /SM590000 Live VM mobility and remote restart for high availability
- /SM590000 Role-based access control and automated I/O configuration

The Private Cloud Edition builds on this by adding:

- /SM590000 A self-service portal for end users to provision VMs
- /SM590000 Approval workflows for provisioning requests
- /SM590000 Pre-built deployment templates
- /SM590000 Cloud management policies and metering for chargeback

PowerVC also supports Dynamic Resource Optimization (DRO), which automatically balances workloads based on CPU and memory usage, and Simplified Remote Restart, which ensures VMs can be restarted on alternate hosts in case of failure. These features reduce administrative overhead and improve system resilience.

NovaLink is a lightweight, Linux-based virtualization management interface that acts as a bridge between PowerVC (IBM's cloud and virtualization manager) and PowerVM (the hypervisor on IBM Power Systems). It enables direct, scalable, and efficient control of virtual machines (VMs) and system resources without relying solely on the traditional Hardware Management Console (HMC).

Installed on the Power System: NovaLink runs as a service on a dedicated Linux partition (LPAR) on the same Power server it manages. This LPAR communicates directly with the PowerVM hypervisor the HMC, which manages systems externally. NovaLink has direct

access to the hypervisor APIs. This allows for faster and more granular control of virtualization tasks like VM creation, deletion, migration, and resource allocation. This makes PowerVC more responsive and scalable, especially in environments with many VMs or frequent provisioning changes.

Table 5-2   Comparisons of PowerVC with HMC management:

| Feature/ Capability                   | PowerVC                                                                  | HMC                                            |
|---------------------------------------|--------------------------------------------------------------------------|------------------------------------------------|
| Policy-Based Placement                | Automatically places VMs based on resource availability and policies     | Manual placement                               |
| Integration with Cloud Tools          | Integrates with OpenStack, Red Hat OpenShift, and hybrid cloud platforms | No native integration                          |
| Live VM Migration                     | Fully supported and automated                                            | Supported but requires manual steps            |
| Dynamic Resource Optimization (DRO)   | Automatically balances workloads across hosts                            | Not available                                  |
| Simplified Networking & Storage Setup | Automated configuration of virtual I/O, storage, and networking          | Manual configuration required                  |
| NovaLink Support                      | Works directly with NovaLink for faster, scalable VM operations          | HMC is external and less scalable              |
| Scalability                           | Designed for large-scale environments with many VMs                      | Best suited for smaller or static environments |
| User Interface                        | Modern, web-based UI with dashboards and templates                       | Traditional interface, more administrative     |

.

<!-- image -->

Chapter 6.

6

## Operating Systems

The IBM E1150 supports two robust operating systems - AIX, and Linux - each tailored to meet distinct enterprise computing needs.

AIX, IBM's UNIX operating system, is designed for high-performance, scalable enterprise workloads. Built on a UNIX System V base and enhanced with IBM innovations, AIX offers advanced features like dynamic system tuning, workload partitioning, and robust security. It is widely used in environments that demand stability and performance, such as ERP systems, databases, and analytics platforms. AIX is optimized for IBM Power hardware, ensuring seamless integration and efficient resource utilization.

IBM Power also supports enterprise-grade Linux distributions, including Red Hat Enterprise Linux (RHEL) and SUSE Linux Enterprise Server (SLES). These Linux environments provide flexibility and open-source innovation, making them ideal for cloud-native applications, containerized workloads, and AI/ML development. Running Linux on Power enables organizations to leverage the performance advantages of the Power architecture while maintaining compatibility with the broader Linux ecosystem. Together, these operating systems make IBM Power a versatile platform capable of supporting a wide range of modern and legacy workloads.

Complementing these operating systems is Red Hat OpenShift, a Kubernetes-based container platform that runs natively on IBM Power Systems. OpenShift enables organizations to build, deploy, and manage containerized applications across hybrid cloud environments with consistency and scalability. OpenShift brings a unified DevOps experience and supports modern application architectures, including microservices and AI workloads. This integration empowers enterprises to modernize their IT infrastructure while leveraging the performance, security, and reliability of IBM Power.

This chapter provides the following topics:

- /SM590000 AIX
- /SM590000 Linux on IBM Power Systems
- /SM590000 Red Hat OpenShift
- /SM590000 PowerVM Virtual I/O Server
- /SM590000 Setting your LPAR compatibility mode

## 6.1  AIX

The AIX operating system is a secure, scalable, and robust open standards-based UNIX system. For over thirty years, AIX has been the cornerstone of mission-critical computing for enterprise organizations in highly complex industries, evolving to introduce a wealth of new hybrid cloud and open-source capabilities.

AIX 7.3 is the latest AIX release available in the market. It builds on a solid foundation by offering new functions and capabilities that further enhance performance, scalability, availability, and security, all while preserving application-binary compatibility to safeguard IT investments in AIX.

Coupled with the IBM Power11 processor-based systems, AIX 7.3 provides an optimized and more resilient computing platform that adapts to changing business demands, including new cloud use cases and improved economics.

## 6.1.1  AIX 7.3 Key Features

Here is a list of the key features in the latest AIX release.

- /SM590000 Workload Scalability &amp; Automation: AIX 7.3 offers enhanced workload scalability, improved cloud automation via Ansible, and over 300 open-source packages through the AIX Toolbox for Open Source Software, enabling modern application development.
- /SM590000 Live Kernel Update: Introduced in AIX 7.2 and enhanced in 7.3, this feature allows interim fixes, service packs, and technology level updates without reboots, supporting PowerVC-managed environments and Power Enterprise Pool systems for resource optimization.
- /SM590000 High Availability &amp; Disaster Recovery: IBM PowerHA and VM Recovery Manager provide automated recovery and multi-site replication, minimizing downtime and ensuring business continuity in hybrid or public cloud environments.
- /SM590000 AI Integration: AIX workloads are a natural source for AI. These systems host a tremendous amount of high-quality data on customer behavior and transactional information that can be further leveraged for AI development, enabling machine and deep learning for actionable insights on a unified platform.
- /SM590000 Cloud Flexibility: AIX enables private, on-premises cloud transformation with PowerVC, offering hybrid cloud functionality for seamless AIX VM import/export and software-defined infrastructure for SAN-less DevOps environments. Available via IBM Power Systems Virtual Server, AIX supports mission-critical databases with enhanced scalability, cloud automation, robust security, and flexible licensing. Workloads can run in hybrid or public clouds without refactoring, ensuring reliability and efficiency.

## 6.1.2  Supported Levels

Currently, the IBM Power11 processor-based server supports the following minimum levels of the AIX operating system when installed by using direct I/O connectivity:

- -AIX 7.3 with the 7300-03 Technology Level and Service Pack 1 or later
- -AIX 7.3 with the 7300-02 Technology Level and Service Pack 4 or later
- -AIX 7.2 with the 7200-05 Technology Level and Service Pack 10 or later

Currently, the IBM Power11 processor-based server supports the following minimum levels of AIX operating system when installed using virtual I/O:

- -AIX 7.3 with the Technology Level 7300-03 and Service Pack 0 or later
- -AIX 7.3 with the Technology Level 7300-02 and Service Pack 2 or later
- -AIX 7.2 with the Technology Level 7200-05 and Service Pack 8 or later

## Important:

- -Starting with AIX Release 7.3 Technology Level 3 Service Pack 1, AIX logical partitions operate in native Power11 processor compatibility mode, fully leveraging Power11's advanced capabilities.
- -Logical partitions running versions older than AIX Release 7.3 Technology Level 3 Service Pack 1 operate in POWER9 or Power10 processor compatibility mode on Power11 systems.
- -Logical partitions running AIX Release 7.2 can operate in POWER9 or Power10 processor compatibility mode.

## 6.1.3  AIX Maintenance levels

IBM periodically releases maintenance packages (service packs (SPs) or technology levels (TLs) for the AIX operating system. For more information about these packages, downloading, and obtaining the installation packages, see IBM Fix Central. For more information about hardware features compatibility and the corresponding AIX Technology Levels, see IBM Support.

The Service Update Management Assistant (SUMA), which can help you automate the task of checking and downloading operating system downloads, is part of the base operating system. For more information about the suma command, see Service Update Management Assistant (SUMA).

The Fix Level Recommendation Tool (FLRT) provides cross-product compatibility information and fix recommendations for IBM products. Use FLRT to plan upgrades of key components or to verify the current health of a system.

The IBM AIX Operating System Service Strategy and Best Practices is a free resource available to AIX clients and gives insight into the AIX service strategy, while also providing helpful lifecycle information to best maintain your version of AIX. For more information on AIX, visit AIX on IBM Power.

For additional information:

- /SM590000
- AIX support lifecycle information

https://www.ibm.com/support/pages/aix-support-lifecycle-information

- /SM590000 System Software Maps
- https://www.ibm.com/support/pages/system-software-maps
- /SM590000
- System to AIX maps
- https://www.ibm.com/support/pages/system-aix-maps

## 6.1.4  Licensing

The AIX operating system is available in the following editions:

- /SM590000 AIX Standard Edition
- /SM590000 AIX Enterprise Edition
- /SM590000 AIX Cloud Edition

The Enterprise and Cloud Editions include Power-related software, typically required to manage larger IBM Power environments, including those in hybrid clouds.

There are two licensing models for AIX:

- /SM590000 CPU-based licensing
- /SM590000 Subscription licensing

## CPU-based licensing

CPU-based AIX licenses can either be ordered together with the server, or purchased later as a MES upgrade for the server. The license grants you the right to use AIX on a specific server. If you require support for AIX, you must have a valid Software Maintenance Agreement (SWMA) for AIX.

## Subscription licensing

Subscription licensing is a new model that offers greater flexibility in AIX acquisition. The subscription license includes access to IBM software maintenance for a specified subscription term (1 or 3 years). After this term, you can renew the subscription if you continue using AIX.

This approach aligns with modern IT consumption trends, enabling businesses to scale usage up or down based on workload demands while simplifying budgeting and procurement processes. Subscription licensing also includes access to updates, patches, and IBM support, ensuring systems remain secure and up to date.

AIX Standard Edition, AIX Enterprise Edition, IBM Private Cloud Edition and IBM Private Cloud Edition with AIX are available under a subscription licensing model that provides access to an IBM program and IBM software maintenance for a specified subscription term (one or three years). The subscription term begins on the start date and ends on the expiration date, which is reflected at the IBM ESS website.

This model provides flexible and predictable pricing over a specific term with lower up front acquisition costs. Another benefit is that the licenses are customer-number-entitled, meaning you can use the licenses on any IBM Power server you have in your environment. You can even move them between on-premises, your private cloud, and public cloud if needed.

These subscriptions are available for deployment on IBM Power Systems, including Power11, and are particularly well-suited for hybrid cloud environments where agility and operational efficiency are key. By adopting subscription licensing, organizations can modernize their licensing strategy while continuing to leverage the reliability and performance of AIX.

The product IDs for the subscription licenses are listed in Table 6-1.

Table 6-1 Subscription license product IDs (one or three year terms)

| Product ID   | Description                                    |
|--------------|------------------------------------------------|
| 5765-2B1     | IBM AIX 7 Standard Edition Subscription 7.3.0  |
| 5765-2E1     | IBM AIX Enterprise Edition Subscription 1.10.0 |

| Product ID   | Description                                                 |
|--------------|-------------------------------------------------------------|
| 5765-2C1     | IBM Private Cloud Edition with AIX 7 Subscription 1.10.0    |
| 5765-6C1     | IBM Private Cloud Edition Subscription 1.10.0 (without AIX) |

The subscription licenses are orderable through IBM configuration tools. The AIX perpetual and monthly term licenses for standard edition are still available.

## 6.1.5  New AIX Editions

The IBM AIX operating system is an open standards-based UNIX operating system that has been the foundation of mission-critical workloads and databases for tens of thousands of customers for over 35 years. AIX provides an enterprise-class IT infrastructure that delivers the reliability, availability, performance, and security that is required for organizations to be successful in a global economy.

Now, IBM offers the following updates and enhancements to the AIX operating system and products that include AIX for IBM Power8, Power9, Power10 and Power11 technology based servers:

- /SM590000 IBM AIX 7 Enterprise Edition 1.13
- /SM590000 IBM Private Cloud Edition 1.13
- /SM590000 IBM Private Cloud Edition with AIX 1.13
- /SM590000 VM Recovery Manager 1.9

## IBM AIX 7 Enterprise Edition 1.13

IBM has updated the AIX 7 Enterprise Edition and its corresponding subscription offering to version 1.13. The bundled software components offered with AIX 7 Enterprise Edition 1.13 (5765-CD3 and 5765-2E1) now include:

- /SM590000 IBM AIX 7.3 TL3 or IBM AIX 7.2 TL5
- /SM590000 IBM PowerSC 2.3
- /SM590000 IBM PowerVC for Private Cloud 2.3.1
- /SM590000 IBM VM Recovery Manager HA 1.9
- /SM590000 IBM Tivolifi Monitoring 6.3

The bundle components are updated as follows:

- /SM590000 AIX 7.3 TL3 has been updated with service pack 1
- /SM590000 AIX 7.2 TL5 has been updated with service pack 10
- /SM590000 IBM PowerSC 2.3 has been updated from 2.2 to 2.3
- /SM590000 IBM PowerVC for Private Cloud has been updated from 2.3.0 to 2.3.1
- /SM590000 IBM VM Recovery Manager HA has been updated from 1.8 to 1.9

## Additional information

- /SM590000 Clients with active Software Maintenance (SWMA) or subscriptions for earlier versions of AIX Standard Edition or AIX Enterprise Edition are entitled to upgrade at no charge. To update, download, or install, see the IBM Entitled Systems Support website.
- /SM590000 Clients can choose either AIX 7.3 TL3 or AIX 7.2 TL5.
- /SM590000 Clients selecting AIX 7.2 TL5 can later upgrade to AIX 7.3 TL3 at any time, provided SWMA or subscription is current.
- /SM590000 Clients with AIX Enterprise Edition can trade up to IBM Private Cloud Edition with AIX.

## IBM Private Cloud Edition 1.13

IBM has updated the Private Cloud Edition and its corresponding subscription offering to version 1.13. The bundled software components offered with Private Cloud Edition 1.13 (5765-ECB and 5765-6C1) now include:

- /SM590000 IBM PowerSC 2.3
- /SM590000 IBM PowerVC for Private Cloud 2.3.1
- /SM590000 IBM VM Recovery Manager DR 1.9
- /SM590000 IBM Tivoli Monitoring 6.3

The bundle components are updated as follows:

- /SM590000 IBM PowerSC 2.3 has been updated from 2.2 to 2.3
- /SM590000 IBM PowerVC for Private Cloud has been updated from 2.3.0 to 2.3.1
- /SM590000 IBM VM Recovery Manager DR has been updated from 1.8 to 1.9
- /SM590000 Cloud Management Console (CMC) has been updated from 1.22 to 1.23.

## IBM Private Cloud Edition with AIX 1.13

IBM has updated Private Cloud Edition with AIX and its corresponding subscription offering to version 1.13. The bundled software components offered with Private Cloud Edition with AIX 1.13 (5765-CBA and 5765- 2C1) now include:

- /SM590000 IBM AIX 7.3 TL3 or IBM AIX 7.2 TL5
- /SM590000 IBM PowerSC 2.3
- /SM590000 IBM PowerVC for Private Cloud 2.3.1
- /SM590000 IBM VM Recovery Manager DR 1.9
- /SM590000 IBM Tivoli Monitoring 6.3

The bundle components are updated as follows:

- /SM590000 AIX 7.3 TL3 has been updated with service pack 1
- /SM590000 AIX 7.2 TL5 has been updated with service pack 10
- /SM590000 IBM PowerSC 2.3 has been updated from 2.2 to 2.3
- /SM590000 IBM PowerVC for Private Cloud has been updated from 2.3.0 to 2.3.1
- /SM590000 IBM VM Recovery Manager DR has been updated from 1.8 to 1.9
- /SM590000 Cloud Management Console (CMC) has been updated from 1.22 to 1.23.

## Additional information for IBM Private Cloud Edition

- /SM590000 Private Cloud Edition 1.13 and Private Cloud Edition 1.13 with AIX 7 include an entitlement for subscription to IBM Cloud Management Console (5765-CMT) for the same term as their SWMA.
- /SM590000 Clients with active SWMA or subscriptions for earlier versions of Private Cloud Edition or Private Cloud with AIX are entitled to upgrade at no charge. To update, download, or install, see the IBM Entitled Systems Support website.
- /SM590000 Clients can choose either AIX 7.3 TL3 or AIX 7.2 TL5.
- /SM590000 Clients selecting AIX 7.2 TL5 can later upgrade to AIX 7.3 TL3 at any time, provided SWMA or subscription is current.
- /SM590000 Clients with AIX Enterprise Edition can trade up to Private Cloud Edition with AIX.

## VM Recovery Manager 1.9

VM Recovery Manager provides automated VM management in the data center and disaster recovery management between sites. In IBM Power Virtual Server public the product is called DR Automation. DR Automation provides automated disaster recover management between geographically dispersed Power Virtual Server data centers.

- /SM590000 Work group support for HA-DR-HA
- /SM590000 Failover rehearsal for IBM Power Virtual Server

## 6.2  Linux on IBM Power Systems

Linux is a powerful, open-source, cross-platform operating system that runs on a broad spectrum of hardware - from embedded devices to mainframes - delivering a consistent, UNIX-like environment across diverse architectures. Its compatibility with IBM Power Systems offers a flexible and cost-effective alternative for running a wide range of applications, while taking full advantage of the platform's renowned performance, availability, and reliability.

As a free and open-source solution, Linux significantly reduces total cost of ownership by eliminating licensing fees and enabling deep customization. It supports modern technologies such as containers, Kubernetes, and cloud-native applications, making it ideal for building scalable, agile IT environments. Its high portability allows it to run seamlessly on architectures like x86, ARM, and IBM Power, and it is supported by a vast, active global community that provides extensive documentation, tools, and peer support.

IBM has been a leading advocate for Linux for over two decades, integrating it deeply into its enterprise ecosystem - including IBM Power Systems, IBM Zfi mainframes, and IBM Cloud. The company collaborates closely with major Linux distributions such as Red Hat, SUSE, and Canonical to ensure optimized performance and seamless integration. Through initiatives like LinuxONE and ongoing contributions to the Linux kernel and open-source projects, IBM continues to position Linux as a cornerstone of secure, modern infrastructure.

Security is another key strength of Linux. Its robust user privilege model, combined with community-driven updates and tools like SELinux and AppArmor, helps protect systems from vulnerabilities and malware. Linux is also highly efficient and lightweight, capable of running on everything from low-powered IoT devices to high-performance enterprise servers. Its modularity allows users to tailor the system to their specific needs - whether that means a minimal installation or a fully featured desktop environment.

Additionally, Linux provides access to a vast ecosystem of tools and software, some of which are exclusive or better supported on Linux than on other operating systems. Its open-source nature allows for greater flexibility and adaptability, making it easier to align with specific business or technical requirements. Linux also serves as a solid foundation for hybrid cloud infrastructure, supporting application modernization and deployment at scale. Its scalability, reliability, and cost-effectiveness make it especially well-suited for large enterprise environments and mission-critical workloads.

## 6.2.1  Supported distributions

The Linux distributions that are described next are supported on the Power E1150. Other distributions, including open source releases, can run on these servers, but do not include any formal Enterprise Grade support.

## Red Hat Enterprise Linux

RHEL on IBM Power servers delivers excellent scalability and performance for high-demand workloads such as big data, cloud, and SAP HANA. Power Systems' efficiency allows organizations to optimize resource usage and reduce costs by using fewer processor cores.

Red Hat can be deployed in both on-premises and cloud environments, giving organizations full control over their data and infrastructure with the flexibility to choose their preferred platform.

The latest version of the Red Hat Enterprise Linux (RHEL) distribution from Red Hat is supported in native Power11 mode, which allows it to access all of the features of the Power11 processor and platform.

At launch, IBM Power11 servers support:

- /SM590000 RHEL 10 is the latest Red Hat Enterprise Linux release with native Power11 support, fully leveraging the architecture's advanced features.
- /SM590000 RHEL 9.6 for Power LE or later (Power11 native support)
- /SM590000 RHEL 9.4 for Power LE or later (Power10 compatibility)
- /SM590000 RHEL 8.10 for Power LE or later (Power10 compatibility)

## Red Hat Enterprise Linux roadmap

IBM Power servers support Linux workloads while leveraging POWER hardware's performance, reliability, and availability. This section outlines supported enterprise Linux distributions. While other distributions may run on Power, only select enterprise versions offer formal support.

Red Hat Enterprise Linux (RHEL) is a leading open-source platform for Linux, hybrid cloud, containers, and Kubernetes. It provides secure, transparent, and proactive lifecycle management, supporting autonomous operations. Lifecycle planning is essential for customers, partners, ISVs, and the broader ecosystem. Starting with RHEL 8, the lifecycle includes three phases: Full Support, Maintenance Support, and Extended Life. Red Hat also shares projected release time lines and extended support details for minor versions. RHEL 8, 9, and 10 each offer a 10-year lifecycle across the first two phases, followed by Extended Life.

Figure 6-1   Red Hat Enterprise Linux support lifecycle

<!-- image -->

Note: The Red Hat Enterprise Linux life cycle phases are designed to minimize changes within each major release over time and ensure predictable availability and content.

- /SM590000 Full Support Phase

During this phase, Red Hat provides updates for:

- -Security issues (CVEs with CVSS greater or equal to 7) via RHSAs
- -Urgent and select high-priority bugs via RHBAs
- -Additional errata as needed

New or improved hardware support and select software enhancements may be included, typically in minor releases. These minor releases are cumulative and focus on resolving medium or higher-priority issues. Updated installation images are also provided.

- /SM590000 Maintenance Support Phase

For RHEL 8, 9, and 10, Red Hat continues releasing:

- -Security updates (CVEs with CVSS ? 7)
- -Urgent and selected high-priority bug fixes

New features and hardware support are not included during this phase.

- /SM590000 Extended Life Phase

Customers retain access to existing content via the Red Hat Customer Portal, including documentation and migration guidance. Only limited technical support is available - no new fixes, hardware support, or root-cause analysis is provided. Support applies to existing installations only, and Red Hat may end support at its discretion.

## Release Cadence

To provide predictability, minor RHEL releases are scheduled every six months during the Full Support Phase. Details on Extended Update Support (EUS) and SAP-specific services are included per release. Figure 6-2 shows the planned release schedule for RHEL8.

Figure 6-2   Release schedule for RHEL8

<!-- image -->

Figure 6-3 shows the planned release schedule for RHEL9.

Figure 6-3   Release schedule for RHEL9

<!-- image -->

Figure 6-4 shows the release schedule for RHEL10.

Figure 6-4   Release schedule for RHEL10

<!-- image -->

For additional details regarding the RHEL roadmap, please refer to the Red Hat Enterprise Linux Life Cycle. To find the latest Red Hat certified IBM Power servers, please refer to the Red Hat Certified Hardware catalog and System to Red Hat Enterprise Linux maps.

## SUSE Linux Enterprise Server

The latest version of the SUSE Linux Enterprise Server distribution of Linux from SUSE is supported in native Power11 mode, which allows it to access all of the features of the Power11 processor and platform.

At announcement, the Power E1150 servers support the following minimum levels of the SUSE Linux Enterprise Server operating system:

- /SM590000 SUSE Linux Enterprise Server 15 Service Pack 6, or later (Power11 native)

## SUSE roadmap

SUSE Linux Enterprise Server (SLES) is designed with long-term enterprise stability and support in mind. Its product lifecycle provides predictable and reliable maintenance phases, enabling organizations to plan deployments, updates, and migrations effectively.

- /SM590000 Lifecycle Duration

SLES offers a total lifecycle of 13 years per major release, divided into two main phases:

- -General Support (10 years): This phase includes full maintenance, security patches, bug fixes, hardware enablement, and new certified third-party software. It is ideal for production environments requiring continuous updates and active support.
- -Extended Support (3 years): After General Support ends, Extended Support offers critical security updates and selected bug fixes, allowing customers more time to transition to newer versions while maintaining operational security.
- /SM590000 Release Cadence
- -Major Releases: Every 4 years, introducing new features, platform support, and architectural improvements.
- -Service Packs (SPs): Released every 12 to 14 months, these include incremental updates, feature enhancements, and hardware enablement.

Each Service Pack is supported for six months after the release of the subsequent SP, providing customers time to validate and upgrade within a consistent and predictable window. Figure 6-5 shows the service pack schedule.

Figure 6-5   SLES major releases and service packs

<!-- image -->

Figure 6-6 shows the release lifecycle, including long term service pack support.

Figure 6-6   SLES long-term service pack support.

<!-- image -->

For additional details regarding the SUSE roadmap, please refer to the SUSE Linux Enterprise Server Documentation and Product Support Lifecycle. To find the latest SUSE certified IBM Power servers, please refer to the System to SUSE Linux Enterprise Server maps and SUSE YES Certified Hardware - Bulletin Search.

## Linux and Power11 technology

The Power11 specific toolchain is available in the IBM Advance Toolchain for Linux version 15.0, which allows customers and developers to use all new Power11 processor-based technology instructions when programming. Cross-module function call overhead was reduced because of a new PC-relative addressing mode.

One specific benefit of Power11 technology is a 10x to 20x advantage over Power9 processor-based technology for AI inferencing workloads because of increased memory bandwidth and new instructions. One example is the new special purpose-built matrix math accelerator (MMA) that was tailored for the demands of machine learning and deep learning inference. It also supports many AI data types.

## 6.2.2  Licensing

Linux licensing on IBM Power Systems follows the same foundational principles as on other platforms, but with considerations tailored to enterprise environments. The Linux kernel itself is licensed under the GNU General Public License version 2 (GPLv2), which ensures that the software remains free and open-source, allowing users to run, study, modify, and distribute it. On IBM Power, several major Linux distributions are supported, including Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Ubuntu. Each of these distributions adheres to open-source licensing models but may differ in how they package and deliver their software.

For example, RHEL and SLES provide access to source code under open-source licenses but require a paid subscription for access to precompiled binaries, updates, and enterprise support. This model allows organizations to benefit from open-source flexibility while receiving the stability and support needed for mission-critical workloads.

On IBM Power Systems, these distributions are optimized to take advantage of the architecture's performance, scalability, and reliability features, and licensing terms typically include support for virtualization technologies like PowerVM.

## Red Hat Licensing

Red Hat Enterprise Linux is sold on a subscription basis, with initial subscriptions and support available for one, three, or five years. Support is available directly from Red Hat or IBM Technical Support Services.

Red Hat Enterprise Linux 8 for Power LE subscriptions covers up to four cores and up to four LPARs, and can be stacked to cover a larger number of cores or LPARs.

When you order RHEL from IBM, a subscription activation code is automatically published in Enterprise Storage Server. After retrieving this code from Entitled Systems Support (ESS), you use it to establish proof of entitlement and download the software from Red Hat.

Red Hat Licensing information can be found here.

## SUSE Linux Enterprise Server Licensing

SUSE Linux Enterprise Server is sold on a subscription basis, with initial subscriptions and support available for one, three, or five years. Support is available directly from SUSE or from IBM Technical Support Services.

SUSE Linux Enterprise Server 15 subscriptions cover up to one socket or one LPAR, and can be stacked to cover a larger number of sockets or LPARs.

When you order SLES from IBM, a subscription activation code is automatically published in Entitled Systems Support (ESS), you use it to establish proof of entitlement and download the software from SUSE.

Clients are required to register the Linux offering purchased at the Distributor's website with the activation code. After registered, clients are able to download the software packages electronically and obtain the latest upgrades available for the product purchased.

SLES Licensing Information can be found here.

## 6.2.3  Introduction to KVM support

KVM stands for Kernel Virtual Machine, and its a widely spread virtualization technology in X86\_64, and also can be used in Power Servers. Even when IBM remains committed to the PowerVM being the premier enterprise virtualization software in the industry.

With KVM on Power, IBM will be targeting x86 customers on entry servers but will offer both KVM and PowerVM to meet the varying virtualization needs Power Linux customers. However, KVM virtualization technology represents an opportunity to simplify customer's virtualization infrastructure with a single hypervisor and management software across multiple platform.

The KVM Guests may run within a PowerVM LPAR, KVM can be used to run KVM guests. These guests are essentially VMs that run on top of the LPAR, using the existing resources of the LPAR.

On of the Benefits is that this approach combines the advantages of PowerVM's virtualization capabilities with the power and flexibility of KVM.

KVM guests running in a PowerVM LPAR have a unique runtime architecture, different from other virtualization mechanisms on IBM POWER system. For more information on KVM support on IBM Power see section 9.2, 'KVM support' on page 193.

## 6.2.4  Other Linux Distributions Available for IBM Power

In addition to the operating systems officially supported and certified by IBM for the IBM Power architecture, namely AIX, IBM i, Red Hat Enterprise Linux (RHEL), and SUSE Linux Enterprise Server (SLES), a variety of other Linux distributions have been released by the open-source community and third parties to run on IBM Power servers.

These alternative distributions, while not officially supported by IBM, can be installed and executed on IBM Power systems, particularly within LPARs (Logical Partitions) configured to run Linux workloads. So, in addition to supporting enterprise-grade operating systems, the IBM Power platform also provides an open environment for developers and enthusiasts to experiment with other Linux distributions. This flexibility allows users to explore their preferred Linux variants on IBM Power and take full advantage of the platform's performance, reliability, and security features.

By enabling access to alternative Linux distributions, IBM Power fosters innovation and broadens the community of contributors who can test, optimize, and run workloads on Power architecture, while continuing to benefit from the system's advanced infrastructure capabilities.

A useful tool for identifying Linux distributions available for IBM Power is the https://distrowatch.com/search.php#advanced search engine, which allows filtering by architecture. To find distributions compatible with IBM Power, it is recommended to select one of the following architectures during the search:

- /SM590000 Power
- /SM590000 PowerPC
- /SM590000 ppc64le
- /SM590000 ppc64el

At the time of writing, the following Linux distributions have been released to run on IBM Power LPARs (listed in order of popularity):

- -Debian
- -Ubuntu
- -Fedora
- -OpenSUSE
- -AlmaLinux
- -CentOS
- -Alpine Linux
- -Rocky Linux
- -ALT Linux
- -Chimera Linux
- -Gentoo Linux
- -T2 SDE
- -Trisquel GNU/Linux
- -OpenEuler
- -Bedrock Linux
- -Vine Linux
- -AdØlie Linux
- -UOS (Unity Operating System)

## 6.3  Red Hat OpenShift

Red Hat OpenShift on IBM Power provides a powerful platform for building, deploying, and managing containerized applications across hybrid cloud environments. Leveraging the performance and reliability of IBM Power architecture, OpenShift enables enterprises to modernize their IT infrastructure with cloud-native technologies like Kubernetes, containers, and microservices. With support for AIX, IBM i, and Linux, organizations can co-locate containerized workloads alongside traditional applications, reducing latency and improving data access. OpenShift's integration with IBM Cloud services and automation tools simplifies cluster provisioning, scaling, and lifecycle management, making it easier for developers to focus on innovation rather than infrastructure.

The latest release, Red Hat OpenShift 4.18, brings enhanced capabilities to IBM Power, including improved networking with User Defined Networks (UDNs), advanced Operator Lifecycle Management (OLM), and deeper GitOps integration 1. These features support more secure, scalable, and flexible deployments, whether on-premises or in the IBM Power Virtual Server cloud. OpenShift on Power also supports IBM Cloud Paks and AI workloads, enabling enterprises to accelerate digital transformation while maintaining enterprise-grade security and performance. This combination of technologies empowers businesses to innovate faster, optimize resource usage, and build a resilient hybrid cloud foundation.

For more details on how Red Hat OpenShift can improve your hybrid cloud experience see section 10.2, 'Red Hat OpenShift' on page 200.

## Licensing

Red Hat OpenShift Container Platform is offered via subscription, with terms available for 1, 3, or 5 years. Subscriptions are sold in increments covering two processor cores and can be stacked to match workload requirements. Support is available either directly from Red Hat or through IBM Technical Support Services.

When ordering OpenShift Container Platform for Power from IBM, the client receives a subscription activation code via the IBM Entitled Systems Support (ESS) portal. This code serves as proof of entitlement and is used to download the software directly from the Red Hat Customer Portal.

For further information and documentation, refer to the official Red Hat OpenShift Documentation.

## Supported Levels

The IBM Power E1150 server supports Red Hat OpenShift Container Platform 4.18 and later. Red Hat OpenShift 4.18 will support IBM Power11 when running with Power10 Compatibility mode at GA.

Full support for IBM Power11 native mode is planned for Red Hat OpenShift version 4.19.

## Red Hat OpenShift Container Platform roadmap

Red Hat provides a defined product life cycle for OpenShift Container Platform (OCP) to help customers and partners plan, deploy, and support their infrastructure. This life cycle is published for transparency, though exceptions may occur.

OpenShift v4 follows a phased, time-based life cycle, with at least four minor versions supported concurrently. Each minor version has a fixed support period, offering varying levels of maintenance. Red Hat targets a release cadence of every four months to support customer planning. All errata remain available to active subscribers throughout the life cycle.

Figure 6-7   Figure 4. OCP Life Cycle Phases

<!-- image -->

## Full Support

Begins at GA of a minor version and ends 6 months later or 90 days after the next minor version GA-whichever is later.

## During this phase:

- /SM590000 Full and Development Support are provided per the Scope of Coverage and SLA.
- /SM590000 Critical and Important security fixes (RHSAs) are released as needed.
- /SM590000 Urgent and high-priority bug fixes (RHBAs) are released promptly; others may be included in periodic updates.
- /SM590000 Customers must stay on a supported micro version (e.g., 4.x.z) to receive updates.

## Maintenance Support

Starts after Full Support and ends 18 months after the minor version GA.

## During this phase:

- /SM590000 Critical and Important RHSAs continue.
- /SM590000 Select urgent RHBAs may be released.
- /SM590000 Other fixes and enhancements (RHEAs) may be issued at Red Hat's discretion.
- /SM590000 No technical support is provided after this phase, except assistance with upgrading.

- /SM590000 Access to hosted services for unsupported versions is not guaranteed.

## Extended Update Support (EUS)

Even-numbered minor versions (e.g., 4.8, 4.10) are designated as EUS releases, offering extended support phases to simplify upgrades and reduce node reboots.

## EUS Add-On - Term 1 (6 Months)

- /SM590000 Optional support following Maintenance Support:
- /SM590000 Includes Critical/Important security updates and urgent bug fixes.
- /SM590000 Allows customers to stay on a minor release for up to 24 months.
- /SM590000 Included with Premium subscriptions (x86\_64) and available as an add-on to Standard. Contact your Red Hat Sales Representative for access or guidance.

## EUS Add-On - Term 2 (12 Months)

- /SM590000 Optional support after Term 1:
- /SM590000 Includes Critical/Important updates and urgent fixes for platform-aligned Operators and selected OCP components.

With both Term 1 and Term 2, support extends up to 36 months for stable, mission-critical environments.

Table 6-1shows the dates for current versions of Red Hat OpenShift Container Platform.

Table 6-1   OCP Life Cycle Dates

| Version                                                                | General availability                                                   | Full support ends                                                      | Maintenanc e support ends                                              | Extended Update Support Add-On - Term 1 ends                           | Extended Update Support Add-On - Term 2 ends                           | Extended life phase ends                                               |
|------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|
| Full Support Openshift Container Platform 4 Full Support               | Full Support Openshift Container Platform 4 Full Support               | Full Support Openshift Container Platform 4 Full Support               | Full Support Openshift Container Platform 4 Full Support               | Full Support Openshift Container Platform 4 Full Support               | Full Support Openshift Container Platform 4 Full Support               | Full Support Openshift Container Platform 4 Full Support               |
| 4.19                                                                   | 17-Jun-25                                                              | GA of 4.20 + 3 Months                                                  | 17-Dec-26                                                              | N/A                                                                    | N/A                                                                    | N/A                                                                    |
| 4.18                                                                   | 25-Feb-25                                                              | 17-Sep-25                                                              | 25-Aug-26                                                              | 25-Feb-27                                                              | 25-Feb-28                                                              | N/A                                                                    |
| Maintenance Support OpenShift Container Platform 4 Maintenance Support | Maintenance Support OpenShift Container Platform 4 Maintenance Support | Maintenance Support OpenShift Container Platform 4 Maintenance Support | Maintenance Support OpenShift Container Platform 4 Maintenance Support | Maintenance Support OpenShift Container Platform 4 Maintenance Support | Maintenance Support OpenShift Container Platform 4 Maintenance Support | Maintenance Support OpenShift Container Platform 4 Maintenance Support |
| 4.17                                                                   | 1-Oct-24                                                               | 25-May-25                                                              | 1-Apr-26                                                               | N/A                                                                    | N/A                                                                    | N/A                                                                    |
| 4.16                                                                   | 27-Jun-24                                                              | 1-Jan-25                                                               | 27-Dec-25                                                              | 27-Jun-26                                                              | 27-Jun-27                                                              | N/A                                                                    |
| 4.15                                                                   | 27-Feb-24                                                              | 27-Sep-24                                                              | 27-Aug-25                                                              | N/A                                                                    | N/A                                                                    | N/A                                                                    |
| Extended Support OpenShift Container Platform 4 Extended Support       | Extended Support OpenShift Container Platform 4 Extended Support       | Extended Support OpenShift Container Platform 4 Extended Support       | Extended Support OpenShift Container Platform 4 Extended Support       | Extended Support OpenShift Container Platform 4 Extended Support       | Extended Support OpenShift Container Platform 4 Extended Support       | Extended Support OpenShift Container Platform 4 Extended Support       |
| 4.14                                                                   | 31-Oct-23                                                              | 27-May-24                                                              | 1-May-25                                                               | 31-Oct-25                                                              | 31-Oct-26                                                              | N/A                                                                    |
| 4.12                                                                   | 17-Jan-23                                                              | 17-Aug-23                                                              | 17-Jul-24                                                              | 17-Jan-25                                                              | 17-Jan-26                                                              | N/A                                                                    |

Note: Red Hat OpenShift Container Platform (RHOCP) 4 uses Red Hat Enterprise Linux CoreOS (RHCOS) as its managed node operating system. RHCOS is updated during cluster upgrades, which may include changes between RHEL minor versions.

For additional details regarding the Red Hat OpenShift Container Platform roadmap, please refer to the Red Hat OpenShift Container Platform Life Cycle Policy and Product Life Cycles.

## 6.4  PowerVM Virtual I/O Server

IBM PowerVM software is a virtualization environment that can run AIX, IBM i and Linux virtual machines on IBM Power servers. Businesses are turning to server virtualization to consolidate multiple workloads onto fewer systems, increase server utilization and reduce costs. PowerVM provides a secure and scalable server virtualization environment for your applications, built upon the advanced RAS features and leading performance of the IBM Power systems platform.

PowerVM is designed to protect and isolate critical workloads through a highly secure, enterprise-grade hypervisor. It enforces strong workload isolation and I/O integrity, ensuring the reliability of mission-critical applications. With robust automation capabilities, PowerVM accelerates service delivery by streamlining the provisioning and management of virtual machines and storage resources, making it well-suited for cloud-based infrastructures. It also enhances operational efficiency and maximizes return on investment through features like live partition mobility, which enables zero-downtime workload migration, and resource optimization tools that improve the utilization of compute and storage infrastructure.

The Virtual I/O Server (VIOS) is part of the PowerVM Editions hardware feature. The VIOS is a software that is located in a logical partition and facilitates the sharing of physical I/O resources between client logical partitions within the server.

Currently, the IBM Power11 processor-based server supports the following minimum levels of the PowerVM VIOS operating system:

- /SM590000 VIOS 4.1.1.10
- /SM590000 VIOS 4.1.0.40
- /SM590000 VIOS 3.1.4.60

## 6.5  Setting your LPAR compatibility mode

IBM Power Processor Compatibility Mode is a feature that enables newer IBM Power Systems to run applications and operating systems originally compiled for earlier generations of Power processors. This is especially important when transitioning to the latest hardware, such as Power11, where some operating systems and applications may not yet support native execution. In such cases, compatibility mode allows these workloads to run in a Power10 environment, ensuring continuity and minimizing disruption during upgrades.

The compatibility mode is defined at the logical partition (LPAR) level during the creation of the LPAR. Administrators can specify which processor generation the partition should emulate, allowing multiple workloads to operate in different compatibility modes on the same physical server. This flexibility is particularly useful in mixed environments where not all software has been updated to support the latest architecture.

While running in a previous-generation compatibility mode helps maintain application portability and eases the migration process, it may restrict access to newer processor features and performance enhancements. For this reason, IBM recommends using compatibility mode as a transitional solution. The long-term goal should be to recompile applications and upgrade operating systems to fully leverage the advanced capabilities and performance improvements of the latest Power processor architecture.

Figure 6-8 shows how to set compatibility mode in your LPAR definition.

Figure 6-8   Set LPAR processor compatibly mode.

<!-- image -->

<!-- image -->

Chapter 7.

7

## Enterprise Solutions

IBM Power servers are engineered to support a wide range of enterprise workloads that demand high availability, performance, and scalability. These systems are ideal for mission-critical applications such as SAP HANA, Oracle, and IBM Db2fi, where consistent uptime and fast transaction processing are essential. With advanced RAS (Reliability, Availability, and Serviceability) features and capabilities like live partition mobility, Power servers ensure minimal downtime and seamless workload management. They are also well-suited for enterprise resource planning (ERP) and customer relationship management (CRM) systems, offering the processing power and memory capacity needed to handle large-scale, real-time data operations.

In addition to traditional workloads, IBM Power servers are optimized for modern enterprise needs, including artificial intelligence (AI), machine learning, and hybrid cloud deployments. Built-in accelerators and support for containerized environments like Red Hat OpenShift allow organizations to run AI models and cloud-native applications efficiently. These servers also excel in high-performance computing (HPC) scenarios, such as financial modeling, scientific research, and health care analytics, thanks to their high core counts and memory bandwidth. With robust security features like transparent memory encryption and secure boot, IBM Power is a trusted platform for industries with strict compliance requirements. Overall, Power servers provide a flexible, secure, and cost-effective foundation for evolving enterprise IT landscapes.

This chapter contains the following topics:

- /SM590000 High Availability and Disaster Recovery Solutions
- /SM590000 IBM Db2
- /SM590000 Oracle
- /SM590000 SAP HANA
- /SM590000 Banking
- /SM590000 Health Care

## 7.1  High Availability and Disaster Recovery Solutions

IBM offers a suite of high availability and disaster recovery (HA/DR) solutions tailored to its Power Systems platform, each designed to meet different operational needs and workloads. PowerHA SystemMirror for AIX is a clustering solution that ensures application uptime through an active-standby model, enabling fast failover and minimal disruption during outages or maintenance. For IBM i environments, PowerHA SystemMirror for i provides tightly integrated HA/DR capabilities using features like Independent Auxiliary Storage Pools (IASPs) and real-time data replication to protect mission-critical workloads. Complementing these is VM Recovery Manager (VMRM), a flexible solution that supports AIX, IBM i, and Linux workloads by managing full LPAR recovery across systems. VMRM is ideal for environments where full system recovery is acceptable, offering automated failover and disaster recovery without requiring application-level configuration. Together, these tools provide a comprehensive HA/DR strategy across the IBM Power ecosystem.

## 7.1.1  PowerHA SystemMirror for AIX

PowerHA SystemMirror for AIX, generally known as PowerHA, is a high availability clustering solution to ensure application uptime and fast recovery. It is considered an active-standby or active-passive system of application availability. It is also known as an application restart model, where an application and its resources (a file system with data, an IP address, application scripts, a volume group, etc.) are logically combined and operate as a single entity on a VM or LPAR (known as a 'node') in a cluster. That group of resources are moved as a unit to a standby VM or LPAR, and the application is restarted. In the event of a planned outage - PowerHA can gracefully quiesce the application on the primary node. In any case, the outage lasts just as long as it takes to acquire the resources and restart the application this might be measured in minutes. It is also considered to be a warm standby method of high availability, in that the target node already has a running version of AIX.

PowerHA for AIX can also be used across data centers, where data is replicated through the storage subsystem or, using the AIX Geographic Logical Volume Manager (GLVM) subsystem, over IP to the target node.

In any case - within or across data centers - the failover process can be automated to reduce application outages. Applications can be moved to another running node to allow for maintenance, allowing for near-zero downtime - although the PowerHA application can be maintained with a live kernel update.

PowerHA for AIX is tightly integrated with the AIX operating system's inherent reliability, and IBM Power's hardware features such as virtual I/O, concurrent hardware maintenance, dynamic resizing of LPARs, etc. PowerHA supports critical enterprise apps, like Oracle, SAP, Db2, and Epic systems - all common workloads. Licensing is on a n+1 model per cluster, where n is the number of cores in the production online copy of the application, with 1 additional core added for the standby. For example, on a 24-core Power system, if two clusters are created, one supporting an LPAR with 4 cores and another with 5, a total of 11 cores will need to be licensed - 4+1=5 and 5+1=6.

## 7.1.2  IBM PowerHA SystemMirror for i

PowerHA for IBM i is IBM's high-availability and disaster recovery (HA/DR) solution designed specifically for the IBM i operating system. It provides automated failover capabilities and data replication to ensure business continuity in the event of planned or unplanned outages. PowerHA integrates tightly with IBM i's native features, such as the Independent Auxiliary

Storage Pool (IASP), to enable fast and efficient switching between systems with minimal downtime.

At the core of PowerHA is its ability to replicate data in real time between primary and backup systems using technologies like Geographic Mirroring or IBM's Storage-based replication. This ensures that critical applications and data remain available even if the primary system becomes unavailable. PowerHA supports both local high availability within a single data center and remote disaster recovery across geographically dispersed locations, offering flexibility for various business continuity strategies.

PowerHA also includes tools for monitoring, automation, and management, making it easier for IT administrators to configure and maintain HA/DR environments. With features like role-based access, cluster resource groups, and automated failover policies, PowerHA helps reduce the complexity of managing high-availability systems. It is a key component for organizations that rely on IBM i to run mission-critical workloads and require continuous access to their data and applications.

## 7.1.3  VM Recovery Manager (VMRM)

VM Recovery Manager (aka VMR or VMRM) operates on an active-inactive model, where the entire definition of an VM or LPAR Is recreated onto a target Power system and then restarted. It supports any workload running on a Power System - AIX, IBM I, and Linux (as noted elsewhere, the E1150 only supports AIX and Linux). Recovery will take longer than an application restart model because the entire LPAR must be restarted after being redefined, instead of just restarting the application.

In HA mode, VMRM uses the Live Partition Mobility to dynamically move an LPAR to a system that shared an HMC. In DR mode, it will manage the data replication process from the storage provider to a set of disks at the target system. The LPAR only exists in one place at any given time, except in some limited circumstances where a disaster exercise can be tested on the target system.

As with PowerHA, VMRM helps automate disaster recovery - it can detect when a VM or entire Power System is done and initiate policy-based failover to a single target or to a group of targets. It can coexist with PowerHA - where a cluster of LPARs can be failed as a group over to Power Systems at an alternate site. It is often used to provide simple DR support for Linux VMs - no software installation or OS configuration required; the Linux VM will be restarted.

The ideal use cases for VM Recovery Manager are development, test, and sandbox environments that may not require the same RTO requirements as production systems, and Linux on Power. It integrates well with solutions like SAP HANA and Oracle. No configuration work is required on the individual operating systems being managed, although there are some agents that can be installed to check for application failures and trigger a failover of the LPAR.

Similarly to PowerHA; it is licensed based on the number of production cores in use, not on an entire Power system. However, unlike PowerHA, no standby licenses are required. On a 24-core Power system, if the VMs needing VMRM support have 9 cores, then only n=9 cores need to be procured.

## 7.2  IBM Db2

In today's data-driven world, organizations across industries - including manufacturing, health care, and the public sector - must extract maximum value from rapidly growing volumes of information. Whether the goal is to uncover actionable insights from customer data or accelerate the processing of high-volume online transactions, businesses need solutions that are optimized for performance, scalability, and cost-efficiency. These solutions must support mission-critical workloads with high availability while keeping operational costs under control.

By combining IBM Db2 with IBM Power Systems built on the advanced POWER11 architecture, organizations can meet these demands with confidence. This integrated, workload-optimized stack delivers exceptional performance for both transactional and analytical workloads. Db2 leverages cutting-edge database innovations to maximize efficiency and throughput, earning top rankings in industry benchmarks such as TPC-C, TPC-H, and SAP SD 3-Tier. Power Systems provide the robust hardware foundation, offering superior price/performance and the ability to handle over a million transactions per minute at a cost of less than $1 per transaction. Db2 automatically exploits POWER11's parallelism and large page sizes, simplifying deployment and enabling cost-effective scaling for web applications, messaging backbones, and workload consolidation.

Beyond performance, the combination of Db2 and Power Systems ensures high availability and operational efficiency. Power Systems are engineered with built-in redundancy, error-handling, and reliability features, while IBM PowerHA SystemMirror adds automated monitoring and recovery to minimize downtime. Cost control is further enhanced through Db2 Deep Compression, which reduces storage needs, and IBM PowerVM virtualization, which allows for efficient resource utilization and workload consolidation. These capabilities reduce hardware, energy, and management costs, enabling a more agile and cost-effective IT infrastructure. Together, Db2 and IBM Power Systems offer a powerful, scalable, and resilient platform for modern data-driven enterprises.

For more information on Db2fi high availability see the IBM Redbook IBM PowerHA SystemMirror for AIX Cookbook , SG24-7739

## 7.3  Oracle

For over 35 years, clients have relied on IBM Power to deploy their Oracle database and application workloads. Organizations, both big and small, can take advantage of Power's class leading reliability and security as well as its advanced recovery, self-healing and diagnostic capabilities designed to reduce application downtime.

IBM Power11 servers, fully certified by Oracle software, can enable organizations the ability to consolidate multiple workloads on fewer servers - increasing overall system utilization and lowering overall costs. This efficiency can also lead to less Oracle licenses required. For example, IBM Power S1124 servers running Oracle Database SE2 can help reduce application cost per database instance by up to 33% compared to fourth- generation Intel Xeon scalable processors, and reduce the overall number of servers needed to improve energy costs.

The cost of security breaches continues to grow, averaging at USD 4 million per breach.2 IBM Power technology is built to protect businesses against cyberthreats with end-to-end security protocols, including new transparent memory encryption that doesn't impact performance. Power servers may be considered 60 times more secure than unbranded white box servers.2 IBM Power servers, combined with IBM AIX Trusted Execution (TE), you can verify the

integrity of your system and implement advance security policies to enhance the trust level of the complete system.

As data continues to rapidly expand, organizations may struggle when it comes to modernizing their IT infrastructure. IBM Power servers and the IBM AIX operating system (OS) create a solid foundation for the modernization of traditional Oracle database workloads, new application developments and workload consolidation. With the Red Hat Ansible Automation Platform, clients can manage Oracle workloads on IBM Power servers as part of their wider enterprise automation strategy.

Mission-critical workloads need a server and OS with reliability, high availability and the ability to scale without impacting mission-critical performance. IBM Power servers can offer 2.5 times better core than compared to x86 servers, enabling Oracle workloads to grow linearly without hitting bottlenecks.

Oracle certifies its products on Power systems, delivering a host of benefits - including comprehensive end-to-end support, portability and efficiency. IBM Power servers provide 99.999% of reliability to maintain maximum availability. The combined design of the AIX OS on Power servers with IBM PowerHA technology can bring clients stunning uptime, and the ability manage and monitor availability to prevent both planned and unplanned outages.

## Benefits of running Oracle on Power11

Here are some benefits of running your Oracle database workloads on IBM Power11:

- /SM590000 Industry-leading security
- IBM Power and AIX keep your critical Oracle workloads protected and available while reducing costs.2
- /SM590000 Simplified management
- Automatically deploy a 'cloud ready' OS capable of meeting any organization's private cloud requirements with IBM PowerVC.
- /SM590000 Unmatched uptime
- IBM Power supports the most demanding workloads and provides 99.999% of reliability to maintain maximum availability.1
- /SM590000 Improved workload cost-effectiveness
- Leverage Power LPAR and DLPAR eligibility for Oracle hard partitioning, clients can license only this Power cores available to Oracle software.
- /SM590000 Standards-based automation

Improve manageability and scalability ensuring consistent and repeatable outcomes by leveraging our enhanced and expanded automation portfolio for Oracle workloads, built on Ansible.

- /SM590000 Data Protection

Benefit from Top-to-Bottom security in Power with trusted boot, main memory encryption, run-time verification of OS files, and role-based access and execution control with PowerSC

For additional information on running Oracle on Power reference the IBM Redbook publication Oracle on IBM Power Systems , SG24-8485.

## 7.3.1  Running Oracle Standard Edition 2 on IBM Power

Oracle Standard Edition 2 (SE2) is a database option suitable for many business needs and can be used on IBM Power servers. SE2 offers a full-featured database with ease of use, power, and performance, including features like relational, JSON, and XML data handling, and Real Application Clusters for clustering services. SE2 is licensed on servers with up to two sockets, with licensing costs remaining the same regardless of the number of cores within each socket.

## Key aspects of using Oracle Standard Edition 2 on IBM Power servers

Here are some key benefits of running Oracle Standard Edition 2 on IBM Power:

- /SM590000 Licensing:

SE2 can be licensed with either the processor metric or the Named User Plus metric. Licensing rules include a maximum of two occupied sockets and a maximum of 16 CPU threads per socket, effectively limiting the number of cores.

- /SM590000 Power System Compatibility:

Oracle Database Standard Edition 2 is compatible with IBM Power Systems, and IBM has even designed specific Power10 servers (Power S1012 and Power S1014) to compete with x86 engines in the two-socket space, catering to Oracle's need for SE2 to be cost-competitive with SQL Server.

## Benefits of IBM Power for Oracle Workloads:

IBM Power offers several advantages for Oracle databases, including main memory encryption, runtime verification of OS files, role-based access control, and advanced recovery, self-healing, and diagnostic capabilities.

- /SM590000 High Availability:

Starting with Oracle Database 19c Release Update 19.7, Standard Edition High Availability is supported on IBM AIX on POWER Systems. This feature provides cluster-based database failover for Standard Edition Oracle Databases 19c.

- /SM590000 Other Features:

SE2 offers a range of features for building business applications, including support for relational, JSON, XML, spatial, graph, and unstructured data, as well as Oracle Multitenant Architecture.

## Licensing and implementation

Consider the following as you plan for your Oracle Standard Edition 2 implementation:

- /SM590000 While the number of cores in each socket does not affect the license price, it is important to adhere to the maximum number of CPUs allowed for the cluster, which is not per node.
- /SM590000 Standard Edition High Availability provides cluster-based failover for single-instance Standard Edition Oracle Databases using Oracle Clusterware.
- -Oracle Standard Edition High Availability benefits from the cluster capabilities and storage solutions that are already part of Oracle Grid Infrastructure, such as Oracle Clusterware, Oracle Automatic Storage Management (Oracle ASM) and Oracle ASM Cluster File System (Oracle ACFS).
- -Using integrated, shared, and concurrently mounted storage, such as Oracle ASM and Oracle ACFS for database files as well as for unstructured data, enables Oracle Grid Infrastructure to restart an Oracle Database on a failover node much faster than any cluster solution that relies on failing over and remounting volumes and file systems.

- -Starting with Oracle Database 19c Release Update (19.13), Standard Edition High Availability is supported on IBM AIX on POWER Systems (64-bit).

## 7.4  SAP HANA

IBM Power has a variety of options to meet clients where they are on their journey to SAP S/4HANA. Whether you are considering SAP HANA or the next-generation S/4HANA, IBM Power offers on-premises, off-premises or fully-managed (RISE with SAP) solutions, built to run mission-critical applications like SAP, help accelerate your ERP and application deployments, and help maximize the impact they can have on your data management, data integration, automation and business processes.

IBM and SAP have partnered for over 50 years:

- /SM590000 Over 4,800 clients are running SAP HANA on IBM Power servers
- /SM590000 Over 120 external client references for SAP HANA on IBM Power
- /SM590000 39 SAP Pinnacle Awards won by IBM
- /SM590000 30,000 organizations run essential workloads such as SAP on IBM Power

The key to IT efficiency and business continuity is a platform that integrates with your current infrastructure while simultaneously supporting digital transformation. IBM Power servers are purpose built for data-intensive applications such as SAP HANA and S/4HANA that require large amounts of in-memory computing but still let you maintain the high availability and flexibility required for your hybrid cloud.

## Benefits of running SAP HANA on IBM Power

With global data volumes set to grow to more than 180 zeta bytes in 2025, organizations across every sector are facing tremendous pressure to manage, process, store and extract valuable insights from their critical data. By running SAP HANA on IBM Power servers, businesses can:

- /SM590000 Provision Faster
- Simplify system management and boost business agility. 0.01 cores, 1GB memory Create new environments flexibly by allocating incrementally starting as low as 0.01 cores and 1GB memory.
- /SM590000 Maximize uptime

Minimize disruption to business-as-usual activities. #1 Best-in-class reliability for 15 years. 99.999% Five-nines server reliability score achieved during independent testing. 2x Two-times better memory RAS than Industry-Standard DIMMs

- /SM590000 Cut Energy Usage

Reduce datacenter costs and enhance environmental sustainability. 50% less energy IBM Power E1150 provides comparable performance and requires half the amount of energy used by compared x86-based server. 54% performance boost IBM Power E1180 uses 15% less energy and provides 54% more performance at maximum input power than the compared x86-based server

- /SM590000 Scale affordably

Reduce the risk of over-provisioning with flexible hybrid cloud solutions, instant scaling, and pay-per-use consumption options. 40TB Scale up capacity - the largest supported for SAP S/4HANA and SAP BW.

- /SM590000 Strengthen security

Protect critical data and applications from cyberthreats with end-to-end security, including new transparent memory encryption with no performance impact. 60x More secure than unbranded commodity servers.

- /SM590000 Gain Faster Insights

Make rapid decisions to maximize business efficiency. 2.5x Better per core performance than compared x86 servers.

## 7.5  Banking

As financial institutions accelerate digital transformation, IT architects are tasked with designing platforms that are secure, scalable, and AI-ready. IBM Power systems - particularly Power10 and Power11 - offer a robust foundation for deploying AI workloads in banking environments. With built-in AI acceleration, enterprise-grade RAS features, and seamless integration with hybrid cloud and container platforms, Power systems enable architects to modernize legacy infrastructure while embedding intelligence into core banking workflows.

## Hybrid Cloud Deployment Models for AI in Banking

Hybrid cloud is the preferred architecture for modern banking platforms, enabling agility, regulatory compliance, and cost optimization. IBM Power systems support hybrid cloud deployments through:

- /SM590000 IBM Power Virtual Server (PowerVS) for public cloud scalability
- /SM590000 Red Hat OpenShift for container orchestration across environments
- /SM590000 IBM Cloud Pakfi for Data and watsonx for AI lifecycle management

## Banking use cases utilizing AI on IBM Power

Here is a list of some of the use cases for AI for banks and other financial institutions:

- /SM590000 On-Premises AI Training with Cloud-Based Inference
- Banks often need to train AI models on-premises to comply with strict data residency and regulatory requirements. Using IBM Power10 or Power11 systems, financial institutions can securely train models on sensitive customer or transactional data. Once trained, these models are deployed to IBM Power Virtual Server (PowerVS) for inference, enabling scalable, real-time decision-making in the cloud. This hybrid approach is ideal for applications like real-time credit scoring or fraud detection, where data privacy is paramount but rapid response times are also critical.
- /SM590000 End-to-End AI on PowerVS with Secure On-Premises Data Access

In this model, banks run the full AI lifecycle - training, tuning, and inference - on IBM PowerVS, while maintaining secure access to on-premises data sources. This setup allows institutions to leverage the scalability and flexibility of the cloud without moving sensitive data offsite. For example, a bank might deploy a generative AI model on PowerVS to automate the summarization of complex financial reports, pulling data securely from internal systems. This ensures compliance with data governance policies while accelerating insights and reporting.

- /SM590000 Distributed AI Microservices Across Hybrid Cloud

Some banks adopt a microservices architecture, distributing AI components across both on-premises IBM Power systems and PowerVS in the cloud. This model supports modular, scalable AI applications that can operate across environments. For instance, asset valuation engines or personalized client engagement tools can run inference in the cloud while accessing real-time data from on-premises systems. This approach enables

agility and performance, especially for institutions managing diverse workloads across geographies or business units.

- /SM590000 AI for Risk and Compliance on IBM Power

Global Tier-1 banks are increasingly turning to AI to automate risk management and compliance processes. By deploying generative AI and machine learning models on IBM Power systems, these institutions can continuously monitor transactions, flag anomalies, and generate regulatory reports with minimal human intervention. This model supports high-throughput, low-latency processing, making it ideal for meeting stringent regulatory requirements such as anti-money laundering (AML), know-your-customer (KYC), and Basel III compliance.

- /SM590000 AI-Enhanced Core Banking Applications

IBM Power systems also support the integration of AI directly into core banking platforms. This enables real-time intelligence within mission-critical applications such as loan underwriting, treasury operations, and fraud analytics. For example, a bank might use AI models running on Power10 to detect unusual transaction patterns or predict liquidity needs, enhancing operational efficiency and decision-making. This model ensures that AI capabilities are embedded where they can deliver the most immediate business value.

## Integrating IBM watsonx with IBM Power Systems

IBM watsonx is a modular AI and data platform designed to accelerate the deployment of enterprise-grade AI. When integrated with IBM Power10 and Power11 systems, watsonx enables financial institutions to build, deploy, and govern AI models across hybrid cloud environments - including IBM PowerVS.

Here are the components of watsonx:

- /SM590000 watsonx.ai: Model training and inference
- /SM590000 watsonx.data: Data lakehouse integration
- /SM590000 watsonx.governance: Model risk and compliance
- /SM590000 watsonx.orchestrate: AI agents and workflow automation

IBM Power Systems are high-performance servers optimized for data-intensive workloads. The integration between watsonx and IBM Power enables:

- /SM590000 Accelerated AI Workloads: Power10 processors are optimized for AI inference and training, making them ideal for running watsonx models efficiently.
- /SM590000 Hybrid Cloud Flexibility: watsonx can run on IBM Power in hybrid cloud environments, allowing businesses to keep sensitive data on-premises while leveraging cloud-native AI capabilities.
- /SM590000 Enterprise-Grade Security and Reliability: Power Systems provide robust security and uptime, aligning with watsonx's focus on trustworthy AI.
- /SM590000 Tight Integration with Red Hat OpenShift: Both watsonx and IBM Power support OpenShift, enabling containerized AI workloads to run seamlessly across environments.

## 7.6  Health Care

IBM Power Systems provide a secure, high-performance, and resilient infrastructure that enables health care organizations to manage and protect digital health data, drive data-informed decisions, and meet regulatory requirements with confidence.

Note: Solution Edition for Healthcare

IBM intends to offer a cost-effective Power Solution Edition for Healthcare for next Power generation E1180, E1150, S1124 and S1122 which is intended to be aimed at reducing initial investments and enhancing the economics of IT infrastructure for the healthcare industry. The potential healthcare solution is intended to be available for US and Canada only.

Designed for mission-critical workloads, Power empowers health care providers to improve patient outcomes, enhance operational efficiency, and reduce risk.

## Security and Compliance

Health care organizations handle sensitive patient data that must be protected under regulations such as HIPAA. IBM Power delivers advanced security features including secure boot, role-based access control, and encryption at rest and in transit. With quantum-safe firmware signing and encrypted Live Partition Mobility (LPM), Power ensures data integrity and privacy. According to ITIC, organizations using Power experience an average of just 3.3 minutes of unplanned downtime per year due to security issues - demonstrating its industry-leading resilience.

## Unmatched Reliability for Clinical Systems

Power is engineered for continuous availability of mission-critical applications such as EHRs, PACS, and health care information systems. Features like predictive failure analysis, dynamic resource allocation, and system redundancy contribute to its 99.9999% availability rating, as reported by 1,900 C-level executives in ITIC's global reliability survey. Power has held the title of the most reliable non-mainframe server platform for over 15 years.

## High-Performance Computing for Health care Analytics

Health care workloads - from medical imaging to genomics and real-time analytics - demand exceptional compute power. Power10-based servers deliver up to 2.5x more performance per core than previous generations, enabling faster insights for research, drug discovery, and personalized medicine.

## Scalable and Flexible Infrastructure

Health care environments often face unpredictable demand. IBM Power supports dynamic scaling to accommodate workload fluctuations, whether driven by seasonal surges or new clinical initiatives. Its support for multiple operating systems and virtualization technologies allows health care IT teams to run diverse workloads efficiently on a single platform.

## AI-Ready for Clinical Innovation

IBM Power is optimized for AI and machine learning workloads, with on-chip Matrix Math Accelerators (MMAs) that enable real-time inferencing at the point of care. Power S1022 servers can process up to 42% more batch queries per second than comparable x86 systems under peak load, with sub-second inferencing latency - ideal for applications like diagnostic imaging, predictive analytics, and treatment optimization.

## Seamless Integration with Health care Ecosystems

IBM Power integrates with a wide range of health care applications, including EMRs, imaging platforms, analytics tools, and telemedicine systems. It supports interoperability and data exchange across systems, and works closely with leading ISVs such as Epic to certify hardware for industry-standard health care software.

## 7.6.1  Epic

Epic is a Health care Information System (HIS) provider that develops and delivers a comprehensive electronic medical record (EMR) system covering all aspects of the medical health care profession. The Epic solution includes a variety of applications that cover areas, such as medical billing, emergency room, radiology, outpatient, inpatient, and ambulatory care.

IBM and Epic Systems maintain a strategic partnership that enables health care organizations to run Epic's electronic health record (EHR) platform on IBM infrastructure, particularly IBM Power Systems and IBM Storage solutions. This collaboration is designed to support the performance, security, and compliance needs of health care providers managing mission-critical clinical workloads.

IBM Power Systems are certified to run Epic Operational Database (ODB) workloads, with best practices published for tuning AIX environments to meet Epic's stringent performance requirements. These configurations are optimized for high throughput, low latency, and high availability - critical for real-time clinical operations. Currently, IBM Power10 servers are certified by Epic Systems, supporting key infrastructure components on both AIX and Linux for large health care deployments, primarily in the database and backend services tier. These platforms are used to host InterSystems CachØ or IRIS for Health, which serve as the core databases behind Epic's EHR system.

It is expected that IBM and Epic will partner to certify Power11 configurations after the Power11 systems are available.

## Supported Products

Here is a list of currently supported Epic products:

- /SM590000 InterSystems CachØ / IRIS for Health
- -Officially supported by Epic on both AIX and Linux (RHEL) on Power10.
- -Deployed in production for the Epic operational database (Chronicles).
- /SM590000 Epic Production and Reporting Database Tiers
- -Most often run on AIX or RHEL on Power10 for performance and reliability.
- /SM590000 Other backend services, such as database copies used for Clarity extracts, may also run on AIX or Linux, depending on the customer's architecture.

Note: The Epic application layer (Hyperspace, Interconnect, etc.) typically runs on Windows or Linux on x86. Power-based AIX or Linux deployments are focused on the infrastructure and database tiers.

Required OS and Firmware Levels for Power10:

- /SM590000 AIX:
- -AIX 7.2 TL5 SP4 or later
- -AIX 7.3 TL1 or later
- Certified by both Epic and InterSystems for Power10 compatibility.
- /SM590000 Linux:
- -Red Hat Enterprise Linux (RHEL) 8.6 or later on Power10 (ppc64le)
- Certified by InterSystems for IRIS for Health on Power.
- Required for newer installations using containerized services or performance-optimized Linux environments.

- /SM590000 Virtualization:
- -Both AIX and Linux deployments use IBM PowerVM for LPAR management.
- -Linux LPARs can also be deployed using OpenShift on Power for modern container-based workloads.
- /SM590000 Storage:
- -High-performance SAN such as IBM FlashSystemfi is commonly used, with tuning for CachØ/IRIS.
- /SM590000 HA/DR:
- -PowerHA for AIX,
- -Linux HA tools (including IBM VM Recovery Manager),
- -InterSystems mirroring for resilience.

<!-- image -->

Chapter 8.

8

## Servicing Power11

The goal of serviceability is to enable efficient system repair while minimizing or avoiding any disruption to operations. It encompasses system installation, upgrades or downgrades (Miscellaneous Equipment Specification or MES), and ongoing maintenance or repair activities. Depending on the system configuration and warranty agreement, service tasks may be performed by the client, an IBM technician, or an authorized service provider.

IBM Power Systems are designed with advanced serviceability features to support a highly efficient maintenance environment. These features help streamline service operations and reduce downtime by incorporating the following key attributes:

- -Simplified installation and upgrade processes
- -Support for concurrent maintenance and guided repair
- -Automated diagnostics and error reporting
- -End-to-end service workflows, from issue detection to resolution.

The following topics are covered in this chapter:\

- /SM590000 IBM Maintenance
- /SM590000 IBM Expert Care
- /SM590000 IBM tools and interfaces
- /SM590000 eBMC card
- /SM590000 Managing the system by using the ASMI GUI
- /SM590000 Entitled System Support
- /SM590000 System firmware

## 8.1  IBM Maintenance

IBM offers a comprehensive global maintenance and support framework for its Power Systems, designed to ensure high availability, rapid issue resolution, and minimal disruption to business operations. Maintenance services are available under various service level agreements (SLAs), tailored to meet the needs of different environments - from standard business hours to mission-critical 24/7 operations. IBM's support infrastructure includes remote diagnostics, on-site service, and proactive monitoring, all backed by a global network of skilled service professionals and parts depots.

Customers can choose between 8x5 and 24x7 service coverage. The 8x5 option provides support during standard business hours (typically 8 a.m. to 5 p.m., Monday through Friday, excluding holidays), which is ideal for non-critical systems or environments with internal IT support. For systems that require continuous uptime, the 24x7 service option ensures around-the-clock support, including weekends and holidays. This level of coverage is essential for industries such as finance, healthcare, and manufacturing, where downtime can have significant operational or financial consequences.

IBM also defines response time targets based on the selected service level. For example, under a 24x7 agreement, IBM may commit to a 2-hour or 4-hour on-site response for critical hardware issues, depending on the location and contract terms. These response times are supported by IBM's extensive global logistics and service infrastructure, which includes strategically located parts centers and field engineers in over 170 countries.

This worldwide coverage ensures that IBM Power Systems customers receive consistent, high-quality support regardless of their geographic location. IBM's maintenance services are further enhanced by features such as call-home diagnostics, automated error reporting, and remote problem determination, which help accelerate issue resolution and reduce the need for manual intervention. Combined, these capabilities provide a robust and reliable maintenance ecosystem that supports both traditional on-premises deployments and modern hybrid cloud environments.

## 8.2  IBM Expert Care

Managing your IBM Power systems should be seamless and efficient. Power Expert Care delivers immediate access to a curated bundle of high-value services trusted by the Power community - eliminating the delays and complexity of traditional procurement processes. With this preselected service package, clients benefit from streamlined support, enhanced by AI-driven tools that accelerate response times, improve case resolution, and elevate overall satisfaction.

What sets Power Expert Care Premium apart is its focus on mission-critical environments where downtime is not an option. Backed by the same experts who helped design and build the Power11 platform, this service tier ensures your infrastructure is supported by unmatched technical depth and operational excellence.

Key Features of the Premium Bundle:

- /SM590000 30-minute response time for both hardware and software support cases
- /SM590000 4-hour on-site response target for urgent issues
- /SM590000 System and microcode compatibility guidance to ensure optimal performance
- /SM590000 Dedicated Technical Account Manager and mission-critical support resources

- /SM590000 Automated case creation and log analysis for faster issue resolution
- /SM590000 Health checks for hardware, OS, and applications (available as add-ons)
- /SM590000 Zero planned downtime support with proactive planning and TAM assistance

For non-mission-critical systems IBM offers the Advanced Expert Care bundle providing 24x7 standard response times on repair and maintenance cases.

## Technical Account Manager

The Technical Account Manager (TAM) plays a vital role in delivering proactive, product-based support for IBM Power systems. Acting as the primary point of contact, the TAM provides strategic guidance and direct engagement for both hardware and software within the scope of your support agreement. TAM services are delivered in English during the client's business hours, with support in other languages available upon request and mutual agreement, subject to availability.

TAMs help streamline operations and reduce downtime through a wide range of responsibilities, including:

- /SM590000 Enabling Call Home for proactive error reporting, Autonomous Error Resolution (AER), and Zero Planned Downtime (ZPD) - available on Power11
- /SM590000 Activating Support Insights for predictive analytics and delivering monthly reports
- /SM590000 Providing firmware and microcode compatibility analysis (exclusive to Power11 Premium clients)
- /SM590000 Sharing software lifecycle and roadmap updates
- /SM590000 Delivering HIPER alerts to help avoid high-impact issues
- /SM590000 Offering best practices documentation and priority handling of Severity 1 and 2 cases
- /SM590000 Leading complex case resolution and managing case progression
- /SM590000 Coordinating welcome calls, support planning, monthly reporting, and quarterly reviews
- /SM590000 Engaging IBM resources for Remote Code Load on Power10 and Power11 systems (on-site code load available separately for Power11)
- /SM590000 Supporting change management by communicating planned events to relevant teams

For clients running SAP HANA on Power10 or Power11, the TAM also:

- /SM590000 Advises on SAP HANA best practices
- /SM590000 Coordinates troubleshooting across the full software/hardware stack
- /SM590000 Provides technical recommendations for error identification, environment optimization, and known defect mitigation

## 8.3  IBM tools and interfaces

Servicing IBM Power servers relies on a robust combination of proactive maintenance, intelligent diagnostics, and modern remote management tools - all designed to ensure high availability and system reliability. At the heart of this ecosystem is the Hardware Management Console (HMC), which serves as the central hub for monitoring system health, managing logical partitions (LPARs), and coordinating service activities. Complementing the HMC is the service processor, a dedicated hardware component that operates independently of the main system. It continuously monitors hardware status, logs errors, and enables early detection of potential issues. Features like call-home support and automated error reporting further

streamline the support process by alerting IBM support teams to critical events - often before they impact operations.

The service processor can be accessed and managed through the HMC, but also directly via the Advanced System Management Interface (ASMI), which provides low-level control and additional troubleshooting capabilities. For modern, automated environments, the service processor also supports DMTF Redfish APIs, enabling secure, standardized remote management and integration with broader data center orchestration tools. Together, these technologies form a resilient and intelligent service infrastructure that supports both on-premises and hybrid cloud deployments, helping ensure that IBM Power systems remain secure, efficient, and continuously operational.

## 8.4  eBMC card

The IBM Power E1150 server utilizes the eBMC as a central component for system service management, monitoring, maintenance, and control. The eBMC is a specialized service processor that operates independently from the main system processors, continuously monitoring the physical state of the system through onboard sensors. It provides access to critical diagnostic data, including System Event Log (SEL) files, which are essential for identifying and resolving hardware issues. This independent operation ensures that system health can be monitored even when the main system is powered down or unresponsive.

Administrators and service personnel can interact with the eBMC through multiple interfaces. It integrates seamlessly with the Hardware Management Console (HMC) for centralized system management and supports direct access via the Advanced System Management Interface (ASMI) for low-level configuration and troubleshooting. Additionally, the eBMC supports DMTF Redfish APIs, enabling secure, modern, and automated management through RESTful interfaces. This makes it easier to integrate IBM Power servers into broader data center orchestration and monitoring frameworks. Designed with serviceability in mind, the eBMC card is field-replaceable, helping to minimize downtime and maintain high system availability.

## 8.5  Managing the system by using the ASMI GUI

The ASMI is the GUI to the eBMC. It is similar in terms of its function to the ASMI of FSP-managed servers (for example, the Power E1180 server), but it is a complete redesign of the UI driven through customer feedback from a Design Thinking workshop.

To enter the ASMI GUI, go to an HMC, select the server, and then select Operations → Launch Advanced System Management , as shown on Figure 8-1 on page 155. A window opens and shows the name of the system, MTM and serial number, and the IP address of the service processor (eBMC). Click OK, and the ASMI window opens.

Figure 8-1   Launch Advanced System Management (ASMI).

<!-- image -->

If the eBMC is not in a private but reachable network, you can connect directly by entering https://&lt;eBMC IP&gt; into your web browser. Figure 8-2 show the login window for the ASMI.

Figure 8-2   ASMI login window.

<!-- image -->

The default login credentials for the eBMC are username: admin and password: admin. Upon first login - or after performing a factory reset - this default password is immediately invalidated, and you are required to set a new, secure password. This policy is in place to prevent the system from being left in a vulnerable state with a well-known default password. The new password must meet strong security standards and should not be something easily guessable, such as abcd1234. For detailed information on password complexity requirements and best practices, refer to Setting the password.

After login, you see the Overview page with server, firmware, network, power, and status information.

Figure 8-3 shows the system overview page on the ASMI.

Figure 8-3   ASMI Overview window.

<!-- image -->

The new ASMI for eBMC managed servers have some major differences and some valuable new features:

- /SM590000 System firmware updates

It is possible to install a firmware update for the server by using the ASMI GUI, even if the system is managed by an HMC. In this case, the firmware update is always disruptive. To install a concurrent firmware update, you must use the HMC.

- /SM590000 Download of dumps

Dumps can be downloaded by using the HMC, but if necessary, you can also download them from the ASMI menu.

It is also possible to initiate a dump from the ASMI by selecting Logs -&gt; Dumps, selecting the dump type, and clicking Initiate dump. The possible dump types are:

- -Baseboard management controller (BMC) dump (nondisruptive)
- -Resource dump
- -System dump (disruptive)
- /SM590000 Network Time Protocol (NTP) server support
- /SM590000 Lightweight directory access protocol (LDAP) for user management
- /SM590000 Host console

By using the host console, you can watch the boot process of the server. The host console can also be used to access the OS if there is only a single LPAR that uses all resources.

Note: The host console can also be accessed by using an SSH client over port 2200 and logging in with the admin user.

- /SM590000 User management

In the eBMC, it is possible to create you own users. This feature can also be used to create an individual user that can be used for the HMC to access the server.

There are two types of privileges for a user: Administrator or ReadOnly. As the name indicates, with the ReadOnly privileges, you cannot modify something (except the password of that user), and a user with that privilege cannot be used for HMC access to the server.

- /SM590000 Access Control for eBMC-Managed Systems

For systems utilizing the enhanced Baseboard Management Controller (eBMC), such as the IBM Power E1150, access to the service processor is managed through a secure and auditable process. In scenarios where administrative credentials are unavailable - such as a lost or forgotten password - IBM Support can generate a time-limited, digitally signed Access Control File (ACF).

The ACF must be uploaded to the server via the eBMC interface to temporarily enable administrative access. This mechanism ensures that access is tightly controlled, traceable, and compliant with security best practices, without requiring permanent credential resets or exposure.

- /SM590000 Jumper reset

It is possible to reset everything on the server by using a jumper. This reset is a factory reset that resets everything, like LPARs, eBMC settings, or the NVRAM.

## Real-time progress indicator

The ASMI of an eBMC server also provides a real-time progress indicator to see the operator panel codes. To open the windows that show the codes, select Logs → Progress logs , and then click View code in real time .

## Inventory and LEDs

Under the Hardware status → Inventory and LEDs menu in the eBMC interface, you can view the status of most hardware components, including their associated identification LEDs. This section also provides access to the System Identify LED and the System Attention LED.

All identification LEDs can be toggled on or off to assist with physical component identification. However, only the System Attention LED can be turned off manually.

Additionally, you can view detailed information for each hardware component. This includes specifications such as the size of a DIMM or part numbers, which is particularly useful when planning hardware replacements or upgrades.

## Sensors

The ASMI has many sensors for the server, which you can access by selecting Hardware status → Sensors . The loading of the sensors takes some time, and during that time you see a progress bar on the top of the window.

Note: Although the progress bar might be finished, it might take some extra time until the sensors appear.

Figure 8-4 shows an example of the Sensors window.

Figure 8-4   ASMI Sensors window.

<!-- image -->

## Network settings

The default network setting for the two eBMC ports is DHCP. Therefore, when you connect a port to a private HMC network where the HMC is connected to a DHCP server, the new system should get its IP address from the HMC during the startup of the firmware. Then, the new system automatically appears in the HMC and can be configured. As a best practice, use DHCP to attach a server to the HMC.

If you do not use DHCP and want to use a static IP address, you can set the IP address in the ASMI GUI. However, because there are no default IP addresses that are the same for every server, you first must discover the configured IP address.

To discover the configured IP address, use the operator panel and complete the following steps:

1. Use the Increment or Decrement buttons to scroll to function 02.
2. Press Enter until the value changes from N to M, which activates access to function 30.
3. Scroll to function 30 and press Enter. Function 30** appears.
4. Scroll to 3000 and press Enter, which shows you the IP address of the eth0 port.
5. If you scroll to 3001 and press Enter, you see the IP address of eth1.
6. After you discover the IP address, scroll again to function 02 and set the value back from M to N.

For more information about function 30 in the operator panel, see IBM Documentation.

Now that you have discovered the IP address, you can connect any computer with a web browser to an IP address in the same subnet (class C) and connect the computer with the correct Ethernet port of the Power E1150 server.

Hint: Most connections work by using a standard Ethernet cable. If you do not see a link when using a standard Ethernet cable, try a crossover cable, where the send and receive wires are crossed.

After connecting the cable, you can use a web browser to access the ASMI by using the URL https://&lt;IP address&gt; , and then you can configure the network ports. To configure the network ports, select Settings → Network and select the correct adapter to configure. Figure 8-5 shows an example of changing eth1. Before you can configure a static IP address, turn off DHCP. It is possible to configure several static IP addresses on one physical Ethernet port.

Figure 8-5   ASMI network settings

<!-- image -->

You cannot configure the Virtualization Management Interface (VMI) address in the ASMI network settings. The VMI address is another IP address that is configured on the physical eBMC Ethernet port of the server to manage the virtualization of the server. The VMI address can be configured only in the HMC.

## Using an Access Control File

If you lose the access password for the ASMI service user, you can access the ASMI by using an ACF. An ACF is a digital certificate that is provided by IBM Support when you open a support case. To use the ACF, the system must be enabled at the server by using the operator panel.

## Complete the following steps:

1. On the operator panel, use the Increment or Decrement buttons to scroll to function 74.
2. Press Enter, and select 00 to accept the function (FF rejects it).
3. The ACF function now is active for 30 minutes. To use it, access the ASMI login window.
4. To upload the ACF into the system and access the ASMI, click Upload service login certificate .

For more information, see IBM Documentation.

## Policies

In the Security and access → Policies menu, you can turn on and off security-related functions, for example, whether you can manage your server by using Intelligent Platform Management Interface (IPMI).

Some customers require that the USB ports of the server should be disabled. You can accomplish this task by using policies. To do so, clear Host USB enablement , as shown in Figure 8-6.

Figure 8-6   How to turn off the USB ports of the server

<!-- image -->

## 8.5.1  Managing the system by using DMTF Redfish API

eBMC-based systems can be managed by using the DMTF Redfish application programming interfaces (APIs). Redfish is a Representational State Transfer (REST) API that is used for platform management and is standardized by the Distributed Management Task Force, Inc.

There are multiple ways of working with Redfish. One possibility is to use the OS command curl . The following examples show how to work with curl and Redfish.

Before you can get data from the server or run systems management tasks by using Redfish, you must authenticate against the server. After you authenticate by using a user ID and password, you receive a token from the serve that you use later.

Example 8-1 shows how to obtain a token.

```
# export eBMC=<IP> # export USER=admin # export PASSWORD=<PW> # export TOKEN=`curl -k -H "Content-Type: application/json" -X POST https://${eBMC}/login -d "{\"username\" : \"${USER}\", \"password\" : \"${PASSWORD}\"}" | grep token | awk '{print $2;}' | tr -d '"'`
```

Example 8-1   Obtaining a token from Redfish

With the token, you can get data from the server. First, request the data of the Redfish root by using /Redfish/v1 . You get data with other branches in the Redfish tree, for example, Chassis (uppercase C). To dig deeper, use the newly discovered odata.id /Redfish/v1/Chassis , as shown in Example 8-2.

```
#curl -s -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/redfish/v1 {
```

Example 8-2   Getting chassis data from Redfish

```
"@odata.id": "/redfish/v1", "@odata.type": "#ServiceRoot.v1_12_0.ServiceRoot", "AccountService": { "@odata.id": "/redfish/v1/AccountService" }, "Cables": { "@odata.id": "/redfish/v1/Cables" }, "CertificateService": { "@odata.id": "/redfish/v1/CertificateService" }, "Chassis": { "@odata.id": "/redfish/v1/Chassis" }, # curl -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/Redfish/v1/Chassis { "@odata.id": "/Redfish/v1/Chassis", "@odata.type": "#ChassisCollection.ChassisCollection", " Members ": [ { "@odata.id": "/Redfish/v1/Chassis/chassis" } ], "Members@odata.count": 1, "Name": "Chassis Collection" # curl -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/Redfish/v1/Chassis/chassis { "@odata.id": "/Redfish/v1/Chassis/chassis", "@odata.type": "#Chassis.v1_16_0.Chassis", "Actions": { ... "PCIeSlots": { "@odata.id": "/Redfish/v1/Chassis/chassis/PCIeSlots" }, ... "Sensors": { "@odata.id": "/Redfish/v1/Chassis/chassis/Sensors" }, ...
```

Under Chassis (uppercase C), there is another chassis with a lowercase c. Use the tree with both ( /Redfish/c1/Chassis/chassis ) . After running the chassis, you can see in Example 8-2 on page 160 for example, that there are PCIeSlots and Sensors , among other resources of the server.

In Example 8-3, you can see what is under Sensors . There you can find the same sensors as in the ASMI GUI (see Figure 8-4 on page 158). In the output, you find, for example, the sensor total\_power . When you ask for details about that sensor, as shown in Example 8-3, you can see that the server needed 1.426 watts at the time of running the command.

```
# curl -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/Redfish/v1/Chassis/chassis/Sensors { "@odata.id": "/Redfish/v1/Chassis/chassis/Sensors", "@odata.type": "#SensorCollection.SensorCollection", "Description": "Collection of Sensors for this Chassis",
```

Example 8-3   Getting sensor data from Redfish

```
"Members": [ { "@odata.id": "/Redfish/v1/Chassis/chassis/Sensors/Altitude" }, { "@odata.id": "/Redfish/v1/Chassis/chassis/Sensors/1V1CS_0167_p1_rail_iout" }, ... { "@odata.id": "/Redfish/v1/Chassis/chassis/Sensors/total_power" }, # curl -k -H "X-Auth-Token: $TOKEN" -X GET \ https://${eBMC}/Redfish/v1/Chassis/chassis/Sensors/total_power { "@odata.id": "/Redfish/v1/Chassis/chassis/Sensors/total_power", "@odata.type": "#Sensor.v1_0_0.Sensor", "Id": "total_power", "Name": "total power", "Reading": 1426.0, "ReadingRangeMax": null, "ReadingRangeMin": null, "ReadingType": "Power", "ReadingUnits": "W", "Status": { "Health": "OK", "State": "Enabled" }
```

Some other useful strings to ask for are shown in Example 8-4.

```
#Type and model of the server # curl -s -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/Redfish/v1/Systems/system | grep Model | grep -v SubModel | grep \ -v \"\" "Model": "9043-MRU", #Serial number # curl -s -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/redfish/v1/Systems/system | grep SerialNumber \ "SerialNumber": "<SN>", #Type, model, and serial number #curl -s -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/redfish/v1/Systems/system | grep AssetTag "AssetTag": "Server-9043-MRU-<SN>", #System indicator LED #curl -s -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/redfish/v1/Systems/system | grep IndicatorLED "IndicatorLED": "Off", #Total memory # curl -s -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/redfish/v1/Systems/system | grep TotalSystemMemoryGiB "TotalSystemMemoryGiB": 8192 #Power state # curl -s -k -H "X-Auth-Token: $TOKEN" -X GET https://${eBMC}/redfish/v1/Systems/system | grep PowerState "PowerState": "On",
```

Example 8-4   Other useful Redfish data

It is also possible to run operations on the server by using the POST method with the Redfish API interface. In Example 8-5, you can see the curl commands that can start or stop the server.

```
#Power on server # curl -k -H "X-Auth-Token: $TOKEN" -X POST https://${eBMC}/redfish/v1/Systems/ system/Actions/Reset -d '{"ResetType":"On"}' #Power off server # curl -k -H "X-Auth-Token: $TOKEN" -X POST https://${eBMC}/redfish/v1/Systems/ system/Actions/Reset -d '{"ResetType":"ForceOff"}'
```

Example 8-5   POST command examples for Redfish

For more information about Redfish, see Managing the system by using DMTF Redfish APIs. For more information about how to work with Redfish in IBM Power servers, see Managing Power Systems servers by using DMTF Redfish APIs.

## 8.5.2  Managing the system by using the IPMI

The Power E1150 server can also be managed by using the IPMI, but the IPMI is disabled by default on your server. Inherent security vulnerabilities are associated with the IPMI, so consider using Redfish APIs or the GUI to manage your system.

If you want to use IPMI, you must first enable it. To do so, select Security and access → Policies . Then, enable the policy Network IPMI (out-of-band IPMI) .

A list of common IPMI commands can be found at Common IPMI commands.

## 8.6  Entitled System Support

IBM Enterprise Storage Server is available to view and manage Power and Storage software and hardware. In general, most products that are offered by IBM Systems that are purchased through our IBM Digital Sales representatives or Business Partners are accessed on this site when the IBM Configurator is used.

The site features the following three main sections:

- /SM590000 My entitled software

Activities are listed that are related to Power and Storage software, including the ability to download licensed, free, and trial software media, place software update orders, and manage software keys.

- /SM590000 My entitled hardware

Activities are listed related to Power and Storage hardware including the ability to renew Update Access Keys, buy and use Elastic Capacity on Demand, assign or buy credits for new and existing pools in a Power Private Cloud environment (Enterprise Pools 2.0), download Storage Capacity on-Demand codes, and manage Hybrid Capacity credits.

- /SM590000 My inventory

Activities related to Power and Storage inventory including the ability to browse software license, software maintenance, and hardware inventory, manage inventory retrievals by way of Base Composer or generate several types of reports.

## 8.7  System firmware

System firmware provides low-level control for the system hardware. New features and fixes are introduced with new firmware release levels. Fixes are often bundled into service packs. A service pack is referred to as an update level. A new release is referred to as an upgrade level. All system firmware is available for download in IBM FixCentral.

## Terminology

Release Level:

A major new function (introduction of new hardware models and significant function and features enabled via firmware). This firmware upgrade is disruptive.

Service Pack (SP):

Primarily firmware fixes and minor function changes applicable to a specific Release Level. These firmware updates are usually concurrent.

Concurrent:

A code update that allows the operating system(s) running on the Power system to continue running while the update is installed and activated.

Deferred:

A code fix that is installed concurrently, but does not activate until the system is restarted.

Partition Deferred:

A code fix that is installed concurrently but does not activate until the partition is restarted.

Disruptive:

A code fix which requires a system restart during the code update process.

## Service pack severity

The severity classification is specific for each service pack that becomes available in FixCentral. All types are listed below:

NEW:

New features and functions. This is considered a new release level for a product.

PE (PTF in error):

This service pack addresses minor issues. It can be installed when convenient.

ATT (attention):

This service pack addresses low impact and low potential issues. It should be installed at the customer's earliest convenience.

SPE (special attention):

This Service Pack addresses high impact but low potential issues. It should be installed at earliest convenience.

HIPER (high impact / pervasive):  This service pack addresses high impact and/or pervasive issues with significant customer impacts, and therefore should be installed as soon as possible.

For more details refer to: https://www.ibm.com/support/pages/glossary

## System Firmware Update Strategy for IBM Power11 Systems

The system management model determines the appropriate firmware update strategy for IBM Power11 systems.

- /SM590000 For managed systems, the recommended method for updating system firmware is via the Hardware Management Console (HMC).
- /SM590000 For unmanaged systems, the preferred method is to update the system firmware directly through the operating system.

While these are the recommended approaches, it is also possible to update the system firmware using the embedded BMC (eBMC).

The update method impacts the type of update:

- /SM590000 Firmware updates from the HMC can be either concurrent or disruptive, depending on the firmware version.
- /SM590000 Firmware updates from the operating system or eBMC are always disruptive.

A concurrent firmware update does not require a system restart. This is indicated in the firmware description file under the Service Pack Summary, which categorizes the update as either Disruptive Service Pack, Deferred Service Pack, or Concurrent Service Pack.

However, any fixes marked as DEFERRED in the Service Pack Summary will not take effect until the next system IPL (Initial Program Load). Fixes labeled as DEFERRED: PARTITION\_DEFERRED will require a partition reboot to take effect.

Depending on the system management model, the following firmware update options are available:

- /SM590000 HMC-managed systems: Firmware updates can be performed using the HMC or the eBMC ASMI interface.
- /SM590000 Co-managing a system with both PowerVM NovaLink and the Hardware Management Console (HMC): Firmware updates can be performed using the HMC, the NovaLink partition, or the eBMC/ASMI interface. However, to perform the update through the HMC, the system must first be transitioned from PowerVM NovaLink management to HMC management.
- /SM590000 Unmanaged systems running IBM i: Updates can be applied using PTFs or via the eBMC ASMI interface.
- /SM590000 Unmanaged systems running AIX: Firmware updates are supported through AIX system diagnostics or the eBMC ASMI interface.
- /SM590000 All IBM Power11 systems also support system firmware updates via the eBMC USB port.

Note: The responsibility for performing system firmware updates resides with the customer. If the customer requests that a IBM Support Service Representative (SSR) carry out the firmware update, the request will be considered a billable service - unless the customer has a valid support agreement that explicitly includes on-site firmware update coverage.

The firmware description file available on IBM Fix Central provides essential information about dependencies between HMC versions, AIX APARs, IBM i PTFs, and system firmware levels. Before installing a new system firmware release or service pack, it is strongly recommended to review the firmware description package.

For cross-version compatibility details, refer to the IBM Power Systems Fix Level Recommendation Tool (POWER code matrix).

## Updating System Firmware from the HMC

To update or upgrade the system firmware using the Hardware Management Console (HMC), follow the steps below:

1. In the System View, select the system you want to update.
2. Click on the Firmware menu.
3. Choose Update System Firmware.

The Update System Firmware Wizard will guide you through the necessary steps to complete the firmware update process, as shown in Figure 8-7.

Figure 8-7   Launch the Update System Firmware wizard.

<!-- image -->

The Firmware Update Wizard guides you through the process of updating your system. It begins with accepting the license agreement, followed by performing a system health check using the Readiness Check. Next, you will provide the required firmware files and select the firmware level to be applied. The final step involves monitoring the update progress and confirming its successful completion. Figure 8-8 through 3-6 illustrate these key steps in the update process.

Figure 8-8   The license agreement is displayed for the user to review and accept the terms.

<!-- image -->

Figure 8-9 on page 167 shows that the System Readiness Check has completed successfully for the IBM Power11 system firmware update process.

Figure 8-9   shows that the System Readiness Check completed successfully.

<!-- image -->

Figure 8-10 illustrates the 'Choose source files location' drop-down menu, which is used to update the system firmware. Several options are available for loading the required update files, including:

- /SM590000 IBM Website
- /SM590000 FTP Server
- /SM590000 SFTP Server
- /SM590000 Mount Point on the HMC
- /SM590000 CD/DVD
- /SM590000 USB Drive

Figure 8-10   the 'Choose Source File Location' drop-down menu used for updating the system firmware.

<!-- image -->

As shown in Figure 8-11 on page 168, click the 'Search Available Levels' button to locate the new system firmware on the installation media. On the following screen, use the drop-down menu under the 'Target eBMC Level' column to select the desired system firmware level.

Figure 8-11   System Firmware Target Level Selection Screen. This screen allows you to choose between a firmware update or upgrade.

<!-- image -->

## Import Update Files from HMC

The Import Update Files wizard in the HMC GUI, as shown in Figure 8-12, guides you through the necessary steps to import firmware update or upgrade files. These files can then be reused to update the firmware on other IBM Power11 systems.

Figure 8-12   The 'Import Update Files' option in the HMC GUI.

<!-- image -->

## Delete Files from Import Location

The Delete Files from Import Location wizard in the HMC GUI, as shown in Figure 8-13 on page 169, guides you through the steps to delete imported firmware files. This allows you to manage and clean up firmware files stored on the HMC.

Figure 8-13   The 'Delete Files from Import Location' option in the HMC GUI.

<!-- image -->

## SR-IOV Shared Mode Adapter Firmware Management

SR-IOV capable adapters running in Shared mode use a different firmware versioning mechanism compared to those operating in Dedicated mode.

## SR-IOV Configuration Requirements

- /SM590000 Ensure that the managed system includes PCIe adapter(s) that support SR-IOV functionality.
- /SM590000 Each SR-IOV-capable adapter must be installed in a PCIe slot that supports SR-IOV. Refer to the adapter placement guidelines in the IBM Documentation.
- /SM590000 Verify that the system is running a supported level of:
- -System firmware
- -HMC or PowerVM NovaLink
- -Operating system (with SR-IOV driver support)
- /SM590000 Configure the adapter(s) in SR-IOV Shared Mode using either the HMC or NovaLink interface.

## In SR-IOV (Shared mode):

- /SM590000 The adapter firmware and driver are integrated with the system firmware.
- /SM590000 Firmware updates and accompanying ReadMe documentation are available on IBM Fix Central.
- /SM590000 When an adapter is configured in SR-IOV Shared mode, its firmware and driver are automatically updated to the latest versions included with the system firmware. This update occurs during standard maintenance operations such as a system IPL, adapter replacement, or when switching the adapter mode between Shared and Dedicated.
- /SM590000 Additionally, if system firmware updates are installed concurrently, selective manual updates of SR-IOV adapter firmware can be performed using the Hardware Management Console (HMC).

## For more information see:

https://www.ibm.com/docs/en/power10/7063-CR2?topic=firmware-update-sr-iov or https://www.ibm.com/docs/en/power11/9043-MRU?topic=adapters-updating-sr-iov-adapte r-firmware

Note: You may update either the adapter firmware alone or both the adapter driver and firmware. During the firmware update process, network traffic on the configured logical ports of the SR-IOV adapter may be temporarily disrupted. Updating each SR-IOV adapter typically takes between 2 to 5 minutes. The update is performed sequentially across all SR-IOV adapters in the system.

Figure 8-14 displays the 'View SR-IOV Firmware Levels' and 'Update SR-IOV Firmware' options available in the HMC GUI.

Figure 8-14   'View SR-IOV Firmware Levels' and 'Update SR-IOV Firmware' options available in the HMC GUI.

<!-- image -->

## Updating System Firmware via eBMC

The eBMC ASMI (Advanced System Management Interface) can be used to update the system firmware on both managed and unmanaged systems. To perform a firmware update via eBMC ASMI, the system must be powered off. The firmware package includes multiple files, but only the firmware image with the .tar file extension is required for the update process. For example, refer to Figure 8-15.

Figure 8-15   Firmware Update via eBMC ASMI.

<!-- image -->

## I/O Microcode Update Strategy

Microcode updates are the responsibility of the customer, unless the services are provided under a Microcode Support Services contract.

Device microcode can be updated using the following methods:

## /SM590000 HMC (Hardware Management Console):

The 'Update I/O Firmware' feature available in the HMC GUI allows administrators to apply microcode updates to supported I/O devices.

## /SM590000 Diagnostic Menus (AIX and VIOS):

Microcode updates can be performed using either the system diagnostics or stand-alone diagnostics utilities, from a USB key or a network location. This method is applicable to AIX and VIOS environments.

- /SM590000 Linux: Refer to the microcode README files available on IBM FixCentral for detailed instructions

on updating microcode on Linux systems. These instructions may include the use of additional vendor-provided tools, depending on the I/O devices.

## /SM590000 IBM i (PTFs):

This method is applicable only to IBM i systems. Microcode updates are delivered through Program Temporary Fixes (PTFs), which can be downloaded from IBM FixCentral.

Note: IBM i is not supported on E1150 servers. The information provided in this section is included for reference purposes only, to highlight IBM i options available on other IBM Power11 systems that do support IBM i.

## Viewing or Updating I/O Firmware from the HMC

The Hardware Management Console (HMC) can be used to view the current I/O firmware levels for a system and to update those levels using a firmware repository. The HMC does not have direct access to log in to the partitions to perform I/O firmware updates. Instead, it relies on the Resource Monitoring and Control (RMC) infrastructure also used for features like Dynamic Logical Partitioning (DLPAR) and Service Focal Point to facilitate communication between the HMC and the partitions. On each partition, the invscout command handles the query and update process. Invscout exchanges inventory and update-related files with the HMC over the RMC interface.

Figure 8-16 on page 172 displays the 'View I/O Firmware Levels' and 'Update I/O Firmware' options available in the HMC GUI.

Figure 8-16   'View I/O Firmware Levels' and 'Update I/O Firmware' options available in the HMC GUI.

<!-- image -->

Figure 8-17 displays the 'View I/O Firmware Levels' panel, which shows I/O firmware information for both FSP and eBMC-based systems, as available in the HMC GUI.

Figure 8-17   'View I/O Firmware Levels' panel available in the HMC GUI.

<!-- image -->

Figure 8-18 on page 173 illustrates the 'Choose Source Files Location' drop-down menu, which is used to specify the source location for updating the microcode on I/O devices. Several options are available for loading the required update files, including:

- /SM590000 IBM Website
- /SM590000 FTP Server
- /SM590000 SFTP Server
- /SM590000 Mount Point on the HMC
- /SM590000 CD/DVD
- /SM590000 USB Drive

Figure 8-18   Update I/O firmware screen

<!-- image -->

For additional information on how to view or update the I/O firmware from the HMC, please refer to the following technical document: 'View or Update the I/O Firmware from the HMC'.

## 8.7.1  Update Access Keys

Since the introduction of the IBM Power8 processor-based servers, IBM uses the concept of an Update Access Key (UAK) for each server.

When system firmware updates are applied to the system, the UAK and its expiration date are checked. System firmware updates include a release date. If the release date for the firmware updates is past the expiration date for the update access key when attempting to apply system firmware updates, the updates are not processed.

## Managing Update Access Keys

The system uses an Update Access Key (UAK) to control the application of system firmware updates. Each UAK includes an expiration date, while firmware updates include a release date. When a firmware update is applied, the system checks whether the update's release date is later than the UAK's expiration date. If it is, the firmware update will not be processed. When a UAK expires, it must be replaced using either the Hardware Management Console (HMC) or the eBMC ASMI interface. Importantly, the expiration of a UAK does not affect I/O microcode updates these can still be applied even if the UAK has expired. Customers can obtain a new UAK by opening a case with IBM Support and requesting a renewal key, or by downloading it directly from the IBM Entitled Systems Support (ESS) website.

By default, newly delivered systems include an UAK that expires after three years. Thereafter, the UAK can be extended every six months, but only if a current hardware maintenance contract exists for that server. The contract can be verified on the Enterprise Storage Server web page.

The validity and expiration date of the current UAK can be checked using either the HMC or eBMC graphical interfaces, as well as through their respective command-line interfaces. Additionally, the expiration date can also be retrieved from the operating system level.

## Verifying the Expiration Date of UAKs Using HMC

The current Update Access Key (UAK) expiration date is visible on the View current system firmware levels of the HMC, as shown in Figure 8-19.

Figure 8-19   UAK expiration date displayed on the HMC View current system firmware levels page.

<!-- image -->

The Hardware Management Console (HMC) or vHMC (Virtual HMC Software Appliance) can be used to configure the Automatic Firmware Update Access Key. For more information, refer to the documentation titled 'How to Automatically Update the Access Key'.

## Verifying the Expiration Date of UAKs Using eBMC ASMI

The current Update Access Key (UAK) expiration date is visible on the Firmware and Overview pages of the eBMC ASMI, as shown in Figure 8-20 and Figure 8-21 on page 175.

Figure 8-20   UAK expiration date displayed on the eBMC Firmware page.

<!-- image -->

Figure 8-21   UAK expiration date displayed on the eBMC Overview page.

<!-- image -->

## Verifying the Expiration Date of UAKs Using AIX

There are multiple methods of checking the UAK expiration date within AIX:

- /SM590000 The first option utilizes the lscfg command. Use the following command:

lscfg -vpl sysplanar0 |grep -p "System Firmware"

The output is similar to the output that is shown in Example 8-6.

```
lscfg -vpl sysplanar0 |grep -p "System Firmware" System Firmware: Code Level, LID Keyword.....Phyp_1 21372025061280A00701 Code Level, LID Keyword.....PFW 21212025060681CF0681 Code Level, LID Keyword.....FSP_Fil 16112025061681E00109 Code Level, LID Keyword.....FipS_BU 16112025061681E00208 Microcode Image.............RK1110_058 RK1110_057 RK1110_058 Microcode Level.............FW1110.00 FW1110.00 FW1110.00 Microcode Build Date........20250725 20250725 20250725 Update Access Key Exp Date..20301014 Hardware Location Code......U9080.HEU.DEN0013-Y1 Physical Location: U9080.HEU.DEN0043-Y1
```

Example 8-6   Output of the lscfg command to check UAK expiration date.

- /SM590000 Alternatively, you can grep on Access Key as shown in Example 8-7.
- /SM590000 A third option on AIX 7.3 is the lparstat command which is shown in Example 8-8.

```
# lscfg -vpl sysplanar0 | grep "Access Key" Update Access Key Exp Date..20301014
```

Example 8-7   Output of the lscfg command to check UAK expiration date

```
# lparstat -u FW Update Access Key Expiration  (YYYYMMDD):  20301014 AIX Update Access Key Expiration (YYYYMMDD):  20301014 AIX Image Date                   (YYYYMMDD):  20250725
```

Example 8-8   Output of the lparstat command to check UAK expiration date

## Verifying the Expiration Date of UAKs Using Linux

There isn't a single Linux command to view a UAK directly. Instead, UAKs are managed through IBM's tools and interfaces, like ESA, ASMI and HMC.

<!-- image -->

Chapter 9.

9

## Virtualization

Virtualization on IBM Power Systems is a cornerstone of its enterprise computing strategy, offering robust, scalable, and secure environments for running multiple workloads on a single physical server. At the heart of this capability is PowerVM, IBM's enterprise-grade virtualization technology. PowerVM enables the creation of logical partitions (LPARs), allowing multiple operating systems such as AIX, IBM i, and Linux to run concurrently on the same hardware. It supports advanced features like live partition mobility, dynamic resource allocation, and micro-partitioning, which help maximize hardware utilization and reduce operational costs.

In addition to PowerVM, IBM Power Systems also support KVM (Kernel-based Virtual Machine), an open-source virtualization option. KVM on Power provides a flexible and cost-effective alternative for Linux-based workloads, particularly in cloud-native and containerized environments. It integrates well with modern orchestration tools and supports features like nested virtualization and SR-IOV for high-performance networking. KVM is ideal for organizations looking to leverage open-source technologies while still benefiting from the performance and reliability of IBM Power hardware.

A key component of virtualization efficiency on Power Systems is the use of Shared Processor Pools (SPPs). SPPs allow multiple LPARs to share a pool of physical processors, enabling dynamic allocation of CPU resources based on workload demand. With Power11 PowerVM add support for Resource Groups to further enhance the processor sharing capabilities and add additional efficiencies. Processor Pools add to the capabilities of Shared Processor Pools to improve processor utilization and enhance the isolation and control by capping the maximum CPU resources available to each pool. When combined with PowerVM or KVM, SPPs help optimize performance, reduce licensing costs, and ensure consistent service levels across virtual environments.

This chapter contains the following topics:

- /SM590000 PowerVM
- /SM590000 KVM support

## 9.1  PowerVM

The PowerVM platform is the family of technologies, capabilities, and offerings that delivers industry-leading virtualization for enterprises. It is the umbrella branding term for Power processor-based server virtualization, that is, IBM PowerVM Hypervisor, logical partitioning, IBM Micro-Partitioning, Virtual I/O Server (VIOS), Live Partition Mobility (LPM), and more. PowerVM is a combination of hardware enablement and software.

Note : PowerVM Enterprise Edition License Entitlement is included with each Power E1150 server. PowerVM Enterprise Edition is available as a hardware feature (# EPVW ), and supports up to 20 partitions per core, VIOS, Resource Groups which can be combined with shared processor pools (SPPs) and also offers LPM.

## 9.1.1  IBM PowerVM Hypervisor

Power processor-based servers are combined with PowerVM technology and offer the following key capabilities that can help to consolidate and simplify IT environments:

- /SM590000 Improve server usage and share I/O resources to reduce the total cost of ownership (TCO) and better use IT assets.
- /SM590000 Improve business responsiveness and operational speed by dynamically reallocating resources to applications as needed to better match changing business needs or handle unexpected changes in demand.
- /SM590000 Simplify IT infrastructure management by making workloads independent of hardware resources so that business-driven policies can be used to deliver resources that are based on time, cost, and service-level requirements.

Combined with features in the Power E1180, the IBM PowerVM Hypervisor delivers functions that enable other system technologies, including logical partition (LPAR) technology, virtualized processors, IEEE virtual local area network (VLAN)-compatible virtual switch, virtual SCSI adapters, virtual Fibre Channel adapters, and virtual consoles.

The PowerVM Hypervisor is a basic component of the system's firmware and offers the following functions:

- /SM590000 Provides an abstraction between the physical hardware resources and the LPARs that use them.
- /SM590000 Enforces partition integrity by providing a security layer between LPARs.
- /SM590000 Controls the dispatch of virtual processors to physical processors.
- /SM590000 Saves and restores all processor state information during a logical processor context switch.
- /SM590000 Controls hardware I/O interrupt management facilities for LPARs.
- /SM590000 Provides VLAN channels between LPARs that help reduce the need for physical Ethernet adapters for inter-partition communication.
- /SM590000 Monitors the Flexible Service Processor (FSP) and performs a reset or reload if it detects the loss of one of the FSP, notifying the operating system if the problem is not corrected.

The PowerVM Hypervisor is always active, regardless of the system configuration or whether it is connected to the managed console. It requires memory to support the resource assignment of the LPARs on the server. The amount of memory that is required by the PowerVM Hypervisor firmware varies according to several factors:

- /SM590000 Memory usage for hardware page tables (HPTs)
- /SM590000 Memory usage to support I/O devices
- /SM590000 Memory usage for virtualization

## Memory usage for hardware page tables

Each partition on the system includes its own HPT that contributes to hypervisor memory usage. The HPT is used by the operating system to translate from effective addresses (EAs) to physical real addresses in the hardware. This translation from effective to real addresses allows multiple operating systems to run simultaneously in their own logical address space. Whenever a virtual processor for a partition is dispatched on a physical processor, the hypervisor indicates to the hardware the location of the partition HPT that can be used when translating addresses.

The amount of memory for the HPT is based on the maximum memory size of the partition and the HPT ratio. The default HPT ratio is 1/128th (for AIX, VIOS, and Linux partitions) of the maximum memory size of the partition. AIX, VIOS, and Linux use larger page sizes (16 KB and 64 KB) instead of using 4 KB pages. The use of larger page sizes reduces the overall number of pages that must be tracked; therefore, the overall size of the HPT can be reduced. For example, the HPT is 2 GB for an AIX partition with a maximum memory size of 256 GB.

When defining a partition, the maximum memory size that is specified is based on the amount of memory that can be dynamically added to the dynamic logical partition (DLPAR) without changing the configuration and restarting the partition.

In addition to setting the maximum memory size, the HPT ratio can be configured. The hpt\_ratio parameter for the chsyscfg Hardware Management Console (HMC) command can be issued to define the HPT ratio that is used for a partition profile. The valid values are 1:32, 1:64, 1:128, 1:256, or 1:512.

Specifying a smaller absolute ratio (1/512 is the smallest value) decreases the overall memory that is assigned to the HPT. Testing is required when changing the HPT ratio because a smaller HPT might incur more CPU consumption because the operating system might need to reload the entries in the HPT more frequently. Most customers choose to use the IBM provided default values for the HPT ratios.

## Memory usage for I/O devices

In support of I/O operations, the hypervisor maintains structures that are called the translation control entities (TCEs), which provide an information path between I/O devices and partitions. The TCEs provide the address of the I/O buffer, indications of read versus write requests, and other I/O-related attributes. Many TCEs are used per I/O device, so multiple requests can be active simultaneously to the same physical device. To provide better affinity, the TCEs are spread across multiple processor chips or drawers to improve performance while accessing the TCEs.

For physical I/O devices, the base amount of space for the TCEs is defined by the hypervisor that is based on the number of I/O devices that are supported. A system that supports high-speed adapters can also be configured to allocate more memory to improve I/O performance. Linux is the only operating system that uses these extra TCEs so that the memory can be freed for use by partitions if the system uses only AIX.

## Memory usage for virtualization features

Virtualization requires more memory to be allocated by the PowerVM Hypervisor for hardware statesave areas and various virtualization technologies. For example, on Power11 processor-based systems, each processor core supports up to eight simultaneous multithreading (SMT) threads of execution, and each thread contains over 80 different registers.

The PowerVM Hypervisor must set aside save areas for the register contents for the maximum number of virtual processors that are configured. The greater the number of physical hardware devices, the greater the number of virtual devices, the greater the amount of virtualization, and the more hypervisor memory is required. For efficient memory consumption, wanted and maximum values for various attributes (processors, memory, and virtual adapters) must be based on business needs, and not set to values that are higher than actual requirements.

## Predicting memory that is used by the PowerVM Hypervisor

The IBM System Planning Tool (SPT) is a resource that can be used to estimate the amount of hypervisor memory that is required for a specific server configuration. After the SPT executable file is downloaded and installed, you can define a configuration by selecting the correct hardware platform and the installed processors and memory, and defining partitions and partition attributes. SPT can estimate the amount of memory that is assigned to the hypervisor, which assists you when you change a configuration or deploy new servers.

The PowerVM Hypervisor provides the following types of virtual I/O adapters:

- /SM590000 Virtual SCSI

The PowerVM Hypervisor provides a virtual SCSI mechanism for the virtualization of storage devices. The storage virtualization is accomplished by using two paired adapters: a virtual SCSI server adapter and a virtual SCSI customer adapter.

- /SM590000 Virtual Ethernet

The PowerVM Hypervisor provides a virtual Ethernet switch function that allows partitions fast and secure communication on the same server without any need for physical interconnection or connectivity outside of the server if a Layer 2 bridge to a physical Ethernet adapter is set in one VIOS partition, also known as Shared Ethernet Adapter (SEA).

- /SM590000 Virtual Fibre Channel

A virtual Fibre Channel adapter is a virtual adapter that provides customer LPARs with a Fibre Channel connection to a storage area network through the VIOS partition. The VIOS partition provides the connection between the virtual Fibre Channel adapters on the VIOS partition and the physical Fibre Channel adapters on the managed system.

- /SM590000 Virtual (TTY) console

Each partition must have access to a system console. Tasks, such as operating system installation, network setup, and various problem analysis activities, require a dedicated system console. The PowerVM Hypervisor provides the virtual console by using a virtual TTY or serial adapter and a set of hypervisor calls to operate on them. Virtual TTY does not require the purchase of any other features or software, such as the PowerVM Edition features.

## Logical partitions

LPARs and virtualization increase the usage of system resources and add a level of configuration possibilities.

Logical partitioning is the ability to make a server run as though it were two or more independent servers. When you logically partition a server, you divide the resources on the server into subsets, called LPARs . You can install software on an LPAR, and the LPAR runs as an independent logical server with the resources that you allocated to the LPAR.

LPAR is also referred to in some documentation as a virtual machine (VM), which makes it look similar to what other hypervisors offer. However, LPARs provide a higher level of security and isolation and other features that are described in this chapter.

Processors, memory, and I/O devices can be assigned to LPARs. AIX, IBM i, Linux, and VIOS can run on LPARs. VIOS provides virtual I/O resources to other LPARs with general-purpose operating systems.

LPARs share a few system attributes, such as the system serial number, system model, and processor Feature Codes. All other system attributes can vary from one LPAR to another.

## Micro-Partitioning

When you use the Micro-Partitioningfi technology, you can allocate fractions of processors to an LPAR. An LPAR that uses fractions of processors is also known as a shared processor partition or micropartition . Micropartitions run over a set of processors that is called a shared processor pool (SPP), and virtual processors are used to enable the operating system manage the fractions of processing power that are assigned to the LPAR.

From an operating system perspective, a virtual processor cannot be distinguished from a physical processor, unless the operating system is enhanced to determine the difference. Physical processors are abstracted into virtual processors that are available to partitions.

On the Power11 processor-based server, a partition can be defined with a processor capacity as small as 0.05 processing units. This number represents 0.05 of a physical core. Each physical core can be shared by up to 20 shared processor partitions, and the partition's entitlement can be incremented fractionally by as little as 0.05 of the processor. The shared processor partitions are dispatched and time-sliced on the physical processors under the control of the PowerVM Hypervisor. The shared processor partitions are created and managed by the HMC.

The Power E1150 supports up to 256 cores in a single system and 1000 micropartitions (1000 is the maximum that PowerVM supports).

Note: Although the Power E1150 supports up to 1000 micropartitions, the real limit depends on application workload demands in use on the server.

## Processing mode

When you create an LPAR, you can assign entire processors for dedicated use, or you can assign partial processing units from an SPP. This setting defines the processing mode of the LPAR.

## Dedicated mode

In dedicated mode, physical processors are assigned as a whole to partitions. The SMT feature in the Power11 processor core allows the core to run instructions from two, four, or eight independent software threads simultaneously.

## Shared dedicated mode

On Power11 processor-based servers, you can configure dedicated partitions to become processor donors for idle processors that they own, which allows for the donation of spare CPU cycles from dedicated processor partitions to an SPP. The dedicated partition maintains absolute priority for dedicated CPU cycles. Enabling this feature can help increase system usage without compromising the computing power for critical workloads in a dedicated processor mode LPAR.

## Shared mode

In shared mode, LPARs use virtual processors to access fractions of physical processors. Shared partitions can define any number of virtual processors (the maximum number is 20 times the number of processing units that are assigned to the partition). The PowerVM Hypervisor dispatches virtual processors to physical processors according to the partition's processing units entitlement. One processing unit represents one physical processor's processing capacity. All partitions receive a total CPU time equal to their processing unit's entitlement. The logical processors are defined on top of virtual processors. Therefore, even with a virtual processor, the concept of a logical processor exists, and the number of logical processors depends on whether SMT is turned on or off.

## 9.1.2  Multiple shared processor pools

MSPPs are supported on Power11 processor-based servers. This capability allows a system administrator to create a set of micropartitions with the purpose of controlling the processor capacity that can be used from the physical SPP.

Micropartitions are created and then identified as members of the default processor pool or a user-defined SPP. The virtual processors that exist within the set of micropartitions are monitored by the PowerVM Hypervisor. Processor capacity is managed according to user-defined attributes.

If the Power server is under heavy load, each micropartition within an SPP is assured of its processor entitlement, plus any capacity that might be allocated from the reserved pool capacity if the micropartition is uncapped.

If specific micropartitions in an SPP do not use their processing capacity entitlement, the unused capacity is ceded and other uncapped micropartitions within the same SPP can use the extra capacity according to their uncapped weighting. In this way, the entitled pool capacity of an SPP is distributed to the set of micropartitions within that SPP.

All Power servers that support the MSPP capability have a minimum of one (the default) SPP and up to a maximum of 64 SPPs.

This capability helps customers reduce TCO significantly when the cost of software or database licenses depends on the number of assigned processor-cores.

Shared Processor Pools: Can be used in conjunction with Resource Groups. Such configuration can improve performance and isolation significantly.

## 9.1.3  Virtual I/O Server

The VIOS is part of PowerVM. It is the specific appliance that allows the sharing of physical resources among LPARs to allow more efficient usage (for example, consolidation). In this case, the VIOS owns the physical I/O resources (SCSI, Fibre Channel, network adapters, or optical devices) and allows customer partitions to share access to them, which minimizes and optimizes the number of physical adapters in the system.

The VIOS eliminates the requirement that every partition owns a dedicated network adapter, disk adapter, and disk drive. The VIOS supports OpenSSH for secure remote logins. It also provides a firewall for limiting access by ports, network services, and IP addresses.

Figure 9-1 shows an overview of a VIOS configuration.

Figure 9-1   Architectural view of the VIOS

<!-- image -->

It is a best practice to run dual VIO servers per physical server.

## Shared Ethernet Adapter

A SEA can be used to connect a physical Ethernet network to a virtual Ethernet network. The SEA provides this access by connecting the PowerVM Hypervisor VLANs to the VLANs on the external switches. Because the SEA processes packets at Layer 2, the original MAC address and VLAN tags of the packet are visible to other systems on the physical network. IEEE 802.1 VLAN tagging is supported.

By using the SEA, several customer partitions can share one physical adapter. You can also connect internal and external VLANs by using a physical adapter. The SEA service can be hosted only in the VIOS (not in a general-purpose AIX or Linux partition) and acts as a Layer 2 network bridge to securely transport network traffic between virtual Ethernet networks (internal) and one or more (Etherchannel) physical network adapters (external). These virtual Ethernet network adapters are defined by the PowerVM Hypervisor on the VIOS.

## Virtual SCSI

Virtual SCSI is used to view a virtualized implementation of the SCSI protocol. Virtual SCSI is based on a customer/server relationship. The VIOS LPAR owns the physical I/O resources and acts as a server or, in SCSI terms, a target device. The client LPARs access the virtual SCSI backing storage devices that are provided by the VIOS as clients.

The virtual I/O adapters (a virtual SCSI server adapter and a virtual SCSI client adapter) are configured by using an HMC. The virtual SCSI server (target) adapter is responsible for running any SCSI commands that it receives, and is owned by the VIOS partition. The virtual SCSI client adapter allows a client partition to access physical SCSI and SAN-attached devices and LUNs that are mapped to be used by the client partitions. The provisioning of virtual disk resources is provided by the VIOS.

## iSCSI

The Internet Small Computer Systems Interface (iSCSI) disk is supported in the Virtual I/O Server (VIOS) 3.1.0, or later

The Internet Small Computer Systems Interface (iSCSI) disk provides block-level access to storage devices by carrying SCSI commands over an Internet Protocol network. The iSCSI disk is used to facilitate data transfers over the internet by using TCP, a reliable transport mechanism that uses either IPV6 or IPV4 protocols. The iSCSI disk is used to manage storage over long distances.

The iSCSI support in VIOS allows iSCSI disks to be exported to client logical partitions as virtual disks (vSCSI disks). This support is available in VIOS version 3.1, and later,

VIOS version 3.1 enables Multipath I/O (MPIO) support for the iSCSI initiator. With MPIO support, you can configure and create multiple paths to an iSCSI disk, similar to other protocols. The client logical partition can run either an AIX or Linux operating system.

VIOS version 3.1.1 enables support for multiple iSCSI initiators on the VIOS. This support also includes performance enhancements for the iSCSI driver. With multiple iSCSI initiator support, you can create multiple iSCSI software initiator devices on a single AIX operating system instance.

## N\_Port ID Virtualization

N\_Port ID Virtualization (NPIV) is a technology that allows multiple LPARs to access one or more external physical storage devices through the same physical Fibre Channel adapter. This adapter is attached to a VIOS partition that acts only as a pass-through that manages the data transfer through the PowerVM Hypervisor.

Each partition features one or more virtual Fibre Channel adapters, each with their own pair of unique worldwide port names. This configuration enables you to connect each partition to independent physical storage on a SAN. Unlike virtual SCSI, only the client partitions see the disk.

For more information and requirements for NPIV, see IBM PowerVM Virtualization Managing and Monitoring, SG24-7590 .

## 9.1.4  Live Partition Mobility

LPM enables you to move a running LPAR from one system to another without disruption. Inactive partition mobility allows you to move a powered-off LPAR from one system to another one.

LPM provides systems management flexibility and improves system availability by avoiding the following situations:

- /SM590000 Planned outages for hardware upgrade or firmware maintenance.
- /SM590000 Unplanned downtime. With preventive failure management, if a server indicates a potential failure, you can move its LPARs to another server before the failure occurs.

For more information and requirements for LPM, see IBM PowerVM Live Partition Mobility&lt;Default ‹' Font&gt;, SG24-7460 .

HMCV10R1 and VIOS 3.1.3 or later provide the following enhancements to the LPM Feature:

- /SM590000 Automatically choose the fastest network for LPM memory transfer.
- /SM590000 Allow LPM when a virtual optical device is assigned to a partition.

## 9.1.5  Active Memory Expansion

Active Memory Expansion (AME) is an optional Feature Code (# EMAM ) that belongs to the technologies under the PowerVM umbrella and enables memory expansion on the system.

AME is an innovative technology that supports the AIX operating system. It helps enable the effective maximum memory capacity to be larger than the true physical memory maximum. Compression and decompression of memory content can enable memory expansion up to 100% or more. This expansion can enable a partition to complete more work or support more users with the same physical amount of memory. Similarly, it can enable a server to run more partitions and do more work for the same physical amount of memory.

AME uses CPU resources to compress and decompress the memory contents. The trade-off of memory capacity for processor cycles can be an excellent choice, but the degree of expansion varies about how compressible the memory content is. It also depends on having adequate spare CPU capacity available for this compression and decompression.

The Power E1080 includes a hardware accelerator that is designed to boost AME efficiency and uses less processor core resources. Each AIX partition can turn on or turn off AME. Control parameters set the amount of expansion that is wanted in each partition to help control the amount of CPU used by the AME function.

An IPL is required for the specific partition that is turning memory expansion. When enabled, monitoring capabilities are available in standard AIX performance tools, such as lparstat , vmstat , topas , and svmon .

A planning tool is included with AIX, which enables you to sample workloads and estimate how expandable the partition's memory is and much CPU resource is needed. The feature can be ordered with the initial order of the Power E1080 or as a Miscellaneous Equipment Specification (MES) order. A software key is provided when the enablement feature is ordered, which is applied to the system node. An IPL is not required to enable the system node. The key is specific to an individual system and is permanent. It cannot be moved to a different server.

IBM i does not support AME.

## 9.1.6  Remote Restart

Remote Restart is a high availability option for partitions. If an error occurs that causes a server outage, a partition that is configured for Remote Restart can be restarted on a different physical server. At times, it might take longer to start the server, in which case the Remote Restart function can be used for faster reprovisioning of the partition. Typically, Remote Restart can be done faster than restarting the server that stopped and then restarting the partitions. The Remote Restart function relies on technology that is similar to LPM where a partition is configured with storage on a SAN that is shared (accessible) by the server that hosts the partition.

HMC V10R1 provides an enhancement to the Remote Restart Feature that enables remote restart when a virtual optical device is assigned to a partition.

## 9.1.7  POWER processor modes

Although they are not virtualization features, the POWER processor modes are described here because they affect various virtualization features.

On Power servers, partitions can be configured to run in several modes, including the following modes:

- /SM590000 Power9

This native mode for Power9 processors implements Version 3.0 of the IBM Power ISA. For more information, see IBM Documentation.

- /SM590000 Power10

This native mode for Power10 processors implements Version 3.1 of the IBM Power ISA. For more information, see IBM Documentation.

- /SM590000 Power11

This native mode for Power11 processors implements Version 3.X of the IBM Power ISA. For more information, see IBM Documentation.

Figure 9-2 shows the available processor modes on a Power E1180.

Figure 9-2   Processor modes

<!-- image -->

Processor compatibility mode is important when LPM migration is planned between different generations of servers. An LPAR that might be migrated to a machine that is managed by a processor from another generation must be activated in a specific compatibility mode.

Note: Migrating an LPAR from a POWER9 processor-based server to Power E1180 by using LPM is not supported; however, the following steps can be completed to accomplish this task:

1. Migrate LPAR from POWER9 processor-based server to Power10 processor-based server by using LPM.
2. Migrate then the LPAR from Power10 processor-based server to Power11.

## 9.1.8  Single Root I/O Virtualization

Single Root I/O Virtualization (SR-IOV) is an extension to the Peripheral Component Interconnect Express (PCIe) specification that allows multiple operating systems to simultaneously share a PCIe adapter with little or no runtime involvement from a hypervisor or other virtualization intermediary.

SR-IOV is a PCI standard architecture that enables PCIe adapters to become self-virtualizing. It enables adapter consolidation through sharing much like logical partitioning enables server consolidation. With an adapter capable of SR-IOV, you can assign virtual slices of a single physical adapter to multiple partitions through logical ports without using a VIOS.

## 9.1.9  More information about virtualization features

The following IBM Redbooks publications provide more information about the virtualization features:

- /SM590000 IBM PowerVM Best Practices, SG24-8062
- /SM590000 IBM PowerVM Virtualization Introduction and Configuration, SG24-7940
- /SM590000 IBM PowerVM Virtualization Managing and Monitoring, SG24-7590
- /SM590000 IBM Power Systems SR-IOV: Technical Overview and Introduction, REDP-5065

## 9.1.10  Resource groups

Resource Groups are a new feature introduced with IBM Power11, designed to enhance system performance by up to 25% through improved workload optimization and resource affinity. While Shared Processor Pools (SPPs) already provide compute capacity isolation by capping the maximum resources available to each pool, Resource Groups take this further by introducing advanced affinity-based optimizations for more efficient workload dispatching.

Early performance evaluations indicate that, when configured effectively, Resource Groups can deliver shared processor performance that closely matches that of dedicated processors - particularly in large, partitioned environments.

## Resource Groups Properties

With the introduction of Resource Groups in IBM Power11, system administrators gain a powerful new tool for organizing and optimizing compute resources. Designed for flexibility and performance, Resource Groups allow for the grouping of both dedicated and shared processor partitions, enabling more granular control over resource allocation and workload management. This feature enhances system efficiency, supports dynamic reconfiguration, and integrates seamlessly with existing technologies like Shared Processor Pools, Live Partition Mobility, and PEP 2.0.

The core features of resource groups are:

- /SM590000 Performance Optimization:
- -Resource Groups improve workload affinity, enabling shared processor performance nearly equivalent to dedicated processors.
- /SM590000 Flexible Configuration:
- -Can include both dedicated processor partitions and shared processor partitions.
- -Resource group configuration will specify the number of general purpose cores (AIX/IBM i/Linux/VIOS) and IFL cores (Linux/VIOS).
- -Powered-off partitions can be reassigned between groups.

- -Resources not assigned to a user defined group are placed in the default resource group.
- -Cores can be dynamically reallocated among groups.
- /SM590000 Affinity and Isolation:
- -Each resource group has its own set of shared processor pools.
- -Enhances compute capacity isolation and affinity-based dispatching.
- /SM590000 Monitoring and Management:
- -Utilization metrics available for Resource Groups, SPPs, and the overall system.
- -Dynamic Platform Optimizer (DPO) can run at system or group level.
- /SM590000 Mobility and Compatibility:
- -When using Live Partition Mobility the resource group on the target system can be selected.
- -Fully compatible with PEP 2.0.

## Resource Groups use cases

There are multiple scenarios in which Resource Groups can deliver significant benefits to customers. In this section, we provide several example use cases, but beyond the examples provided, use cases can also be combined to maximize functionality and tailor performance to specific business needs.

## Consolidation across multiple lines of business

Figure 9-3 illustrates how resources are allocated and shared across multiple lines of business on Power10 systems. With the introduction of Resource Groups in Power11, resource allocation becomes significantly more efficient, enabling better workload isolation and improved performance across the system.

Figure 9-3   Example where Resource Groups consolidate multiple lines of business

<!-- image -->

## Isolation of production workloads from test/dev workloads

In this example, the development and test LPARs are restricted to using resources only from the default Resource Group, limiting their access to shared system resources and isolating them from production workloads.

Figure 9-4 shows how resource groups are used to provide this isolation of development and test workloads.

Figure 9-4   Example where Resource Groups isolate workloads

<!-- image -->

## Improve application performance by grouping workload tiers into resource groups

In this example, performance is enhanced through improved affinity between the database and application server, allowing for more efficient resource utilization and faster communication within the same Resource Group.Figure 9-5 shows how grouping workloads into different resource groups can improve application affinity and improve performance.

Figure 9-5   Example where Resource Groups improve performance by grouping workload tiers

<!-- image -->

## System-level isolation in multi-server consolidation scenarios

Resource Groups enable effective workload isolation on IBM Power11 systems. This allows organizations to consolidate multiple smaller servers into a single, more powerful Power11 system while logically separating workloads into distinct groups. Each group can be managed independently, ensuring performance, security, and resource control across different business

functions or environments. Figure 9-6 shows how using resource groups can help maintain system level isolation while doing server consolidation.

Figure 9-6   Example where Resource Groups provides additional layer of workload isolation

<!-- image -->

## Improved performance by mapping Shared Processor Pools into Resource Groups

Shared Processor Pools (SPPs) and Resource Groups can be used together on IBM Power11 systems to simultaneously optimize application performance and reduce total cost of ownership (TCO). This combination allows for precise control over resource allocation and workload isolation, ensuring efficient utilization of compute capacity while maintaining performance consistency across diverse workloads.

Figure 9-7 shows how SPPs and Resource Groups enhance your performance and reduces your TCO.

Figure 9-7   Example where Resource Groups improve performance by mapping Shared Processor Pools into Resource Groups.

<!-- image -->

## Resource Groups Advisor

Resource Groups Advisor (RGA) is a web-based tool designed to assist with the configuration of resource groups on IBM Power Systems. Offered as a free service, RGA analyzes a customer's server setup and provides tailored recommendations for optimal resource group configurations. It helps model and validate configurations to ensure efficient resource allocation and system performance.

## RGA is available at:

https://www.ibm.com/it-infrastructure/resources/resource-groups-advisor ,

The welcome screen for RGA is shown in Figure 9-8.

Figure 9-8   Welcome screen

<!-- image -->

After selecting Getting Started, you see a screen where you can choose to create a new configuration or upload a previously saved version. Selecting New Configuration brings up the screen shown in Figure 9-9.

Figure 9-9   New configuration screen

<!-- image -->

After providing the system configuration parameters, press Next to be presented with the resource group configuration planned for your system. This is shown in Figure 9-10. The default resource group is always configured, but you can choose to add additional resource groups to your configuration.

Figure 9-10   Resource group definitions

<!-- image -->

After defining your resource groups, you will be given the opportunity to define the LPAR details. Selecting Add LPAR allows you to enter the details about each LPAR such as whether it is running in Shared or Dedicated mode, the processor and memory allocations, and which resource group the LPAR is assigned to. LPARs can also be added from the IBM System Planning Tool.

When all of your LPARs are defined, you will see an LPAR summary screen similar to Figure 9-11.

Figure 9-11   LPAR configuration

<!-- image -->

After entering all of the LPAR information, press Submit to get the model's output similar to what is shown in Figure 9-12.

Figure 9-12   Output from the tool

<!-- image -->

You can save the configuration at this point, or start a new model.

Note: RGA simulates how PowerVM would create resource groups and allocate resources to LPARs on a system, all based on the provided input configurations. No 'customer data' is being pulled off actual servers and assessed.

## 9.2  KVM support

IBM Power11 processor based servers can utilize Kernel-based Virtual Machine (KVM) within a PowerVM logical partition (LPAR). This allows for the creation and management of lightweight Linux virtual machines (VMs) using standard KVM tools, while still leveraging the existing resources of the PowerVM LPAR. Essentially, KVM becomes an additional virtualization option alongside PowerVM on IBM Power.

KVM on IBM Power11 processor based servers is not a replacement for PowerVM, but rather an additional capability that brings the power, speed, and flexibility of the KVM virtualization technology to a PowerVM logical partition (LPAR).

KVM-enabled LPARs can host PPC64-LE KVM guests, which are essentially Linux VMs. The KVM guests within the LPAR utilize resources (CPU, memory, I/O) that have been allocated to the LPAR by the PowerVM hypervisor. This approach offers flexibility in deploying Linux workloads and can be more cost-effective than other virtualization solutions, especially for organizations already invested in the Linux ecosystem. KVM's integration with the Linux kernel can lead to high performance, especially when running Linux-based workloads. This setup enables use cases such as running standard Linux distributions, containers, and other workloads that benefit from the KVM virtualization stack and the benefit to consolidate in one or more IBM Power11 processor based servers different kind of workloads (see Figure 9-13 on page 194 for an example). In essence, KVM on Power10 provides a way to leverage the strengths of both PowerVM and KVM, offering a powerful and flexible virtualization environment for Linux workloads.

Figure 9-13   IBM Power stack for KVM

<!-- image -->

About capabilities, KVM in a PowerVM LPAR utilizes the industry standard Linux KVM virtualization stack and can easily integrate within an existing Linux virtualization ecosystem. KVM in an LPAR is enabled by:

- /SM590000 IBM Power architecture and Power11 implementation has advanced virtualization capabilities to run multiple operating system (OS) instances that share the same hardware resources while providing isolation. The Radix MMU architecture provides the capability to independently manage page tables for the LPAR and the KVM guest instances on the LPAR.
- /SM590000 PowerVM industry-leading virtualization stack provides new functions to create and manage KVM guests. These changes extend the Power platform architecture to include new hypervisor interfaces.
- /SM590000 Linux kernel that includes the KVM kernel module (KVM) provides core virtualization infrastructure to run multiple virtual machines in a Linux host LPAR. Upstream kernels and enabled downstream distributions such as Fedora and Ubuntu use the newly introduced Power architecture extensions to create and manage KVM guests in the PowerLinux LPAR.
- /SM590000 QEMU: user space component that implements virtual machines on the host that use KVM functions.

- /SM590000 LibVirt provides a toolkit for virtual machine management.

Figure 9-14   Industry Standard Linux Virtualization Stack

<!-- image -->

KVM support on IBM Power11 processor based server requires:

- /SM590000 Partition must be a Linux partition that runs in Power10 processor compatibility mode;
- /SM590000 Partition must be enabled for KVM:
- -For HMC-managed systems, you must set the partition to KVM Capable;
- -For unmanaged systems, you must set the default partition environment to Linux KVM on the BMC;
- /SM590000 Partition must be running in Radix mode (default MMU mode for Linux LPARs);
- /SM590000 partition must be assigned with dedicated CPUs with processor sharing set to Never Allow;

The following features are not supported on KVM logical partitions:

- /SM590000 Shared processors
- /SM590000 vPMEM LUNs
- /SM590000 Platform keystore
- /SM590000 Live partition migration (LPM)
- /SM590000 Dynamic platform optimization
- /SM590000 Add or Remove memory, processor, and I/O DLPAR

The following KVM features for guests are not supported:

- /SM590000 PCI pass-through of LPAR-attached PCI devices to KVM guests. This feature will be supported in future releases.

## See also

https://www.ibm.com/docs/en/linux-on-systems?topic=servers-kvm-in-powervm-lpar for additional references.

<!-- image -->

Chapter 10.

10

## Hybrid Cloud Solutions

This chapter explores IBM Power servers as a foundational platform for hybrid cloud environments, emphasizing their ability to support mission-critical workloads with exceptional performance, security, and flexibility. Built on the advanced Power10 architecture, IBM Power Systems deliver significant improvements in compute efficiency and operational cost savings. These systems are engineered to integrate seamlessly across on-premises, private, and public cloud infrastructures, supporting a wide range of enterprise operating systems including AIX, IBM i, and Linux.

The chapter also highlights IBM's strategic approach to hybrid cloud, showcasing how Power servers enable scalable, cloud-native application development using Kubernetes and Red Hat OpenShift. Key management tools such as IBM PowerVC, IBM Cloud Management Console, and Power Enterprise Pools 2.0 are discussed for their roles in simplifying virtualization, optimizing resource allocation, and enhancing operational agility. Additionally, the chapter introduces IBM Power Virtual Server (PowerVS), a cloud-based extension of Power Systems that allows organizations to modernize at their own pace. PowerVS offers flexible, pay-as-you-go access to virtualized Power infrastructure, enabling seamless workload migration, rapid provisioning, and robust business continuity capabilities.

This chapter contains the following topics:

- /SM590000 IBM Power Private Cloud with Shared Utility Capacity
- /SM590000 Red Hat OpenShift
- /SM590000 Power Virtual Server

## 10.1  IBM Power Private Cloud with Shared Utility Capacity

IBM Power Systems Private Cloud with Shared Utility Capacity introduces a flexible and efficient approach to resource management through Power Enterprise Pools (PEP) 2.0. This innovation allows multiple IBM Power Systems to operate as a unified resource pool, enabling shared compute capacity across systems. Designed for clients deploying private cloud infrastructure, PEP 2.0 enhances operational agility by supporting dynamic, minute-by-minute consumption of compute resources.

In this model, each system within the pool is provisioned with a set number of base processor and memory activations. When configured as a Power Enterprise Pool, these base activations and their associated operating system entitlements are aggregated to define the pool's baseline capacity. Simultaneously, all hardware resources across the pool are fully activated and made available for workload deployment - allowing usage to exceed the base capacity when needed. IBM Cloud Management Console continuously monitors average resource usage across the pool and compares it to the base entitlement. Any usage beyond the base is billed as metered capacity, either through Prepaid Capacity Credits or monthly billing. PEP 2.0 eliminates the need for manual reallocation of activations between systems, offering granular resource sharing, improved cost efficiency, and simplified management for enterprise IT environments.

Power Enterprise Pools with Shared Utility Capacity on Power E1150 systems provides enhanced multi-system resource sharing and by-the-minute tracking and consumption of compute resources across a collection of Power E1150 and E1050 systems within a single Power Enterprise Pools (2.0). Shared Utility Capacity provides a complete range of flexibility to tailor initial system configurations with the right mix of purchased and pay-for-use consumption of processor, memory, and software. Clients with existing Power Enterprise Pools 2.0 of Power E1050 systems can simply add one or more Power E1150 systems into their pool and migrate to them at the rate and pace of their choosing, as any Power E1150 and Power E1050 systems may seamlessly interoperate and share compute resources within the same pool. Clients with Mobile Capacity on a Power E1150 may easily upgrade their system to support Power Enterprise Pools 2.0 to leverage Shared Utility Capacity resources.

A Power infrastructure consolidated onto Power E1150 systems has the potential to greatly simplify system management so IT teams can focus on optimizing their business results instead of moving resources around within their data center. Shared Utility Capacity resources are easily tracked by virtual machine (VM) and monitored by a CMC, which integrates with local HMCs to manage the pool and track resource use by system and VM, by the minute, across a pool.

Clients need not over-provision capacity on each individual system to support growth, as all available processor and memory on all systems in a pool are activated and available for use. On Power E1150 systems, Mobile and Shared Utility Capacity capabilities are now offered through the purchase of a Power Enterprise Pools Subscription (5765-P2E) for each Power E1150 system being configured for use in a Power Enterprise Pool.

When a Power Enterprise Pool 2.0 pool is started, each eligible Power E1150 system's purchased processor activations, memory activations, and supported operating system entitlement resources become Base Capacity resources as part of the Power Enterprise Pool and are aggregated across a defined pool of systems for consumption monitoring. Metered Capacity is the additional installed processor and memory resource above each system's Base Capacity. It is activated and made available for immediate use when a pool is started, then monitored by the minute by a CMC.

Metered resource usage is charged only for minutes exceeding the pool's aggregate Base resources, and usage charges are debited in real-time against a client's Capacity Credits (5819-CRD) on account or may be billed monthly, in arrears, where available.

Important: Note: Only two consecutive generations of Power servers are supported in the same Power Enterprise Pool. Customers who want to add Power11 to their existing pool with Power9 and Power10 processor-based systems, are recommended to reach out to their IBM representatives or IBM Expert Labs to discuss transition planning and best practices.

## 10.1.1  IBM Cloud Management Console

As private and hybrid cloud deployments continue to expand, enterprises require deeper management insights into these increasingly complex environments. Tools that deliver unified analytics and consolidated information are essential for ensuring smooth and efficient infrastructure operations.

The IBM Cloud Management Console for Power Systems offers a comprehensive view of your Power Systems cloud environment - regardless of the number of systems or data centers involved. It provides:

- /SM590000 Centralized inventory management of systems and virtual components
- /SM590000 Consolidated performance metrics to help optimize resource utilization and system performance across all data centers
- /SM590000 Aggregated logging and analytics for enhanced operational insights

This unified approach empowers IT teams to manage their infrastructure more effectively and make informed decisions with confidence. The IBM Cloud Management Console (CMC) now offers full support for IBM Power11 systems, ensuring compatibility with the latest advancements in Power architecture.

The IBM Cloud Management Console (CMC) is also used to monitor and manage Power Enterprise Pool 2.0 pools in your enterprise. The CMC Enterprise Pools 2.0 application helps you to monitor base and metered capacity across a Power Enterprise Pool 2.0, with summary and sophisticated drill-down views of real-time and historical resource consumption by the logical partition.

CMC provides a cloud-based interface to monitor, manage, and optimize IBM Power infrastructure. It supports AIX, IBM i, and Linux VM workloads, and is accessible through a secure, web-based dashboard.   The IBM Cloud Management Console is a cloud-based portal that allows clients to:

- /SM590000 View health and performance of IBM Power servers.
- /SM590000 Monitor capacity and performance metrics.
- /SM590000 Get insights into firmware levels and configurations.
- /SM590000 Receive proactive support and alerts.

For organizations with limited IT staff or those looking to streamline infrastructure oversight, CMC offers a way to perform essential monitoring and management tasks without requiring complex, locally hosted solutions.

CMC integrates with the Hardware Management Console (HMC) and provides advanced insights into performance, capacity, firmware compliance, patch planning, and system health. With Power11's scalability and built-in virtualization, CMC helps administrators maintain

control over increasingly complex hybrid IT environments, whether on-premises or connected to IBM Cloud or other supported clouds.

CMC is offered through a software-as-a-service (SaaS) model and is licensed on a per-server subscription basis. Power11 systems are registered to IBM CMC through their HMCs, which securely transmit telemetry and configuration data to the cloud portal. IBM provides both a no-cost base offering and optional chargeable add-ons that include more advanced features, such as predictive analytics, patch automation, and longer data retention for historical performance views. Customers can choose monthly or annual billing, and entitlement can be managed through IBM Passport Advantagefi or directly through the IBM Cloud portal.

## 10.2  Red Hat OpenShift

With the growing demand for digital services and the continuous release of new offerings, organizations face the challenge of delivering exceptional customer experiences while maintaining resilient and highly available services. To meet these demands, they must innovate - by developing new applications, modernizing existing ones, and ensuring seamless scalability.

So how can organizations achieve this? By embracing cloud technologies - whether through full cloud migration, developing cloud-native applications, or adopting a hybrid cloud approach.

Red Hat OpenShift on IBM Power empowers both developers and IT operations teams with the flexibility and speed needed to build, deploy, and manage applications across diverse environments - on-premises, in the cloud, or across multiple infrastructures. This platform accelerates digital transformation by simplifying scalability and enhancing security.

## 10.2.1  Red Hat OpenShift on Power

Red Hat OpenShift Container Platform is fully supported on IBM Power Servers, offering a robust and scalable environment for running cloud-native applications and modernizing traditional workloads on the IBM Power architecture (ppc64le). This integration enhances the IBM Power software ecosystem by enabling enterprise-grade container orchestration for building, deploying, and managing containerized applications with OpenShift.

OpenShift on IBM Power provides a flexible and efficient platform for hybrid IT environments, enabling the seamless coexistence of cloud-native applications with traditional VM-based workloads. By leveraging the same infrastructure, it allows collocation of containerized workloads alongside existing applications running on AIX, IBM i, or Linux, ensuring low-latency communication between applications and data. IBM Power servers provide an extremely resilient and secure platform and how they can be an excellent platform for your cloud implementation using OpenShift. We then describe the benefits of using a multi-architecture cluster and provide implementation guidelines and advice to assist the reader in implementing a multi-architecture cluster using IBM Power control plane nodes and x86 or AMD-based worker nodes.

Red Hat OpenShift is a leading enterprise Kubernetes platform that provides a robust foundation for developing, deploying, and scaling cloud-native applications. It extends Kubernetes with additional features and tools to enhance productivity and security, making it an ideal choice for businesses looking to leverage container technology at scale.

Red Hat OpenShift is a unified platform to build, modernize, and deploy applications at scale. Work smarter and faster with a complete set of services for bringing apps to market on your

choice of infrastructure. Red Hat OpenShift delivers a consistent experience across public cloud, on-premise, hybrid cloud, or edge architecture.

Red Hat OpenShift offers you a unified, flexible platform to address a variety of business needs spanning from an enterprise-ready Kubernetes orchestrator to a comprehensive cloud-native application development platform that can be self-managed or used as a fully managed cloud service

## Red Hat OpenShift clusters

A Red Hat OpenShift Container Platform cluster consists of multiple nodes, which can run on either physical or virtual machines. At a minimum, the following node requirements apply:

- /SM590000 Three control plane nodes to manage and maintain the cluster
- /SM590000 Two worker nodes to run containerized workloads
- /SM590000 A temporary bootstrap node used during installation to host configuration and installation files

In OpenShift Container Platform 4.18, the bootstrap and control plane nodes must run Red Hat Enterprise Linux CoreOS (RHCOS), a minimal, immutable container host OS derived from Red Hat Enterprise Linux (RHEL). Compute nodes can run either RHCOS or standard RHEL, depending on the deployment architecture. RHEL machines are deprecated in OpenShift Container Platform 4.16 and will be removed in a future release.

## Multiple Architecture Compute clusters

Enterprises expanding their operations often deploy applications in heterogeneous environments utilizing different types of hardware. A prime example includes data centers using x86\_64 servers and Power servers, while edge locations might feature ARM-based devices due to their power efficiency. We discussed why IBM Power is an ideal choice for running Red Hat OpenShift clusters. However, it is possible that some applications within the environment aren't compatible with IBM Power architecture nodes. You can retain the benefits of IBM Power while retaining simpler cluster management and efficient resource utilization by incorporating both IBM Power and x86\_64 architecture nodes in the same cluster.

The release of Red Hat OpenShift 4.14 brought the OpenShift Container Platform Multiple-Architecture Compute feature to IBM Power. Multi-Arch Compute provides a single heterogeneous cluster, enabling fit-for-purpose computing so clients can align tasks and applications to CPU strengths and software availability rather than one architecture. This support was expanded in Red Hat OpenShift 4.15 which enabled a Red Hat OpenShift cluster to support an IBM Power control plane and add x86 architecture worker nodes.

Multi-Arch Compute for OpenShift Container Platform lets you use a pair of compute architectures, such as ppc64le and amd64, within a single cluster. This exciting feature opens new possibilities for versatility and optimization for composite solutions that span multiple architectures.

Multi-architecture capability offers several benefits:

- /SM590000 Platform independence
- Multi-Arch Compute enables applications to function flawlessly across various hardware platforms, including Intel servers in data centers, ARM-based Raspberry Pis in remote locations, and IBM Power systems in corporate settings.
- /SM590000 Reduced complexity through standardizing application deployment across architectures
- Streamlines operations and eliminates the need to maintain separate stacks for different hardware. Cost efficiency: Differing hardware architectures provide varying

cost-to-performance ratios, which can be exploited to minimize overall infrastructure expenses.

- /SM590000 Multi-architecture support facilitates optimal resource usage

Organizations can select the most cost-effective architecture for each specific workload. For instance, ARM servers could be more affordable for lightweight services, whereas x86\_64 or IBM Power servers might excel at handling heavy computational tasks.

- /SM590000 Energy efficiency

Specific architectures, like ARM, are renowned for their low power consumption, which can substantially decrease energy costs, notably in scale-out scenarios like IoT and mobile services.

- /SM590000 Scalability and flexibility

Implementing applications on multiple architectures enhances scalability and operational agility, which is vital for businesses experiencing fluctuating loads.

Multi-architecture support allows companies to:

- /SM590000 Scale elastically across platforms: Multi-Arch Compute enables businesses to dynamically allocate resources among different types of hardware to handle surges in demand without being restricted to a single architecture.
- /SM590000 Escape vendor lock-in: With Multi-Arch Compute, corporations are unshackled from a single supplier or type of hardware, thus avoiding vendor lock-in and empowering more bargaining power during procurement decisions.
- /SM590000 Optimize performance: Each architecture boasts unique strengths and weaknesses contingent upon the application or workload. Multi-Arch Compute maximizes performance by aligning application requirements with the architectural advantages.
- /SM590000 Create tailored solutions: Select applications might gain from the high I/O throughput of IBM Power systems, while others could perform optimally on the high-throughput, multi-core processors of x86\_64 architectures.
- /SM590000 Meet specialized computing needs: Certain tasks may necessitate specialized hardware, such as GPUs for machine learning work flows, which might be more accessible or better supported on particular architecture.

## Single Node clusters

For a simpler and more accessible deployment experience, OpenShift now supports a Single Node cluster where all of the management and worker functions are installed on a single node. This approach is especially suitable for development, testing, or edge computing environments.

However, it is important to recognize the inherent limitations of a single-node configuration most notably, the absence of built-in high availability. If the node fails, the entire cluster becomes unavailable, potentially resulting in downtime and data loss. That said, the robust high availability features of IBM Power infrastructure, when properly architected with redundant power, networking, and storage, can significantly mitigate these risks.

## 10.2.2  Red Hat OpenShift AI on IBM Power Servers

OpenShift AI brings enterprise-grade capabilities for artificial intelligence and machine learning directly into the Red Hat OpenShift platform, enabling organizations to build, train, deploy, and monitor AI models within a unified, containerized environment. Red Hat OpenShift AI is built on Red Hat OpenShift, an enterprise-grade Kubernetes platform designed to support AI/ML workloads across hybrid and multi-cloud infrastructures. By

integrating AI workflows into the same infrastructure used for traditional applications, OpenShift AI simplifies operations, accelerates development cycles, and ensures consistent governance and security across the entire AI lifecycle. Red Hat OpenShift AI (RHOAI) enables organizations to build, deploy, and manage AI-enabled applications at scale across hybrid cloud environments.

OpenShift AI (RHOAI) Self-Managed is available by installing the Red Hat OpenShift AI Operator and configuring it to manage the standalone components of the platform. RHOAI Self-Managed is supported on Red Hat OpenShift Container Platform across multiple architectures, including ppc64le (IBM Power), s390x (IBM Z) and x86\_64. This includes support for the following infrastructure providers:

- /SM590000 IBM Power (Technology Preview)
- /SM590000 IBM Z (Technology Preview)
- /SM590000 IBM Cloud
- /SM590000 Red Hat OpenStack
- /SM590000 Bare Metal
- /SM590000 Hosted control planes on Bare Metal
- /SM590000 Amazon Web Services
- /SM590000 Google Cloud Platform
- /SM590000 Microsoft Azure
- /SM590000 VMware vSphere
- /SM590000 Oracle Cloud

IBM Power Servers, renowned for their high performance, scalability, and reliability, are particularly well-suited for compute-intensive AI workloads. The integration of RHOAI with IBM Power Servers enables enterprises to take advantage of both platforms' capabilities for robust, production-grade AI deployments.

As of RHOAI version 2.20 (Self-Managed), the IBM Power architecture (ppc64le) is available as a Technology Preview. Currently, model deployments on IBM Power are supported only in standard mode. RHOAI provides an integrated platform for developing, training, serving, and monitoring AI/ML models. I Support for vLLM is also available on the IBM Power architecture as a Technology Preview, with vLLM runtime templates accessible for experimentation and development.

When deployed on IBM Power Systems, OpenShift AI benefits from the platform's high-performance architecture, which is optimized for data-intensive and compute-heavy workloads. The latest IBM Power processors feature built-in AI inferencing capabilities, delivering faster insights and reduced latency for real-time applications. This will further be enhanced with the planned availability and support of the IBM Spyre™ Accelerator in IBM Power servers. The Spyre adapter is a PCIe-attached AI card engineered for low-precision AI arithmetic and enterprise-grade AI algorithms, making it ideal for deploying large-scale models and generative AI frameworks.

Together, OpenShift AI and IBM Power provide a scalable, production-ready foundation for modern AI initiatives. Whether running models built with PyTorch, TensorFlow, or vLLM, this integrated stack supports hybrid cloud deployments and enables enterprises to operationalize AI with confidence - ensuring performance, efficiency, and flexibility across diverse workloads and environments.

## 10.2.3  IBM Cloud Paks

IBM Cloud Paks are a suite of modular, containerized software solutions designed to accelerate digital transformation. Built on Red Hat OpenShift, they enable organizations to modernize applications, automate business operations, manage data, and integrate AI across hybrid cloud environments with consistency and scalability.

Each Cloud Pak includes a combination of IBM middleware, open-source technologies, Kubernetes operators, and enterprise-grade security, providing a robust foundation for innovation and agility in the cloud. IBM Cloud Paks take a bundled approach that allows you to accelerate your modernization journey by packaging everything you need to get started.

There are three main benefits of IBM Cloud Paks:

- /SM590000 They are comprehensive and easy to use.
- /SM590000 They are supported by IBM.
- /SM590000 They run anywhere Red Hat OpenShift runs.

## IBM Cloud Paks on IBM Power

Optimized for deployment on IBM Power Systems, IBM Cloud Paks deliver high performance and efficiency for containerized workloads. They support a wide range of use cases - from data and automation to AI and security - while ensuring seamless integration across on-premises and cloud environments.

IBM Power enhances the value of Cloud Paks by offering superior performance, scalability, and cost-efficiency for modern workloads. Combined with Red Hat's open-source ecosystem, it empowers businesses to both modernize legacy applications and build new cloud-native solutions on a unified, enterprise-ready platform.

There are several IBM Cloud Paks that are currently supported on IBM Power, each focused on a specific domain:

- /SM590000 Cloud Pak for Applications
- /SM590000 Cloud Pak for Data
- /SM590000 Cloud Pak for Integration
- /SM590000 Cloud Pak for AIops

## IBM Cloud Pak for Applications

IBM Cloud Pak for Applications is an enterprise-ready, containerized software solution designed to modernize existing applications and develop new cloud-native apps. Built on IBM WebSpherefi offerings and Red Hat OpenShift Container Platform, it provides a comprehensive set of tools to help organizations transition between public, private, and hybrid clouds.

IBM Cloud Pak for Applications includes IBM Cloud Transformation Advisor, an AI-powered tool which assists in refactoring and rearchitecting legacy applications. The solution includes automated vulnerability assessment and identification, ensuring continuous security compliance across all deployment environments. It also automates audit reporting, simplifying compliance management. Developers can use their preferred IDEs to build and deploy applications, with support for modern runtimes and DevOps workflows. This integration streamlines the development process and enhances productivity.

## IBM Cloud Pak for Data

IBM Cloud Pak for Data allows you to unify and simplify data collection, organization, and analysis. It is ideal for AI and analytics workloads. IBM Cloud Pak for Data is a unified, preintegrated data and AI platform designed to help organizations collect, organize, analyze, and infuse AI into their data. Running natively on the Red Hat OpenShift Container Platform, it supports deployment across various cloud environments, including IBM Cloud, Amazon Web Services (AWS), and Microsoft Azure.

The platform allows secure access to data at its source, eliminating the need for data migration and reducing data silos, ensuring seamless data integration. It creates a trusted, business-ready analytics foundation, simplifying data preparation, policy enforcement, security, and compliance, while automating data governance and the AI lifecycle. IBM Cloud Pak for Data provides tools for building, deploying, and managing AI and machine learning models, scaling these capabilities consistently across the organization to enable comprehensive data analysis and insights.

By operationalizing AI throughout the business with trust and transparency, the platform supports the end-to-end AI workflow, ensuring effective integration of AI into business processes. Offering a single interface for end-to-end analytics with built-in governance, IBM Cloud Pak for Data simplifies the management of data and AI capabilities, while its scalable Kubernetes environment allows organizations to grow their data and AI capabilities as needed. Supporting multi-cloud deployments, it provides agility and avoids vendor lock-in, making it a powerful tool for accelerating the journey to AI and unlocking the value of data for AI-driven digital transformation.

## IBM Cloud Pak for Integration

IBM Cloud Pak for Integration is a comprehensive, AI-powered hybrid integration platform designed to connect applications, data, systems, and services across any environment. It provides a unified experience with a suite of integration tools that streamline the creation, management, and deployment of integrations. Running on Red Hat OpenShift, IBM Cloud Pak for Integration supports both cloud and on-premises deployments, ensuring scalability and security. The platform includes components such as IBM API Connect for managing APIs, IBM App Connect for no-code integration, and IBM Event Streams for real-time data processing. By leveraging AI and automation, IBM Cloud Pak for Integration accelerates the integration process, reduces manual workflows, and enhances responsiveness to real-time events. This makes it an ideal solution for organizations looking to modernize their integration capabilities and drive digital transformation.

## IBM Cloud Pak for Business Automation

IBM Cloud Pak for Business Automation is a modular set of integrated software components designed to automate work and accelerate business growth. Built for any hybrid cloud, it simplifies complex workflows, facilitates records management, and enhances overall productivity. The platform uses AI to identify gaps and build low-code and no-code automations, making it easier to streamline operations. Running on Red Hat OpenShift, IBM Cloud Pak for Business Automation supports containerized deployments across various cloud environments, providing flexibility and scalability. Key features include automating case and process workflows, converting unstructured content into valuable data, and using software robots to complete tasks based on AI insights. This comprehensive automation solution helps organizations improve efficiency, reduce operational costs, and drive continuous process improvements.

## IBM Cloud Pak for AIOps

IBM Cloud Pak for AIOps is an advanced, AI-powered platform designed to enhance IT operations (ITOps) by leveraging artificial intelligence and machine learning. It integrates seamlessly with existing ITOps toolchains to provide comprehensive visibility, proactive incident management, and automated remediation. By analyzing data from various sources, such as logs, metrics, and events, IBM Cloud Pak for AIOps helps IT teams predict and resolve issues before they impact business operations. The platform supports hybrid cloud environments, enabling organizations to manage their IT infrastructure across on-premises, cloud, and containerized environments. Key features include event correlation and compression, anomaly detection, root cause analysis, and automated runbooks, all aimed at reducing mean time to resolution (MTTR) and improving overall operational efficiency. With its collaborative tools and real-time insights, IBM Cloud Pak for AIOps empowers IT teams to innovate faster, reduce operational costs, and ensure the reliability of mission-critical workloads.

For more information about IBM Cloud Paks see this IBM documentation.

## 10.2.4  Other cloud enablement solutions

From a Red Hat software perspective, there is also a comprehensive set of software solutions to accelerate your modernization efforts, including Red Hat Runtimes, Red Hat 3scale API Management, Red Hat Fuse and Red Hat AMQ.

## 10.3  Power Virtual Server

IBM Power Virtual Server is a cloud-based infrastructure-as-a-service (IaaS) offering that allows businesses to run IBM Power Systems workloads in a flexible, scalable, and secure virtual environment. Built on the same architecture as IBM Power Systems used on-premises, it enables seamless hybrid cloud integration, making it ideal for enterprises looking to modernize their IT infrastructure without completely abandoning their existing investments. With support for AIX, IBM i, and Linux operating systems, Power Virtual Server provides a versatile platform for mission-critical applications. It provides an excellent platform for moving AIX, IBM i, and Linux on Power workloads to the cloud without a time consuming and risky migration of those workloads to a different platform.

One of the key advantages of IBM Power Virtual Server is its ability to deliver high performance and reliability for enterprise workloads. It leverages IBM's Power processors, which are optimized for data-intensive tasks and high-throughput computing. The platform also offers features such as dynamic resource allocation, automated scaling, and integrated backup and disaster recovery options. These capabilities help organizations maintain business continuity and meet demanding service-level agreements (SLAs).

Additionally, IBM Power Virtual Server integrates with a wide range of IBM Cloud services and third-party tools, enabling users to build and manage hybrid cloud environments with ease. It supports DevOps practices, containerization with OpenShift, and AI/ML workloads, making it suitable for both traditional enterprise applications and modern cloud-native development. With its pay-as-you-go pricing model and global availability, IBM Power Virtual Server offers a cost-effective and agile solution for businesses aiming to innovate and grow in a digital-first world.

Power Virtual Server allows you to run IBM i, AIX, and Linux workloads in a cloud experience giving you fast, self-service provisioning, flexible compute, memory, and storage resources.

PowerVS is designed to help organizations modernize their infrastructure, migrate to hybrid cloud models, and optimize their data center resources.

Key features and benefits of PowerVS include:

- /SM590000 Flexible and Scalable:
- Users can easily adjust compute, memory, and storage resources on demand, scaling up or down as needed.
- /SM590000 Pay-as-you-use:
- Billing is based on consumption, allowing organizations to manage cost effectively.
- /SM590000 Hybrid Cloud Integration:
- PowerVS facilitates seamless integration of AIX and IBM i workloads with cloud-native applications and services.
- /SM590000 Performance and Security:
- PowerVS leverages the performance and security features of IBM Power Systems while offering the agility of cloud computing.
- /SM590000 Managed Infrastructure:
- PowerVS offers managed infrastructure, where IBM handles the hardware maintenance, allowing organizations to focus on their workloads from a software and application standpoint.
- /SM590000 Disaster Recovery:

PowerVS can be used to implement disaster recovery solutions for critical applications.

- /SM590000 Simplified Cloud Adoption:
- PowerVS streamlines the cloud adoption process for IBM i, AIX, and other Power Systems workloads.

## 10.3.1  Power Virtual Server options

Power Virtual Server is a single offering that can be delivered in two variations; off-premises, where the infrastructure components are located in IBM data centers, and on-premises, where the infrastructure components are located in the client's data center. The on-premises variation is known as IBM Power Virtual Server Private Cloud. Both variations provide a cloud-based consumption model, where you pay for resources as they are consumed, and are managed with the same management interfaces.

Figure 10-1 shows the two different implementations of IBM Power Virtual Server.

Figure 10-1   IBM Power Virtual Server options

<!-- image -->

Power Virtual Server Private Cloud is discussed in section 10.3.3, 'Power Virtual Server Private Cloud' on page 209.

## Use cases for Power Virtual Server

IBM Power Virtual Server supports a wide range of use cases, particularly for enterprises that rely on IBM Power Systems for mission-critical workloads. Here are some of the most common and impactful scenarios:

## 1. Hybrid Cloud Modernization

Organizations with on-premises IBM Power Systems can extend their infrastructure to the cloud using Power Virtual Server. This enables a hybrid cloud model where workloads can be moved or replicated to the cloud for scalability, disaster recovery, or testing - without needing to replatform or rewrite applications. It Is especially useful for businesses running AIX or IBM i environments that want to modernize gradually.

2. Disaster Recovery and High Availability

Power Virtual Server provides a reliable platform for disaster recovery (DR) by allowing businesses to replicate their on-prem workloads to the cloud. In the event of a failure, workloads can be quickly restored in the cloud, minimizing downtime. It also supports high availability configurations, ensuring business continuity for critical applications.

3. Development and Testing Environments

Developers can use Power Virtual Server to quickly spin up isolated environments for application development, testing, and quality assurance. This is particularly valuable for teams working on AIX, IBM i, or Linux-based applications, as it eliminates the need for dedicated on-prem hardware and allows for faster iteration and deployment cycles.

## 4. SAP HANA and ERP Workloads

IBM Power Virtual Server is certified to run SAP HANA and other SAP ERP applications. Enterprises can migrate or extend their SAP environments to the cloud for better scalability, performance, and cost-efficiency - especially when paired with IBM's high-performance Power10 processors.

5. AI and Machine Learning

Power Virtual Server supports AI/ML workloads, particularly those requiring high compute power and memory bandwidth. It integrates with IBM Watsonfi and Red Hat OpenShift, allowing data scientists to build, train, and deploy models using familiar tools while leveraging the performance of Power Systems.

6. Database Hosting

Many organizations use Power Virtual Server to host enterprise-grade databases like IBM Db2, Oracle, or PostgreSQL. The platform offers high I/O throughput and reliability, making it ideal for transaction-heavy applications such as banking, retail, and logistics.

7. Legacy Application Hosting

For businesses with legacy applications running on AIX or IBM i, Power Virtual Server provides a cloud-based alternative to aging on-prem hardware. This allows companies to extend the life of critical applications while gradually modernizing their infrastructure.

8. Compliance and Data Sovereignty

Organizations in regulated industries (e.g., healthcare, finance, government) use Power Virtual Server to meet strict compliance and data residency requirements. IBM offers regional data centers and compliance certifications to help meet these needs.

## 10.3.2  Power Virtual Server in the Cloud

In this variation, IBM Power Virtual Server resources reside in IBM data centers with dedicated networking and storage area network (SAN) attached storage. Customers can choose from one of the currently 21 different data centers, choosing the data center that is closest to their users. IBM Power clients who rely on private cloud infrastructure can now quickly and economically extend their Power IT resources on the cloud.

In these IBM data centers, Power Virtual Server resources are separated from the rest of IBM cloud servers with separate networks and direct attached storage. In this offering, customers are given flexibility to choose from different IBM Power server models, an IBM supplied OS image, and different tiers for storage based on your specific workload requirements. Customers are also given to choose their own image as Bring Your Own License (BYOL) so that they can bring already configured images from their enterprise infrastructure onto Power Virtual Server. This offering also supports distinctive features such as shared processor pools, public connectivity of VMs, placement groups, and global replication service (GRS) which can be consumed based on customer's requirements.

For more information on IBM PowerVS refer to Getting Started with IBM Power Virtual Server.

## 10.3.3  Power Virtual Server Private Cloud

For those clients that want a cloud experience running on IBM Power servers, but have requirements to keep control of their data and applications (whether those requirements are regulatory, security based, or perhaps performance based). For those clients, IBM designed an offering where we bring the Power Virtual Server offering to a data center of your choice.

Power Virtual Server Private Cloud extends all the benefits of IBM Power Virtual Server into your (or a partner's) data center. The enhanced capabilities of IBM Power Virtual Server Private Cloud provide managed infrastructure as a service at client locations, with metered consumption and no upfront costs to support Hybrid by Design delivery of services.

The IBM Power Virtual Server Private Cloud offering is engineered to:

- /SM590000 Maintain customer data and workloads on your own site.
- Enterprises may have workloads or data that is regulated and cannot be hosted off-premises. In some cases, enterprises can have workloads that are sensitive or with ultra-short latency requirements that are better served on site and in very close in proximity with other on-site workloads.
- /SM590000 Maintain customer data in region and specific geographies in the location of their choice.
- Country sovereignty regulations are requiring some data and workloads to stay in country. According to a recent IBM Institute of Business Value study, 61% of cloud leaders cite security or compliance as reasons for moving certain workloads from public clouds to private clouds or on-premises data centers.
- /SM590000 Provide a seamless hybrid cloud experience.
- Enterprises can foster a unified hybrid cloud landscape by seamlessly integrating Power Virtual Server running both at an IBM site and at a client site location with the ability to manage all the virtual machines (VMs) and infrastructure effortlessly through a unified user interface. Clients can receive the flexibility utilizing as-a-service with intentional workload placement on and off premises.
- /SM590000 Deliver predictable charging model with committed monthly spend combined with flexible consumption with metered usage-based pricing.
- Both Power Virtual Server offerings, off-premises running at an IBM location and on-premises running at the client site, include compute, memory, storage, and operating system licenses that are fully metered by the hour allowing clients to pay for how much they use each month with no upfront payment.
- /SM590000 Streamline IT operations.
- Whether in the cloud or at an enterprise's site, IBM manages the infrastructure, freeing enterprises to focus on business outcomes and less on managing infrastructure. IBM will own, deliver, and set up the Power Virtual Server in datacenter of choice, and provide a fully managed solution, including monitoring, security, firmware updates, and infrastructure management.
- /SM590000 Provide enhanced security and control of data.
- IBM Power Virtual Server is designed to provide comprehensive security for IBM Power infrastructure by integrating with IBM Cloud tooling to manage security. This alleviates the need to manage Power infrastructure security with the added benefit of maintaining sensitive data and workload on-premises.

The physical infrastructure is delivered as a point of delivery (pod) which will be deployed in customer's data center. A pod is the physical component which resides within the client datacenter and contains the compute, storage and network components. A pod contains one or more racks where each of the components are installed. The racks are interconnected to provide a completely self-contained infrastructure, including both customer usable components, spare components, and management components.

This pod will be maintained by IBM site reliability engineering (SREs) and managed through IBM Cloud platform. Each pod is associated with an IBM Cloud satellite location that is owned by customer's IBM cloud account. This architecture provides the ability to scale your private cloud infrastructure horizontally by adding more pods to meet your workloads requirements.

For more information about Power Virtual Server Private Cloud refer to the IBM Redpaper Introduction to IBM Power Virtual Server Private Cloud , REDP-5745.

## 10.3.4  Introducing IBM Power11 in the Cloud

Power11 will be available on the day of announcement in IBM Power Virtual Server and IBM Power Virtual Server Private Cloud. With the launch of IBM Power11, clients and ISVs gain immediate access to the latest Power hardware in the cloud - enabling faster innovation and greater flexibility. Whether you're modernizing applications or enhancing business continuity, Power11 in PowerVS delivers a seamless hybrid cloud experience.

## Key benefits include:

- /SM590000 Rapid Deployment - Launch Power11 virtual servers in the cloud in under 10 minutes.
- /SM590000 Accelerated Modernization - Enable agile development and testing on the latest Power infrastructure.
- /SM590000 Enhanced Business Resiliency - Strengthen continuity strategies with cloud-based Power11 resources.
- /SM590000 Data Sovereignty and Compliance - Meet regulatory and sensitive data requirements with on-premises options.
- /SM590000 Consistent Hybrid Cloud Experience - Enjoy unified operations across on-premises and cloud environments.

Figure 10-2 shows the initial implementation of Power11 processors in Power Virtual Server on announcement day.

Figure 10-2   Availability of Power11 in Power Virtual Server

<!-- image -->

Power11 technology in Power Virtual Server allows you to immediately take advantage of the new functionality without a requirement for initial capital investment. You can safely implement workloads taking advantage of the Trusted security capabilities of Power11 like:

- /SM590000 Quantum-safe infrastructure compliance with optional workload cryptographic inventory discovery
- /SM590000 A unified cyber resiliency solution responding to evolving cyber threats and regulatory standards with optional 3rd party services

In addition, you can immediately take advantage of the improved performance of Power11 as shown in Figure 10-3.

Figure 10-3   Potential performance benefits from Power in PowerVS

<!-- image -->
